{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Automated test data management tool","text":"Say Goodbye to Slow and Complex Integration Tests Automate end-to-end data tests for any job or application"},{"location":"#generate","title":"Generate","text":"<p>Generate production-like data to test your jobs or applications. Create data in files, databases, HTTP APIs and messaging systems.</p> Generate data in existing Postgres database <p>You have existing tables in Postgres and you want to generate data for them whilst maintaining relationships between tables.</p> ScalaJavaYAMLUI <pre><code>val accountTask = postgres(\"customer_postgres\", \"jdbc:postgresql://localhost:5432/customer\")\nval config = configuration.enableGeneratePlanAndTasks(true)\n</code></pre> <pre><code>var accountTask = postgres(\"customer_accounts\", \"jdbc:postgresql://localhost:5432/customer\");\nvar config = configuration().enableGeneratePlanAndTasks(true);\n</code></pre> <pre><code>#with env variable ENABLE_GENERATE_PLAN_AND_TASKS=true\nname: \"postgres_example_plan\"\ntasks:\n  - name: \"postgres_account\"\n    dataSourceName: \"postgresCustomer\"\n</code></pre> <ol> <li>Click on <code>Connection</code> tab and add your Postgres conneciton</li> <li>Go back to <code>Home</code> tab and <code>Select data source</code> as your Postgres connection</li> <li>Click on <code>Generate</code> and select <code>Auto</code></li> <li>Click on <code>Execute</code> to generate data</li> </ol> Sample <pre><code>SELECT * FROM account.accounts LIMIT 1;\n\naccount_number | account_status |     created_by      | created_by |     open_timestamp      \n---------------+----------------+---------------------+------------+------------------------\n0499572486     | closed         | Stewart Hartmann    | eod        | 2023-12-02 12:30:37.602 \n\nSELECT * FROM account.balances where account_number='0499572486';\n\naccount_number |  balance  |    update_timestamp      \n---------------+-----------+------------------------\n0499572486     | 285293.23 | 2024-01-30 03:30:29.012 \n\nSELECT * FROM account.transactions where account_number='0499572486';\n\naccount_number |  amount  |    create_timestamp      \n---------------+----------+------------------------\n0499572486     |  1893.46 | 2024-03-13 18:05:45.565\n</code></pre> Create, get and delete pets via HTTP API using the same <code>id</code> <p>First, generate data for creating pets via POST, then get pets via GET and finally delete pets via DELETE, all using  the same <code>id</code>.</p> ScalaJavaYAMLUI <pre><code>val httpTask = http(\"my_http\")\n  .fields(metadataSource.openApi(\"/opt/app/http/petstore.json\"))\n  .count(count.records(10))\n\nval myPlan = plan.addForeignKeyRelationship(\n  foreignField(\"my_http\", \"POST/pets\", \"body.id\"),\n  foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\"),\n  foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\")\n)\n</code></pre> <pre><code>var httpTask = http(\"my_http\")\n    .fields(metadataSource().openApi(\"/opt/app/http/petstore.json\"))\n    .count(count().records(10));\n\nvar myPlan = plan().addForeignKeyRelationship(\n        foreignField(\"my_http\", \"POST/pets\", \"body.id\"),\n        foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\"),\n        foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\")\n);\n</code></pre> <pre><code>---\nname: \"http_openapi\"\nsteps:\n  - name: \"pets\"\n    count:\n      records: 10\n    options:\n      metadataSourceType: \"openApi\"\n      schemaLocation: \"/opt/app/http/petstore.json\"\n---\nname: \"http_plan\"\ntasks:\n  - name: \"http_openapi\"\n    dataSourceName: \"http\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"http\"\n        step: \"POST/pets\"\n        fields: [\"body.id\"]\n      generate:\n        - dataSource: \"http\"\n          step: \"GET/pets/{id}\"\n          fields: [\"pathParamid\"]\n        - dataSource: \"http\"\n          step: \"DELETE/pets/{id}\"\n          fields: [\"pathParamid\"]\n</code></pre> <ol> <li>Click on <code>Connection</code> tab, add your OpenAPI/Swagger connection to file and add HTTP connection</li> <li>Go back to <code>Home</code> tab and <code>Select data source</code> as your HTTP connection</li> <li>Click on <code>Generate</code>, select <code>Auto with metadata source</code> and then select your OpenAPI/Swagger connection</li> <li>Go to <code>Relationships</code> and click on <code>+ Relationship</code><ol> <li>For Source, select your task name, field as <code>body.id</code>, method as <code>POST</code> and endpoint as <code>/pets</code></li> <li>Click on <code>Generation</code> and <code>+ Link</code>, select your task name, field as <code>pathParamid</code>, method as <code>GET</code> and endpoint as <code>/pets/{id}</code></li> <li>Click on <code>+ Link</code>, select your task name, field as <code>pathParamid</code>, method as <code>DELETE</code> and endpoint as <code>/pets/{id}</code></li> </ol> </li> <li>Click on <code>Advanced Configuration</code>, open <code>Flag</code> and enable <code>Generate Plan And Tasks</code></li> <li>Click on <code>Execute</code> to generate data</li> </ol> Sample <pre><code>[\n  {\n    \"method\": \"POST\",\n    \"url\": \"http://localhost/anything/pets\",\n    \"body\": {\n      \"id\": \"ZzDRmGMnoei9M5D\", \n      \"name\": \"Dave Windler\"\n    }\n  },\n  {\n    \"method\": \"GET\",\n    \"url\": \"http://localhost/anything/pets/ZzDRmGMnoei9M5D\"\n  },\n  {\n    \"method\": \"DELETE\",\n    \"url\": \"http://localhost/anything/pets/ZzDRmGMnoei9M5D\"\n  }\n]\n</code></pre> Populate Kafka topic with account events <p>Create fresh data in your Kafka topics for account events with nested structures.</p> ScalaJavaYAMLUI <pre><code>val kafkaTask = kafka(\"my_kafka\", \"localhost:9092\")\n  .topic(\"accounts\")\n  .fields(field.name(\"key\").sql(\"body.account_id\"))\n  .fields(\n    field.messageBody(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\"),\n      field.name(\"balance\").`type`(DoubleType).round(2),\n      field.name(\"details\")\n        .fields(\n          field.name(\"name\").expression(\"#{Name.name}\"),\n          field.name(\"open_date\").`type`(DateType).min(LocalDate.now())\n        )\n    )\n  )\n</code></pre> <pre><code>var kafkaTask = kafka(\"my_kafka\", \"localhost:9092\")\n        .topic(\"accounts\")\n        .fields(field().name(\"key\").sql(\"body.account_id\"))\n        .fields(\n                field().messageBody(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n                        field().name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\"),\n                        field().name(\"balance\").type(DoubleType.instance()).round(2),\n                        field().name(\"details\")\n                                .fields(\n                                        field().name(\"name\").expression(\"#{Name.name}\"),\n                                        field().name(\"open_date\").type(DateType.instance()).min(LocalDate.now())\n                                )\n                )\n        )\n</code></pre> <pre><code>---\nname: \"simple_kafka\"\nsteps:\n- name: \"kafka_account\"\n  type: \"kafka\"\n  options:\n    topic: \"accounts\"\n  fields:\n    - name: \"key\"\n      type: \"string\"\n      options:\n        sql: \"body.account_id\"\n    - name: \"messageBody\"\n      type: struct\n      fields:\n      - name: \"account_id\"\n        options:\n          regex: \"ACC[0-9]{8}\"\n      - name: \"account_status\"\n        options:\n          oneOf: [\"open\", \"closed\", \"suspended\", \"pending\"]\n      - name: \"balance\"\n        type: \"double\"\n        options:\n          round: 2\n      - name: \"details\"\n        type: struct\n        fields:\n          - name: \"name\"\n          - name: \"open_date\"\n            type: \"date\"\n            options:\n              min: \"now()\"\n</code></pre> <ol> <li>Click on <code>Connection</code> tab, add your Kafka connection</li> <li>Go back to <code>Home</code> tab, <code>Select data source</code> as your Kafka connection and put topic as <code>accounts</code></li> <li>Click on <code>Generate</code> and select <code>Manual</code> checkbox<ol> <li>Click on <code>+ Field</code>, add name <code>key</code> with type <code>string</code><ol> <li>Click on <code>+</code>, select <code>SQL</code> and enter <code>body.account_id</code></li> </ol> </li> <li>Click on <code>+ Field</code>, add name <code>messageBody</code> with type <code>struct</code><ol> <li>Click on inner <code>+ Field</code>, add name <code>account_id</code> with type <code>string</code></li> <li>Click on <code>+</code>, select <code>Regex</code> and enter <code>ACC[0-9]{8}</code></li> <li>Click on inner <code>+ Field</code>, add name <code>account_status</code> with type <code>string</code></li> <li>Click on <code>+</code>, select <code>One Of</code> and enter <code>open, closed, suspended, pending</code></li> <li>Click on inner <code>+ Field</code>, add name <code>balance</code> with type <code>double</code></li> <li>Click on <code>+</code>, select <code>Round</code> and enter <code>2</code></li> <li>Click on inner <code>+ Field</code>, add name <code>details</code> with type <code>struct</code><ol> <li>Click on inner <code>+ Field</code>, add name <code>name</code> with type <code>string</code></li> <li>Click on inner <code>+ Field</code>, add name <code>open_date</code> with type <code>date</code><ol> <li>Click on <code>+</code>, select <code>Min</code> and enter <code>now()</code></li> </ol> </li> </ol> </li> </ol> </li> </ol> </li> <li>Click on <code>Execute</code> to generate data</li> </ol> Sample <pre><code>[\n  {\n    \"account_id\":\"ACC35554421\",\n    \"account_status\":\"open\",\n    \"balance\":89139.62,\n    \"details\":{\n      \"name\":\"Jonie Farrell\",\n      \"open_date\":\"2025-01-15\"\n    }\n  },\n  {\n    \"account_id\":\"ACC30149813\",\n    \"account_status\":\"closed\",\n    \"balance\":28861.09,\n    \"details\":{\n      \"name\":\"Debrah Douglas\",\n      \"open_date\":\"2025-01-17\"\n    }\n  },\n  {\n    \"account_id\":\"ACC58341320\",\n    \"account_status\":\"pending\",\n    \"balance\":57543.91,\n    \"details\":{\n      \"name\":\"Elmer Lind\",\n      \"open_date\":\"2025-01-20\"\n    }\n  }\n]\n</code></pre>"},{"location":"#and-validate","title":"And Validate","text":"<p>Ensure your job or service is working as expected before going to production by generating data, ingesting it and then  validating the downstream data sources have the correct information.</p> Check all generated records from CSV exist in Iceberg <p>Run data generation for CSV file (based on schema from data contract), consume it from your job (that produces an  Iceberg table) and then validate it.</p> ScalaJavaYAMLUI <pre><code>val csvTask = csv(\"csv_accounts\", \"/data/csv/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(metadataSource.openDataContractStandard(\"/opt/app/mount/odcs/full-example.odcs.yaml\"))\n\nval icebergTask = iceberg(\"iceberg_accounts\", \"dev.accounts\", \"/data/iceberg/customer/account\")\n  .validations(\n    validation.unique(\"account_id\"),\n    validation.groupBy(\"account_id\").sum(\"balance\").greaterThan(0),\n    validation.field(\"open_time\").isIncreasing(),\n    validation.count().isEqual(1000)\n  )\n  .validationWait(waitCondition.file(\"/data/iceberg/customer/account\"))\n</code></pre> <pre><code>var csvTask = csv(\"csv_accounts\", \"/data/csv/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(metadataSource().openDataContractStandard(\"/opt/app/mount/odcs/full-example.odcs.yaml\"));\n\nvar icebergTask = iceberg(\"iceberg_accounts\", \"dev.accounts\", \"/data/iceberg/customer/account\")\n        .validations(\n                validation().unique(\"account_id\"),\n                validation().groupBy(\"account_id\").sum(\"balance\").greaterThan(0),\n                validation().field(\"open_time\").isIncreasing(),\n                validation().count().isEqual(1000)\n        )\n        .validationWait(waitCondition().file(\"/data/iceberg/customer/account\"));\n</code></pre> <pre><code>---\nname: \"csv_accounts\"\nsteps:\n- name: \"accounts\"\n  type: \"csv\"\n  options:\n    path: \"/data/csv/customer/account\"\n    metadataSourceType: \"openDataContractStandard\"\n    dataContractFile: \"/opt/app/mount/odcs/full-example.odcs.yaml\"\n---\nname: \"iceberg_account_checks\"\ndataSources:\n  iceberg:\n    - options:\n        path: \"/data/iceberg/customer/account\"\n      validations:\n        - field: \"account_id\"\n          validation:\n            - type: \"unique\"\n        - field: \"open_time\"\n          validation:\n            - type: \"isIncreasing\"\n        - groupByFields: [ \"account_id\" ]\n          aggType: \"sum\"\n          aggExpr: \"sum(balance) &gt; 0\"\n        - aggType: \"count\"\n          aggExpr: \"count == 1000\"\n</code></pre> <ol> <li>Click on <code>Connection</code> tab, add your CSV, Iceberg and ODCS (Open Data Contract Standard) connection</li> <li>Go back to <code>Home</code> tab, enter task name as <code>csv_accounts</code> and <code>Select data source</code> as your CSV connection</li> <li>Click on <code>Generate</code> and select <code>Auto from metadata source</code> checkbox<ol> <li>Select your ODCS connection as the metadata source</li> </ol> </li> <li>Click on <code>+ Task</code>, select <code>Iceberg</code> and select your Iceberg connection<ol> <li>Click on <code>+ Validation</code>, select <code>Field</code>, enter <code>account_id</code> and select <code>Unique</code></li> <li>Click on <code>+ Validation</code>, select <code>Group By</code> and enter <code>account_id</code><ol> <li>Click on <code>+</code>, select <code>Sum</code> and enter <code>balance &gt; 0</code></li> </ol> </li> <li>Click on <code>+ Validation</code>, select <code>Field</code> and enter <code>open_time</code><ol> <li>Click on <code>+</code>, select <code>Is Increasing</code></li> </ol> </li> <li>Click on <code>+ Validation</code>, select <code>Group By</code> and enter <code>account_id</code></li> <li>Click on <code>+</code>, select <code>Count</code>, click on <code>+</code> next to count, select <code>Equal</code> and enter <code>1000</code></li> </ol> </li> <li>Click on <code>Execute</code> to generate data</li> </ol> Use validations from Great Expectations <p>If you have existing data quality rules from an external source like Great Expectations, you can use them to  validate your data without rewriting them as part of your tests.</p> ScalaJavaYAMLUI <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/taxi_json\")\n  .validations(metadataSource.greatExpectations(\"/opt/app/mount/ge/taxi-expectations.json\"))\n</code></pre> <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/taxi_json\")\n        .validations(metadataSource().greatExpectations(\"/opt/app/mount/ge/taxi-expectations.json\"));\n</code></pre> <pre><code>name: \"taxi_data_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/opt/app/data/taxi_json\"\n        metadataSourceType: \"greatExpectations\"\n        expectationsFile: \"/opt/app/mount/ge/taxi-expectations.json\"\n</code></pre> <ol> <li>Click on <code>Connection</code> tab, select data source type as <code>Great Expecations</code><ol> <li>Enter <code>Expectations file</code> as <code>/opt/app/mount/ge/taxi-expectations.json</code></li> </ol> </li> <li>Click on <code>Home</code> tab, <code>Select data source</code> as your JSON connection</li> <li>Open <code>Validation</code> and select checkbox <code>Auto from metadata source</code><ol> <li>Select your Great Expectations connection as the metadata source</li> </ol> </li> <li>Click on <code>Execute</code> to generate data</li> </ol> Complex validations based on pre-conditions or upstream data <ul> <li>Check <code>balance</code> is <code>0</code> when <code>status</code> is <code>closed</code></li> <li>Check <code>open_time</code> is the same in CSV and Iceberg</li> <li>Check sum of <code>amount</code> in Iceberg is the same as <code>balance</code> in CSV for each <code>account_id</code></li> </ul> ScalaJavaYAMLUI <pre><code>val icebergTask = iceberg(\"iceberg_accounts\", \"dev.accounts\", \"/data/iceberg/customer/account\")\n  .validations(\n    validation.preFilter(validation.field(\"status\").isEqual(\"closed\")).field(\"balance\").isEqual(0),\n    validation.upstreamData(accountTask)\n      .joinFields(\"account_id\")\n      .validations(\n        validation.field(\"open_time\").isEqualField(\"csv_accounts.open_time\"),\n        validation.groupBy(\"account_id\", \"csv_accounts_balance\").sum(\"amount\").isEqualField(\"csv_accounts_balance\")\n      )\n  )\n</code></pre> <pre><code>var icebergTask = iceberg(\"iceberg_accounts\", \"dev.accounts\", \"/data/iceberg/customer/account\")\n        .validations(\n                validation().preFilter(validation().field(\"status\").isEqual(\"closed\")).field(\"balance\").isEqual(0),\n                validation().upstreamData(accountTask)\n                        .joinFields(\"account_id\")\n                        .validations(\n                                validation().field(\"open_time\").isEqualField(\"csv_accounts.open_time\"),\n                                validation().groupBy(\"account_id\", \"csv_accounts_balance\").sum(\"amount\").isEqualField(\"csv_accounts_balance\")\n                        )\n        );\n</code></pre> <pre><code>---\nname: \"iceberg_account_checks\"\ndataSources:\n  iceberg:\n    - options:\n        path: \"/data/iceberg/customer/account\"\n      validations:\n        - preFilterExpr: \"status == 'closed'\"\n          expr: \"balance == 0\"\n        - upstreamDataSource: \"csv_accounts\"\n          joinFields: [\"account_id\"]\n          validations:\n            - expr: \"open_time == csv_accounts.open_time\"\n            - groupByFields: [\"account_id\", \"csv_accounts_balance\"]\n              aggType: \"sum\"\n              aggExpr: \"sum(amount) == csv_accounts_balance\"\n</code></pre> <ol> <li>Click on <code>+ Task</code>, select <code>Iceberg</code> and select your Iceberg connection<ol> <li>Pre-filter is not available yet via UI but will be soon!</li> <li>Click on <code>+ Validation</code>, select <code>Upstream</code> and enter <code>csv_accounts</code></li> <li>Click on <code>+</code>, select <code>Join Field(s)</code> and enter <code>account_id</code></li> <li>Click on <code>+ Validation</code>, select <code>Field</code> and enter <code>open_time</code><ol> <li>Click on <code>+</code>, select <code>Equal</code> and enter <code>csv_accounts.open_time</code></li> </ol> </li> <li>Click on <code>+ Validation</code>, select <code>Group By</code> and enter <code>account_id, csv_accounts_balance</code><ol> <li>Click on <code>+</code>, select <code>Sum</code> and enter <code>amount</code></li> <li>Click on <code>+</code>, select <code>Equal</code> and enter <code>csv_accounts_balance</code></li> </ol> </li> </ol> </li> <li>Click on <code>Execute</code> to generate data</li> </ol>"},{"location":"#why-use-data-caterer","title":"Why use Data Caterer","text":"<ul> <li>Catch bugs before production: Bring stability to your data pipelines</li> <li>Speed up your development cycles: Fast feedback testing locally and in test environments</li> <li>Single tool for all data sources: No custom scripts needed</li> <li>No production data or connection required: Secure first approach, fully metadata driven</li> <li>Easy to use for testers and developers: Use either UI, Java, Scala or YAML</li> <li>Simulate complex data flows: Maintain relationships across data sources</li> </ul>"},{"location":"#main-features","title":"Main features","text":"<ul> <li> Connect to any data source</li> <li> Auto generate production-like data from data connections or metadata sources</li> <li> Relationships across data sources</li> <li> Validate based on data generated</li> <li> Clean up generated and downstream data</li> </ul> <p> Try now Demo </p>"},{"location":"#what-it-is","title":"What it is","text":"<ul> <li> <p> Test data management tool</p> <p>Generate synthetic production-like data to be consumed and validated. Clean up the data after using to keep your  environments clean.</p> </li> <li> <p> Run locally and in test environments</p> <p>Fast feedback loop for developers and testers to ensure the data is correct before going to production.</p> </li> <li> <p> Designed for any data source</p> <p>Support for pushing data to any data source, in any format, batch or real-time.</p> </li> <li> <p> High/Low/No code solution</p> <p>Use the tool via either UI, Java, Scala or YAML.</p> </li> <li> <p> Developer productivity tool</p> <p>If you are a new developer or seasoned veteran, cut down on your feedback loop when developing with data.</p> </li> </ul>"},{"location":"#who-can-use-it","title":"Who can use it","text":"Type Interface User No Code UI QA, Testers, Data Scientist, Analyst Low Code YAML DevOps, Kubernetes Fans High Code Java/Scala Software Developers, Data Engineers <p> Try now Demo </p>"},{"location":"about/","title":"About","text":"<p>Hi, my name is Peter. I am a Software Developer, mainly focussing on data related services. My experience can be found on my LinkedIn.</p> <p>I have created Data Caterer to help serve individuals and companies with data generation and data testing. It is a complex area that has many edge cases or intricacies that are hard to summarise or turn into something actionable and repeatable. Through the use of metadata, Data Caterer can help simplify your data testing, simulating production environment data, aid in data debugging, or whatever your data use case may be.</p> <p>Given that it is going to save you and your team time and money, please help in considering financial support. This will help the product grow into a sustainable and feature-full service.</p>"},{"location":"about/#contact","title":"Contact","text":"<p>Please contact us via Slack or via email <code>hello@data.catering</code> if you have any questions or queries.</p>"},{"location":"demo/","title":"Demo","text":"<p>See what the Data Caterer UI looks like and see what it is capable of.</p> <p> Demo </p> <ul> <li>Create a data generation or validation plan</li> <li>View different options for defining where schema information comes from or manually defined yourself</li> <li>View the possible data source connections available for data generation and validation</li> <li>Edit your existing plans</li> <li>Check the history and reports of the plan runs</li> </ul>"},{"location":"use-case/","title":"Use cases","text":""},{"location":"use-case/#replicate-production-in-lower-environment","title":"Replicate production in lower environment","text":"<p>Having a stable and reliable test environment is a challenge for a number of companies, especially where teams are asynchronously deploying and testing changes at faster rates. Data Caterer can help alleviate these issues by doing the following:</p> <ol> <li>Generates data with the latest schema changes and production like field values</li> <li>Run as a job on a daily/regular basis to replicate production traffic or data flows</li> <li>Validate data to ensure your system runs as expected</li> <li>Clean up data to avoid build up of generated data</li> </ol> <p></p>"},{"location":"use-case/#local-development","title":"Local development","text":"<p>Similar to the above, being able to replicate production like data in your local environment can be key to developing more reliable code as you can test directly against data in your local computer. This has a number of benefits including:</p> <ol> <li>Fewer assumptions or ambiguities when the developer codes</li> <li>Direct feedback loop in local computer rather than waiting for test environment for more reliable test data</li> <li>No domain expertise required to understand the data</li> <li>Easy for new developers to be onboarded and developing/testing code for jobs/services</li> </ol>"},{"location":"use-case/#systemintegration-testing","title":"System/integration testing","text":"<p>When working with third-party, external or internal data providers, it can be difficult to have all setup ready to produce reliable data that abides by relationship contracts between each of the systems. You have to rely on these data providers in order for you to run your tests which may not align to their priorities. With Data Caterer, you can generate the same data that they would produce, along with maintaining referential integrity across the data providers, so that you can run your tests without relying on their systems being up and reliable in their corresponding lower environments.</p>"},{"location":"use-case/#scenario-testing","title":"Scenario testing","text":"<p>If you want to set up particular data scenarios, you can customise the generated data to fit your scenario. Once the data gets generated and is consumed, you can also run validations to ensure your system has consumed the data correctly. These scenarios can be put together from existing tasks or data sources can be enabled/disabled based on your requirement. Built into Data Caterer and controlled via feature flags, is the ability to test edge cases based on the data type of the fields used for data generation (<code>enableEdgeCases</code> flag within <code>&lt;field&gt;.generator.options</code>, see more here).</p>"},{"location":"use-case/#data-debugging","title":"Data debugging","text":"<p>When data related issues occur in production, it may be difficult to replicate in a lower or local environment. It could be related to specific fields not containing expected results, size of data is too large or missing corresponding referenced data. This becomes key to resolving the issue as you can directly code against the exact data scenario and have confidence that your code changes will fix the problem. Data Caterer can be used to generate the appropriate data in whichever environment you want to test your changes against.</p>"},{"location":"use-case/#data-profiling","title":"Data profiling","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), metadata relating all the fields defined in the data sources you have configured will be generated via data profiling. You can run this as a standalone job (can disable <code>enableGenerateData</code>)  so that you can focus on the profile of the data you are utilising. This can be run against your production data sources  to ensure the metadata can be used to accurately generate data in other environments. This is a key feature of Data  Caterer as no direct production connections need to be maintained to generate data in other environments (which can  lead to serious concerns about data security as seen here).</p>"},{"location":"use-case/#schema-gathering","title":"Schema gathering","text":"<p>When using Data Caterer with the feature flag <code>enableGeneratePlanAndTasks</code> enabled (see here), all schemas of the data sources defined will be tracked in a common format (as tasks). This data, along with the data profiling metadata, could then feed back into your schema registries to help keep them up to date with your system.</p>"},{"location":"docs/","title":"Setup","text":"<p>All the configurations and customisation related to Data Caterer can be found under here.</p>"},{"location":"docs/#guide","title":"Guide","text":"<p>If you want a guided tour, you can follow one of the guides found here.</p>"},{"location":"docs/#specific-configuration","title":"Specific Configuration","text":"<ul> <li> Configurations - Configurations relating to feature flags, folder pathways, metadata   analysis</li> <li> Connections - Explore the data source connections available</li> <li> Generators - Choose and configure the type of generator you want used for   fields</li> <li> Validations - How to validate data to ensure your system is performing as expected</li> <li> Relationships - Define links between data elements across data sources</li> <li> Deployment - Deploy Data Caterer as a job to your chosen environment</li> <li> Advanced - Advanced usage of Data Caterer</li> </ul>"},{"location":"docs/#high-level-run-configurations","title":"High Level Run Configurations","text":""},{"location":"docs/advanced/","title":"Advanced use cases","text":""},{"location":"docs/advanced/#special-data-formats","title":"Special data formats","text":"<p>There are many options available for you to use when you have a scenario when data has to be a certain format.</p> <ol> <li>Create expression datafaker<ol> <li>Can be used to create names, addresses, or anything that can be found    under here</li> </ol> </li> <li>Create regex</li> </ol>"},{"location":"docs/advanced/#foreign-keys-across-data-sets","title":"Foreign keys across data sets","text":"<p>Details for how you can configure foreign keys can be found here.</p>"},{"location":"docs/advanced/#edge-cases","title":"Edge cases","text":"<p>For each given data type, there are edge cases which can cause issues when your application processes the data. This can be controlled at a field level by including the following flag in the generator options:</p> JavaScalaYAML <pre><code>field()\n  .name(\"amount\")\n  .type(DoubleType.instance())\n  .enableEdgeCases(true)\n  .edgeCaseProbability(0.1)\n</code></pre> <pre><code>field\n  .name(\"amount\")\n  .`type`(DoubleType)\n  .enableEdgeCases(true)\n  .edgeCaseProbability(0.1)\n</code></pre> <pre><code>fields:\n  - name: \"amount\"\n    type: \"double\"\n    generator:\n      type: \"random\"\n      options:\n        enableEdgeCases: \"true\"\n        edgeCaseProb: 0.1\n</code></pre> <p>If you want to know all the possible edge cases for each data type, can check the documentation here.</p>"},{"location":"docs/advanced/#scenario-testing","title":"Scenario testing","text":"<p>You can create specific scenarios by adjusting the metadata found in the plan and tasks to your liking. For example, if you had two data sources, a Postgres database and a parquet file, and you wanted to save account data into Postgres and transactions related to those accounts into a parquet file. You can alter the <code>status</code> field in the account data to only generate <code>open</code> accounts and define a foreign key between Postgres and parquet to ensure the same <code>account_id</code> is being used. Then in the parquet task, define 1 to 10 transactions per <code>account_id</code> to be generated.</p> <p>Postgres account generation example task Parquet transaction generation example task Plan</p>"},{"location":"docs/advanced/#cloud-storage","title":"Cloud storage","text":""},{"location":"docs/advanced/#data-source","title":"Data source","text":"<p>If you want to save the file types CSV, JSON, Parquet or ORC into cloud storage, you can do so via adding extra configurations. Below is an example for S3.</p> JavaScalaYAML <pre><code>var csvTask = csv(\"my_csv\", \"s3a://my-bucket/csv/accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    ...\n  );\n\nvar s3Configuration = configuration()\n  .runtimeConfig(Map.of(\n    \"spark.hadoop.fs.s3a.directory.marker.retention\", \"keep\",\n    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\",\n    \"spark.hadoop.fs.defaultFS\", \"s3a://my-bucket\",\n    //can change to other credential providers as shown here\n    //https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n    \"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n    \"spark.hadoop.fs.s3a.access.key\", \"access_key\",\n    \"spark.hadoop.fs.s3a.secret.key\", \"secret_key\"\n  ));\n\nexecute(s3Configuration, csvTask);\n</code></pre> <pre><code>val csvTask = csv(\"my_csv\", \"s3a://my-bucket/csv/accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    ...\n  )\n\nval s3Configuration = configuration\n  .runtimeConfig(Map(\n    \"spark.hadoop.fs.s3a.directory.marker.retention\" -&gt; \"keep\",\n    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" -&gt; \"true\",\n    \"spark.hadoop.fs.defaultFS\" -&gt; \"s3a://my-bucket\",\n    //can change to other credential providers as shown here\n    //https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n    \"spark.hadoop.fs.s3a.aws.credentials.provider\" -&gt; \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n    \"spark.hadoop.fs.s3a.access.key\" -&gt; \"access_key\",\n    \"spark.hadoop.fs.s3a.secret.key\" -&gt; \"secret_key\"\n  ))\n\nexecute(s3Configuration, csvTask)\n</code></pre> <pre><code>folders {\n   generatedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\n   planFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\n   taskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nruntime {\n    config {\n        ...\n        #S3\n        \"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n        \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n        \"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n        #can change to other credential providers as shown here\n        #https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n        \"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n        \"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n        \"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n   }\n}\n</code></pre>"},{"location":"docs/advanced/#storing-plantasks","title":"Storing plan/task(s)","text":"<p>You can generate and store the plan/task files inside either AWS S3, Azure Blob Storage or Google GCS. This can be controlled via configuration set in the <code>application.conf</code> file where you can set something like the below:</p> JavaScalaYAML <pre><code>configuration()\n  .generatedReportsFolderPath(\"s3a://my-bucket/data-caterer/generated\")\n  .planFilePath(\"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\")\n  .taskFolderPath(\"s3a://my-bucket/data-caterer/generated/task\")\n  .runtimeConfig(Map.of(\n    \"spark.hadoop.fs.s3a.directory.marker.retention\", \"keep\",\n    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\", \"true\",\n    \"spark.hadoop.fs.defaultFS\", \"s3a://my-bucket\",\n    //can change to other credential providers as shown here\n    //https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n    \"spark.hadoop.fs.s3a.aws.credentials.provider\", \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n    \"spark.hadoop.fs.s3a.access.key\", \"access_key\",\n    \"spark.hadoop.fs.s3a.secret.key\", \"secret_key\"\n  ));\n</code></pre> <pre><code>configuration\n  .generatedReportsFolderPath(\"s3a://my-bucket/data-caterer/generated\")\n  .planFilePath(\"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\")\n  .taskFolderPath(\"s3a://my-bucket/data-caterer/generated/task\")\n  .runtimeConfig(Map(\n    \"spark.hadoop.fs.s3a.directory.marker.retention\" -&gt; \"keep\",\n    \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" -&gt; \"true\",\n    \"spark.hadoop.fs.defaultFS\" -&gt; \"s3a://my-bucket\",\n    //can change to other credential providers as shown here\n    //https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n    \"spark.hadoop.fs.s3a.aws.credentials.provider\" -&gt; \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\",\n    \"spark.hadoop.fs.s3a.access.key\" -&gt; \"access_key\",\n    \"spark.hadoop.fs.s3a.secret.key\" -&gt; \"secret_key\"\n  ))\n</code></pre> <pre><code>folders {\n   generatedPlanAndTaskFolderPath = \"s3a://my-bucket/data-caterer/generated\"\n   planFilePath = \"s3a://my-bucket/data-caterer/generated/plan/customer-create-plan.yaml\"\n   taskFolderPath = \"s3a://my-bucket/data-caterer/generated/task\"\n}\n\nruntime {\n    config {\n        ...\n        #S3\n        \"spark.hadoop.fs.s3a.directory.marker.retention\" = \"keep\"\n        \"spark.hadoop.fs.s3a.bucket.all.committer.magic.enabled\" = \"true\"\n        \"spark.hadoop.fs.defaultFS\" = \"s3a://my-bucket\"\n        #can change to other credential providers as shown here\n        #https://hadoop.apache.org/docs/stable/hadoop-aws/tools/hadoop-aws/index.html#Changing_Authentication_Providers\n        \"spark.hadoop.fs.s3a.aws.credentials.provider\" = \"org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider\"\n        \"spark.hadoop.fs.s3a.access.key\" = \"access_key\"\n        \"spark.hadoop.fs.s3a.secret.key\" = \"secret_key\"\n   }\n}\n</code></pre>"},{"location":"docs/configuration/","title":"Configuration","text":"<p>A number of configurations can be made and customised within Data Caterer to help control what gets run and/or where any metadata gets saved.</p> <p>These configurations are defined from within your Java or Scala class via <code>configuration</code> or for YAML file setup, <code>application.conf</code> file as seen  here.</p>"},{"location":"docs/configuration/#flags","title":"Flags","text":"<p>Flags are used to control which processes are executed when you run Data Caterer.</p> Config Default Description <code>enableGenerateData</code> true Enable/disable data generation <code>enableCount</code> true Count the number of records generated. Can be disabled to improve performance <code>enableFailOnError</code> true Whilst saving generated data, if there is an error, it will stop any further data from being generated <code>enableSaveReports</code> true Enable/disable HTML reports summarising data generated, metadata of data generated (if <code>enableSinkMetadata</code> is enabled) and validation results (if <code>enableValidation</code> is enabled). Sample here <code>enableSinkMetadata</code> true Run data profiling for the generated data. Shown in HTML reports if <code>enableSaveSinkMetadata</code> is enabled <code>enableValidation</code> false Run validations as described in plan. Results can be viewed from logs or from HTML report if <code>enableSaveSinkMetadata</code> is enabled. Sample here <code>enableUniqueCheck</code> false If enabled, for any <code>isUnique</code> fields, will ensure only unique values are generated <code>enableAlerts</code> true Enable/disable alerts to be sent <code>enableGeneratePlanAndTasks</code> false Enable/disable plan and task auto generation based off data source connections <code>enableRecordTracking</code> false Enable/disable which data records have been generated for any data source <code>enableDeleteGeneratedRecords</code> false Delete all generated records based off record tracking (if <code>enableRecordTracking</code> has been set to true) <code>enableGenerateValidations</code> false If enabled, it will generate validations based on the data sources defined. JavaScalaapplication.conf <pre><code>configuration()\n  .enableGenerateData(true)\n  .enableCount(true)\n  .enableFailOnError(true)\n  .enableSaveReports(true)\n  .enableSinkMetadata(true)\n  .enableValidation(false)\n  .enableUniqueCheck(true)\n  .enableAlerts(true)\n  .enableGeneratePlanAndTasks(false)\n  .enableRecordTracking(false)\n  .enableDeleteGeneratedRecords(false)\n  .enableGenerateValidations(false);\n</code></pre> <pre><code>configuration\n  .enableGenerateData(true)\n  .enableCount(true)\n  .enableFailOnError(true)\n  .enableSaveReports(true)\n  .enableSinkMetadata(true)\n  .enableValidation(false)\n  .enableUniqueCheck(true)\n  .enableAlerts(true)\n  .enableGeneratePlanAndTasks(false)\n  .enableRecordTracking(false)\n  .enableDeleteGeneratedRecords(false)\n  .enableGenerateValidations(false)\n</code></pre> <pre><code>flags {\n  enableCount = false\n  enableCount = ${?ENABLE_COUNT}\n  enableGenerateData = true\n  enableGenerateData = ${?ENABLE_GENERATE_DATA}\n  enableFailOnError = true\n  enableFailOnError = ${?ENABLE_FAIL_ON_ERROR}\n  enableGeneratePlanAndTasks = false\n  enableGeneratePlanAndTasks = ${?ENABLE_GENERATE_PLAN_AND_TASKS}\n  enableRecordTracking = false\n  enableRecordTracking = ${?ENABLE_RECORD_TRACKING}\n  enableDeleteGeneratedRecords = false\n  enableDeleteGeneratedRecords = ${?ENABLE_DELETE_GENERATED_RECORDS}\n  enableUniqueCheck = true\n  enableUniqueCheck = ${?ENABLE_UNIQUE_CHECK}\n  enableSinkMetadata = true\n  enableSinkMetadata = ${?ENABLE_SINK_METADATA}\n  enableSaveReports = true\n  enableSaveReports = ${?ENABLE_SAVE_REPORTS}\n  enableValidation = false\n  enableValidation = ${?ENABLE_VALIDATION}\n  enableGenerateValidations = false\n  enableGenerateValidations = ${?ENABLE_GENERATE_VALIDATIONS}\n  enableAlerts = false\n  enableAlerts = ${?ENABLE_ALERTS}\n}\n</code></pre>"},{"location":"docs/configuration/#folders","title":"Folders","text":"<p>Depending on which flags are enabled, there are folders that get used to save metadata, store HTML reports or track the records generated.</p> <p>These folder pathways can be defined as a cloud storage pathway (i.e. <code>s3a://my-bucket/task</code>).</p> Config Default Description <code>planFilePath</code> /opt/app/plan/customer-create-plan.yaml Plan file path to use when generating and/or validating data <code>taskFolderPath</code> /opt/app/task Task folder path that contains all the task files (can have nested directories) <code>validationFolderPath</code> /opt/app/validation Validation folder path that contains all the validation files (can have nested directories) <code>generatedReportsFolderPath</code> /opt/app/report Where HTML reports get generated that contain information about data generated along with any validations performed <code>generatedPlanAndTaskFolderPath</code> /tmp Folder path where generated plan and task files will be saved <code>recordTrackingFolderPath</code> /opt/app/record-tracking Where record tracking parquet files get saved <code>recordTrackingForValidationFolderPath</code> /opt/app/record-tracking-validation Where record tracking parquet files get saved for the purpose of validation JavaScalaapplication.conf <pre><code>configuration()\n  .planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n  .taskFolderPath(\"/opt/app/custom/task\")\n  .validationFolderPath(\"/opt/app/custom/validation\")\n  .generatedReportsFolderPath(\"/opt/app/custom/report\")\n  .generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n  .recordTrackingFolderPath(\"/opt/app/custom/record-tracking\")\n  .recordTrackingForValidationFolderPath(\"/opt/app/custom/record-tracking-validation\");\n</code></pre> <pre><code>configuration\n  .planFilePath(\"/opt/app/custom/plan/postgres-plan.yaml\")\n  .taskFolderPath(\"/opt/app/custom/task\")\n  .validationFolderPath(\"/opt/app/custom/validation\")\n  .generatedReportsFolderPath(\"/opt/app/custom/report\")\n  .generatedPlanAndTaskFolderPath(\"/opt/app/custom/generated\")\n  .recordTrackingFolderPath(\"/opt/app/custom/record-tracking\")\n  .recordTrackingForValidationFolderPath(\"/opt/app/custom/record-tracking-validation\")\n</code></pre> <pre><code>folders {\n  planFilePath = \"/opt/app/custom/plan/postgres-plan.yaml\"\n  planFilePath = ${?PLAN_FILE_PATH}\n  taskFolderPath = \"/opt/app/custom/task\"\n  taskFolderPath = ${?TASK_FOLDER_PATH}\n  validationFolderPath = \"/opt/app/custom/validation\"\n  validationFolderPath = ${?VALIDATION_FOLDER_PATH}\n  generatedReportsFolderPath = \"/opt/app/custom/report\"\n  generatedReportsFolderPath = ${?GENERATED_REPORTS_FOLDER_PATH}\n  generatedPlanAndTaskFolderPath = \"/opt/app/custom/generated\"\n  generatedPlanAndTaskFolderPath = ${?GENERATED_PLAN_AND_TASK_FOLDER_PATH}\n  recordTrackingFolderPath = \"/opt/app/custom/record-tracking\"\n  recordTrackingFolderPath = ${?RECORD_TRACKING_FOLDER_PATH}\n  recordTrackingForValidationFolderPath = \"/opt/app/custom/record-tracking-validation\"\n  recordTrackingForValidationFolderPath = ${?RECORD_TRACKING_VALIDATION_FOLDER_PATH}\n}\n</code></pre>"},{"location":"docs/configuration/#metadata","title":"Metadata","text":"<p>When metadata gets generated, there are some configurations that can be altered to help with performance or accuracy related issues. Metadata gets generated from two processes: 1) if <code>enableGeneratePlanAndTasks</code> or 2) if <code>enableSinkMetadata</code> are enabled.</p> <p>During the generation of plan and tasks, data profiling is used to create the metadata for each of the fields defined in the data source. You may face issues if the number of records in the data source is large as data profiling is an expensive task. Similarly, it can be expensive when analysing the generated data if the number of records generated is large.</p> Config Default Description <code>numRecordsFromDataSource</code> 10000 Number of records read in from the data source that could be used for data profiling <code>numRecordsForAnalysis</code> 10000 Number of records used for data profiling from the records gathered in <code>numRecordsFromDataSource</code> <code>oneOfMinCount</code> 1000 Minimum number of records required before considering if a field can be of type <code>oneOf</code> <code>oneOfDistinctCountVsCountThreshold</code> 0.2 Threshold ratio to determine if a field is of type <code>oneOf</code> (i.e. a field called <code>status</code> that only contains <code>open</code> or <code>closed</code>. Distinct count = 2, total count = 10, ratio = 2 / 10 = 0.2 therefore marked as <code>oneOf</code>) <code>numGeneratedSamples</code> 10 Number of sample records from generated data to take. Shown in HTML report JavaScalaapplication.conf <pre><code>configuration()\n  .numRecordsFromDataSourceForDataProfiling(10000)\n  .numRecordsForAnalysisForDataProfiling(10000)\n  .oneOfMinCount(1000)\n  .oneOfDistinctCountVsCountThreshold(1000)\n  .numGeneratedSamples(10);\n</code></pre> <pre><code>configuration\n  .numRecordsFromDataSourceForDataProfiling(10000)\n  .numRecordsForAnalysisForDataProfiling(10000)\n  .oneOfMinCount(1000)\n  .oneOfDistinctCountVsCountThreshold(1000)\n  .numGeneratedSamples(10)\n</code></pre> <pre><code>metadata {\n  numRecordsFromDataSource = 10000\n  numRecordsForAnalysis = 10000\n  oneOfMinCount = 1000\n  oneOfDistinctCountVsCountThreshold = 0.2\n  numGeneratedSamples = 10\n}\n</code></pre>"},{"location":"docs/configuration/#generation","title":"Generation","text":"<p>When generating data, you may have some limitations such as limited CPU or memory, large number of data sources, or data sources prone to failure under load. To help alleviate these issues or speed up performance, you can control the number of records that get generated in each batch.</p> Config Default Description <code>numRecordsPerBatch</code> 100000 Number of records across all data sources to generate per batch <code>numRecordsPerStep</code> Overrides the count defined in each step with this value if defined (i.e. if set to 1000, for each step, 1000 records will be generated) JavaScalaapplication.conf <pre><code>configuration()\n  .numRecordsPerBatch(100000)\n  .numRecordsPerStep(1000);\n</code></pre> <pre><code>configuration\n  .numRecordsPerBatch(100000)\n  .numRecordsPerStep(1000)\n</code></pre> <pre><code>generation {\n  numRecordsPerBatch = 100000\n  numRecordsPerStep = 1000\n}\n</code></pre>"},{"location":"docs/configuration/#validation","title":"Validation","text":"<p>Configurations to alter how validations are executed.</p> Config Default Description <code>numSampleErrorRecords</code> 5 Number of error sample records to retrieve and display in generated HTML report. Increase to help debugging data issues <code>enableDeleteRecordTrackingFiles</code> true After validations are complete, delete record tracking files that were used for validation purposes (enabled via <code>enableRecordTracking</code>) JavaScalaapplication.conf <pre><code>configuration()\n  .numSampleErrorRecords(10)\n  .enableDeleteRecordTrackingFiles(false);\n</code></pre> <pre><code>configuration\n  .numSampleErrorRecords(10)\n  .enableDeleteRecordTrackingFiles(false)\n</code></pre> <pre><code>validatoin {\n  numSampleErrorRecords = 10\n  enableDeleteRecordTrackingFiles = false\n}\n</code></pre>"},{"location":"docs/configuration/#runtime","title":"Runtime","text":"<p>Given Data Caterer uses Spark as the base framework for data processing, you can configure the job as to your  specifications via configuration as seen here.</p> JavaScalaapplication.conf <pre><code>configuration()\n  .master(\"local[*]\")\n  .runtimeConfig(Map.of(\"spark.driver.cores\", \"5\"))\n  .addRuntimeConfig(\"spark.driver.memory\", \"10g\");\n</code></pre> <pre><code>configuration\n  .master(\"local[*]\")\n  .runtimeConfig(Map(\"spark.driver.cores\" -&gt; \"5\"))\n  .addRuntimeConfig(\"spark.driver.memory\" -&gt; \"10g\")\n</code></pre> <pre><code>runtime {\n  master = \"local[*]\"\n  master = ${?DATA_CATERER_MASTER}\n  config {\n    \"spark.driver.cores\" = \"5\"\n    \"spark.driver.memory\" = \"10g\"\n  }\n}\n</code></pre>"},{"location":"docs/connection/","title":"Data Source Connections","text":"<p>Details of all the connection configuration supported can be found in the below subsections for each type of connection.</p> <p>These configurations can be done via API or from configuration. Examples of both are shown for each data source below.</p>"},{"location":"docs/connection/#supported-data-connections","title":"Supported Data Connections","text":"Data Source Type Data Source Support Cloud Storage AWS S3 Cloud Storage Azure Blob Storage Cloud Storage GCP Cloud Storage Database BigQuery Database Cassandra Database MySQL Database Postgres Database Elasticsearch Database MongoDB Database Opensearch File CSV File Delta Lake File Iceberg File JSON File ORC File Parquet File Hudi HTTP REST API Messaging Kafka Messaging RabbitMQ Messaging Solace Messaging ActiveMQ Messaging Pulsar Metadata Data Contract CLI Metadata Great Expectations Metadata JSON Schema Metadata Marquez Metadata OpenMetadata Metadata OpenAPI/Swagger Metadata Open Data Contract Standard (ODCS) Metadata Amundsen Metadata Datahub Metadata Solace Event Portal"},{"location":"docs/connection/#api","title":"API","text":"<p>All connection details require a name. Depending on the data source, you can define additional options which may be used by the driver or connector for connecting to the data source.</p>"},{"location":"docs/connection/#configuration-file","title":"Configuration file","text":"<p>All connection details follow the same pattern.</p> <pre><code>&lt;connection format&gt; {\n    &lt;connection name&gt; {\n        &lt;key&gt; = &lt;value&gt;\n    }\n}\n</code></pre> <p>Overriding configuration</p> <p>When defining a configuration value that can be defined by a system property or environment variable at runtime, you can define that via the following:</p> <pre><code>url = \"localhost\"\nurl = ${?POSTGRES_URL}\n</code></pre> <p>The above defines that if there is a system property or environment variable named <code>POSTGRES_URL</code>, then that value will be used for the <code>url</code>, otherwise, it will default to <code>localhost</code>.</p>"},{"location":"docs/connection/#data-sources","title":"Data sources","text":"<p>To find examples of a task for each type of data source, please check out this page.</p>"},{"location":"docs/connection/#file","title":"File","text":"<p>Linked here is a list of generic options that can be included as part of your file data source configuration if required. Links to specific file type configurations can be found below.</p>"},{"location":"docs/connection/#csv","title":"CSV","text":"JavaScalaYAML <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>csv(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>csv {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?CSV_PATH}\n  }\n}\n</code></pre></p> <p>Other available configuration for CSV can be found here</p>"},{"location":"docs/connection/#json","title":"JSON","text":"JavaScalaYAML <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>json(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>json {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?JSON_PATH}\n  }\n}\n</code></pre></p> <p>Other available configuration for JSON can be found here</p>"},{"location":"docs/connection/#orc","title":"ORC","text":"JavaScalaYAML <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>orc(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>orc {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?ORC_PATH}\n  }\n}\n</code></pre></p> <p>Other available configuration for ORC can be found here</p>"},{"location":"docs/connection/#parquet","title":"Parquet","text":"JavaScalaYAML <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>parquet(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>parquet {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?PARQUET_PATH}\n  }\n}\n</code></pre></p> <p>Other available configuration for Parquet can be found here</p>"},{"location":"docs/connection/#delta","title":"Delta","text":"JavaScalaYAML <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <pre><code>delta(\"customer_transactions\", \"/data/customer/transaction\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>delta {\n  customer_transactions {\n    path = \"/data/customer/transaction\"\n    path = ${?DELTA_PATH}\n  }\n}\n</code></pre></p>"},{"location":"docs/connection/#iceberg","title":"Iceberg","text":"JavaScalaYAML <pre><code>iceberg(\n  \"customer_accounts\",              //name\n  \"account.accounts\",               //table name\n  \"/opt/app/data/customer/iceberg\", //warehouse path\n  \"hadoop\",                         //catalog type\n  \"\",                               //catalogUri\n  Map.of()                          //additional options\n);\n</code></pre> <pre><code>iceberg(\n  \"customer_accounts\",              //name\n  \"account.accounts\",               //table name\n  \"/opt/app/data/customer/iceberg\", //warehouse path\n  \"hadoop\",                         //catalog type\n  \"\",                               //catalogUri\n  Map()                             //additional options\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>iceberg {\n  customer_transactions {\n    path = \"/opt/app/data/customer/iceberg\"\n    path = ${?ICEBERG_WAREHOUSE_PATH}\n    catalogType = \"hadoop\"\n    catalogType = ${?ICEBERG_CATALOG_TYPE}\n    catalogUri = \"\"\n    catalogUri = ${?ICEBERG_CATALOG_URI}\n  }\n}\n</code></pre></p>"},{"location":"docs/connection/#rmdbs","title":"RMDBS","text":"<p>Follows the same configuration used by Spark as found here. Sample can be found below</p> JavaScalaYAML <pre><code>postgres(\n    \"customer_postgres\",                            //name\n    \"jdbc:postgresql://localhost:5432/customer\",    //url\n    \"postgres\",                                     //username\n    \"postgres\"                                      //password\n)\n</code></pre> <pre><code>postgres(\n    \"customer_postgres\",                            //name\n    \"jdbc:postgresql://localhost:5432/customer\",    //url\n    \"postgres\",                                     //username\n    \"postgres\"                                      //password\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jdbc {\n    customer_postgres {\n        url = \"jdbc:postgresql://localhost:5432/customer\"\n        url = ${?POSTGRES_URL}\n        user = \"postgres\"\n        user = ${?POSTGRES_USERNAME}\n        password = \"postgres\"\n        password = ${?POSTGRES_PASSWORD}\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre></p> <p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> SQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre>"},{"location":"docs/connection/#postgres","title":"Postgres","text":"<p>Can see example API or Config definition for Postgres connection above.</p>"},{"location":"docs/connection/#permissions","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.tables TO &lt; user &gt;;\nGRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\nGRANT SELECT ON information_schema.table_constraints TO &lt; user &gt;;\nGRANT SELECT ON information_schema.constraint_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"docs/connection/#mysql","title":"MySQL","text":"JavaScalaYAML <pre><code>mysql(\n    \"customer_mysql\",                       //name\n    \"jdbc:mysql://localhost:3306/customer\", //url\n    \"root\",                                 //username\n    \"root\"                                  //password\n)\n</code></pre> <pre><code>mysql(\n    \"customer_mysql\",                       //name\n    \"jdbc:mysql://localhost:3306/customer\", //url\n    \"root\",                                 //username\n    \"root\"                                  //password\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jdbc {\n    customer_mysql {\n        url = \"jdbc:mysql://localhost:3306/customer\"\n        user = \"root\"\n        password = \"root\"\n        driver = \"com.mysql.cj.jdbc.Driver\"\n    }\n}\n</code></pre></p>"},{"location":"docs/connection/#permissions_1","title":"Permissions","text":"<p>Following permissions are required when generating plan and tasks:</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"docs/connection/#bigquery","title":"BigQuery","text":"<p>Follows same configuration as defined by the Spark BigQuery Connector as found  here.</p> JavaScalaYAML <pre><code>bigquery(\n    \"customer_bigquery\",   //name\n    \"gs://my-test-bucket\", //temporaryGcsBucket\n    Map.of()               //optional additional connection options\n)\n</code></pre> <pre><code>bigquery(\n  \"customer_bigquery\",   //name\n  \"gs://my-test-bucket\", //temporaryGcsBucket\n  Map()                  //optional additional connection options\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>bigquery {\n    customer_bigquery {\n        temporaryGcsBucket = \"gs://my-test-bucket\"\n        temporaryGcsBucket = ${?BIGQUERY_TEMPORARY_GCS_BUCKET}\n    }\n}\n</code></pre></p>"},{"location":"docs/connection/#cassandra","title":"Cassandra","text":"<p>Follows same configuration as defined by the Spark Cassandra Connector as found here</p> JavaScalaYAML <pre><code>cassandra(\n    \"customer_cassandra\",   //name\n    \"localhost:9042\",       //url\n    \"cassandra\",            //username\n    \"cassandra\",            //password\n    Map.of()                //optional additional connection options\n)\n</code></pre> <pre><code>cassandra(\n    \"customer_cassandra\",   //name\n    \"localhost:9042\",       //url\n    \"cassandra\",            //username\n    \"cassandra\",            //password\n    Map()                   //optional additional connection options\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>org.apache.spark.sql.cassandra {\n    customer_cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre></p>"},{"location":"docs/connection/#permissions_2","title":"Permissions","text":"<p>Ensure that the user has write permission, so it is able to save the table to the target tables.</p> CQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO &lt;user&gt;;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and fields from the below tables.</p> CQL Permission Statements <pre><code>GRANT SELECT ON system_schema.tables TO &lt;user&gt;;\nGRANT SELECT ON system_schema.columns TO &lt;user&gt;;\n</code></pre>"},{"location":"docs/connection/#kafka","title":"Kafka","text":"<p>Define your Kafka bootstrap server to connect and send generated data to corresponding topics. Topic gets set at a step level. Further details can be found here</p> JavaScalaYAML <pre><code>kafka(\n    \"customer_kafka\",   //name\n    \"localhost:9092\"    //url\n)\n</code></pre> <pre><code>kafka(\n    \"customer_kafka\",   //name\n    \"localhost:9092\"    //url\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>kafka {\n    customer_kafka {\n        kafka.bootstrap.servers = \"localhost:9092\"\n        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}\n    }\n}\n</code></pre></p> <p>When defining your schema for pushing data to Kafka, it follows a specific top level schema. An example can be found here . You can define the key, value, headers, partition or topic by following the linked schema.</p>"},{"location":"docs/connection/#jms","title":"JMS","text":"<p>Uses JNDI lookup to send messages to JMS queue. Ensure that the messaging system you are using has your queue/topic registered via JNDI otherwise a connection cannot be created.</p>"},{"location":"docs/connection/#rabbitmq","title":"Rabbitmq","text":"JavaScalaYAML <pre><code>rabbitmq(\n    \"customer_rabbitmq\",                            //name\n    \"amqp://localhost:5672\",                        //url\n    \"guest\",                                        //username\n    \"guest\",                                        //password\n    \"/\",                                            //virtual host\n    \"com.rabbitmq.jms.admin.RMQConnectionFactory\",  //connection factory\n)\n</code></pre> <pre><code>rabbitmq(\n  \"customer_rabbitmq\",                            //name\n  \"amqp://localhost:5672\",                        //url\n  \"guest\",                                        //username\n  \"guest\",                                        //password\n  \"/\",                                            //virtual host\n  \"com.rabbitmq.jms.admin.RMQConnectionFactory\",  //connection factory\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jms {\n    customer_rabbitmq {\n        connectionFactory = \"com.rabbitmq.jms.admin.RMQConnectionFactory\"\n        connectionFactory = ${?RABBITMQ_CONNECTION_FACTORY}\n        url = \"amqp://localhost:5672\"\n        url = ${?RABBITMQ_URL}\n        user = \"guest\"\n        user = ${?RABBITMQ_USER}\n        password = \"guest\"\n        password = ${?RABBITMQ_PASSWORD}\n        virtualHost = \"/\"\n        virtualHost = ${?RABBITMQ_VIRTUAL_HOST}\n    }\n}\n</code></pre></p>"},{"location":"docs/connection/#solace","title":"Solace","text":"JavaScalaYAML <pre><code>solace(\n    \"customer_solace\",                                      //name\n    \"smf://localhost:55554\",                                //url\n    \"admin\",                                                //username\n    \"admin\",                                                //password\n    \"default\",                                              //vpn name\n    \"/jms/cf/default\",                                      //connection factory\n    \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   //initial context factory\n)\n</code></pre> <pre><code>solace(\n    \"customer_solace\",                                      //name\n    \"smf://localhost:55554\",                                //url\n    \"admin\",                                                //username\n    \"admin\",                                                //password\n    \"default\",                                              //vpn name\n    \"/jms/cf/default\",                                      //connection factory\n    \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"   //initial context factory\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jms {\n    customer_solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        connectionFactory = \"/jms/cf/default\"\n        url = \"smf://localhost:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre></p>"},{"location":"docs/connection/#http","title":"HTTP","text":"<p>Define any username and/or password needed for the HTTP requests. The url is defined in the tasks to allow for generated data to be populated in the url.</p> JavaScalaYAML <pre><code>http(\n    \"customer_api\", //name\n    \"admin\",        //username\n    \"admin\"         //password\n)\n</code></pre> <pre><code>http(\n    \"customer_api\", //name\n    \"admin\",        //username\n    \"admin\"         //password\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>http {\n    customer_api {\n        user = \"admin\"\n        user = ${?HTTP_USER}\n        password = \"admin\"\n        password = ${?HTTP_PASSWORD}\n    }\n}\n</code></pre></p>"},{"location":"docs/delete-data/","title":"Delete Generated Data","text":"<p>As you generate and validate data, you may want to clean up the data that has been generated. This helps you:</p> <ol> <li>Keep your test environments clean</li> <li>Reduce chance of existing data interacting with your validations</li> <li>Creates a simple workflow for developers and testers to follow</li> </ol>"},{"location":"docs/delete-data/#foreign-keysrelationships","title":"Foreign Keys/Relationships","text":"<p>You can either define a foreign key for data generation (i.e. create same account numbers across accounts and transactions table) or for data deletion (i.e. account numbers generated in Postgres are consumed by a job and pushed into a Parquet file, you can delete the Postgres and Parquet data via the account numbers generated).</p>"},{"location":"docs/delete-data/#generate","title":"Generate","text":"<p>In scenarios where you have defined foreign keys for multiple data sources, when data is generated, Data Caterer will ensure that the values generated in one data source, will be the same in the other. When you want to delete the data,  data will be deleted in reverse order of how the data was inserted. This ensures that for data sources, such as  Postgres, no errors will occur whilst deleting data.</p> <pre><code>graph LR\n  subgraph plan [\"Plan/Scenario\"]\n    postgresAccount[\"Generate Postgres accounts\"]\n    postgresTransaction[\"Generate Postgres transactions\"]\n  end\n\n  dataCaterer[\"Data Caterer\"]\n\n  subgraph postgres [\"Postgres\"]\n    subgraph postgresAccTable [\"Accounts table\"]\n      accountA[\"ACC12345,2024-01-01\"] ~~~\n      accountB[\"ACC98765,2024-01-23\"] ~~~\n      accountC[\"...\"]\n    end\n    subgraph postgresTxnTable [\"Transactions table\"]\n      accountATxn[\"ACC12345,10.23\"] ~~~\n      accountBTxn[\"ACC98765,93.51\"] ~~~\n      accountCTxn[\"ACC98765,5.72\"] ~~~\n      accountDTxn[\"...\"]\n    end\n  end\n\n  postgresAccount --&gt; dataCaterer\n  postgresTransaction --&gt; dataCaterer\n  dataCaterer --&gt; postgresAccTable\n  dataCaterer --&gt; postgresTxnTable</code></pre>"},{"location":"docs/delete-data/#configuration","title":"Configuration","text":"<p>To define the generated data that should be deleted, follow the below configurations:</p> JavaScalaYAMLUI <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"name\"),\n    ...\n  );\nvar postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"full_name\"),\n    ...\n  );\n\nvar deletePlan = plan().addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List.of(Map.entry(postgresTxn, \"account_id\"))\n);\n\nvar deleteConfig = configuration()\n  .enableRecordTracking(true)\n  .enableDeleteGeneratedRecords(true)\n  .enableGenerateData(false);\n\nexecute(deletePlan, deleteConfig, postgresAcc, postgresTxn);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"name\"),\n    ...\n  )\nval postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"full_name\"),\n    ...\n  )\n\nval deletePlan = plan.addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List(postgresTxn -&gt; \"account_id\")\n)\n\nval deleteConfig = configuration\n  .enableRecordTracking(true)\n  .enableDeleteGeneratedRecords(true)\n  .enableGenerateData(false)\n\nexecute(deletePlan, deleteConfig, postgresAcc, postgresTxn)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_id\"\n      - name: \"name\"\n  - name: \"transactions\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"full_name\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"postgres\"\n        step: \"accounts\"\n        fields: [\"account_id\"]\n      generate:\n        - dataSource: \"postgres\"\n          step: \"transactions\"\n          fields: [\"account_id\"]\n</code></pre> <p></p>"},{"location":"docs/delete-data/#delete","title":"Delete","text":"<p>Once you have generated data, you may consume it via a job or service and push that data down into other data sources. You can choose to also delete the data that is pushed down into the other data sources by defining a delete relationship.</p> <pre><code>graph LR\n  subgraph plan [\"Plan/Scenario\"]\n    postgresAccount[\"Generate Postgres accounts\"]\n  end\n\n  dataCaterer[\"Data Caterer\"]\n\n  subgraph postgresAccTable [\"Postgres accounts table\"]\n    direction TB\n    accountA[\"ACC12345,2024-01-01\"] ~~~\n    accountB[\"ACC98765,2024-01-23\"] ~~~\n    accountC[\"...\"]\n  end\n\n  consumerJob[\"Consumer\"]\n\n  subgraph parquetAcc [\"Parquet accounts file\"]\n    direction TB\n    accountParquetA[\"ACC12345,2024-01-01\"] ~~~\n    accountParquetB[\"ACC98765,2024-01-23\"] ~~~\n    accountParquetC[\"...\"]\n  end\n\n  postgresAccount --&gt; dataCaterer\n  dataCaterer --&gt; postgresAccTable\n  consumerJob --&gt; postgresAccTable\n  consumerJob --&gt; parquetAcc</code></pre>"},{"location":"docs/delete-data/#configuration_1","title":"Configuration","text":"<p>We will use the scenario that we generate data for <code>accounts</code> table in Postgres and a job will insert a record into the  <code>balances</code> table for each record generated. To define the consumed data that should also be deleted,  follow the below example:</p> JavaScalaYAMLUI <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"name\"),\n    ...\n  );\nvar postgresBal = postgres(postgresAcc)\n  .table(\"public.balances\");\n\nvar deletePlan = plan().addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List.of(),\n  List.of(Map.entry(postgresBal, \"account_id\"))\n);\n\nvar deleteConfig = configuration()\n  .enableRecordTracking(true)\n  .enableDeleteGeneratedRecords(true)\n  .enableGenerateData(false);\n\nexecute(deletePlan, deleteConfig, postgresAcc);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"name\"),\n    ...\n  )\nval postgresBal = postgres(postgresAcc)\n  .table(\"public.balances\")\n\nval deletePlan = plan.addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List(),\n  List(postgresBal -&gt; \"account_id\")\n)\n\nval deleteConfig = configuration\n  .enableRecordTracking(true)\n  .enableDeleteGeneratedRecords(true)\n  .enableGenerateData(false)\n\nexecute(deletePlan, deleteConfig, postgresAcc)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_id\"\n      - name: \"name\"\n  - name: \"balances\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.balances\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"postgres\"\n        step: \"accounts\"\n        fields: [\"account_id\"]\n      delete:\n        - dataSource: \"postgres\"\n          step: \"balances\"\n          fields: [\"account_id\"]\n</code></pre> <p></p>"},{"location":"docs/deployment/","title":"Deployment","text":"<p>Three main ways to deploy and run Data Caterer:</p> <ul> <li>Application</li> <li>Docker</li> <li>Helm</li> </ul>"},{"location":"docs/deployment/#application","title":"Application","text":"<p>Run the OS native application from downloading the specific OS application here.</p>"},{"location":"docs/deployment/#docker","title":"Docker","text":"<p>To package up your class along with the Data Caterer base image, you can follow the Dockerfile that is created for you here.</p> <p>Then you can run the following:</p> <pre><code>./gradlew clean build\ndocker build -t &lt;my_image_name&gt;:&lt;my_image_tag&gt; .\n</code></pre>"},{"location":"docs/deployment/#helm","title":"Helm","text":"<p>Link to sample helm on GitHub here</p> <p>Update the configuration to your own data connections and configuration or own image created from above.</p> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\nhelm install data-caterer ./data-caterer-example/helm/data-caterer\n</code></pre>"},{"location":"docs/design/","title":"Design","text":"<p>This document shows the thought process behind the design of Data Caterer to help give you insights as to how and why it was created to what it is today. Also, this serves as a reference for future design decisions which will get updated  here and thus is a living document.</p>"},{"location":"docs/design/#motivation","title":"Motivation","text":"<p>The main difficulties that I faced as a developer and team lead relating to testing were:</p> <ul> <li>Difficulty in testing with multiple data sources, both batch and real time</li> <li>Reliance on other teams for stable environments or domain knowledge</li> <li>Test environments with no reliable or consistent data flows</li> <li>Complex data masking/anonymization solutions</li> <li>Relying on production data (potential privacy and data breach issues)</li> <li>Cost of data production issues can be very high</li> <li>Unknown unknowns staying hidden until problems occur in production</li> <li>Underutilised metadata</li> </ul>"},{"location":"docs/design/#guiding-principles","title":"Guiding Principles","text":"<p>These difficulties helped formed the basis of the principles for which Data Caterer should follow:</p> <ul> <li>Data source agnostic: Connect to any batch or real time data sources for data generation or validation</li> <li>Configurable: Run the application the way you want</li> <li>Extensible: Allow for new innovations to seamlessly integrate with Data Caterer</li> <li>Integrate with existing solutions: Utilise existing metadata to make it easy for users to use straight away</li> <li>Secure: No production connections required, metadata based solution</li> <li>Fast: Give developers fast feedback loops to encourage them to thoroughly test data flows</li> </ul>"},{"location":"docs/design/#high-level-flow","title":"High level flow","text":"<pre><code>graph LR\n  subgraph userTasks [User Configuration]\n  dataGen[Data Generation]\n  dataValid[Data Validation]\n  runConf[Runtime Config]\n  end\n\n  subgraph dataProcessor [Processor]\n  dataCaterer[Data Caterer]\n  end\n\n  subgraph existingMetadata [Metadata]\n  metadataService[Metadata Services]\n  metadataDataSource[Data Sources]\n  end\n\n  subgraph output [Output]\n  outputDataSource[Data Sources]\n  report[Report]\n  end\n\n  dataGen --&gt; dataCaterer\n  dataValid --&gt; dataCaterer\n  runConf --&gt; dataCaterer\n  direction TB\n  dataCaterer -.-&gt; metadataService\n  dataCaterer -.-&gt; metadataDataSource\n  direction LR\n  dataCaterer ---&gt; outputDataSource\n  dataCaterer ---&gt; report</code></pre> <ol> <li>User Configuration<ol> <li>Users define data generation, validation and runtime configuration</li> </ol> </li> <li>Processor<ol> <li>Engine will take user configuration to decide how to run</li> <li>User defined configuration merged with metadata from external sources</li> </ol> </li> <li>Metadata<ol> <li>Automatically retrieve schema, data profiling, relationship or validation rule metadata from data sources or metadata services</li> </ol> </li> <li>Output<ol> <li>Execute data generation and validation tasks on data sources</li> <li>Generate report summarising outcome</li> </ol> </li> </ol>"},{"location":"docs/validation/","title":"Validations","text":"<p>Validations can be used to run data checks after you have run the data generator or even as a standalone task. A report summarising the success or failure of the validations is produced and can be examined for further investigation.</p> <ul> <li>Basic - Basic field level validations</li> <li>Group by/Aggregate - Run aggregates over grouped data, then validate</li> <li>Upstream data source - Ensure record values exist in datasets based on other data sources or data generated</li> <li>Field names - Validate field names and ordering</li> <li>External validation source - Use pre-existing validation rules from sources such as Great Expectations or OpenMetadata</li> <li>Data Profile (Coming soon) - Score how close the data profile of generated data is against the target data profile</li> </ul>"},{"location":"docs/validation/#define-validations","title":"Define Validations","text":"<p>Full example validations can be found below. For more details, check out each of the subsections defined further below.</p> JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validations(\n    validation().field(\"amount\").lessThan(100),\n    validation().field(\"year\").isEqual(2021).errorThreshold(0.1),       //equivalent to if error percentage is &gt; 10%, then fail\n    validation().field(\"name\").matches(\"Peter .*\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n  )\n  .validationWait(waitCondition().pause(1));\n\nvar conf = configuration().enableValidation(true);\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validations(\n    validation.field(\"amount\").lessThan(100),\n    validation.field(\"year\").isEqual(2021).errorThreshold(0.1),       //equivalent to if error percentage is &gt; 10%, then fail\n    validation.field(\"name\").matches(\"Peter .*\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n  )  \n  .validationWait(waitCondition.pause(1))\n\nval conf = configuration.enableValidation(true)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      validations:\n        - expr: \"amount &lt; 100\"\n        - field: \"amount\" #or\n          validation:\n            - type: \"lessThan\"\n              value: 100\n        - expr: \"year == 2021\"\n          errorThreshold: 0.1   #equivalent to if error percentage is &gt; 10%, then fail\n        - expr: \"REGEXP_LIKE(name, 'Peter .*')\"\n          errorThreshold: 200   #equivalent to if number of errors is &gt; 200, then fail\n          description: \"Should be lots of Peters\"\n        - expr: \"amount &gt; 100\"\n          preFilterExpr: \"STARTSWITH(account_id, 'ACC')\"\n        - expr: \"ISNOTNULL(name)\"\n          preFilterExpr: \"STARTSWITH(account_id, 'ACC') AND ISNOTNULL(merchant)\"\n      waitCondition:\n        pauseInSeconds: 1\n</code></pre>"},{"location":"docs/validation/#pre-filter-data","title":"Pre-filter Data","text":"<p>If you need to run data validations on a subset of data, then you can define pre-filter conditions. An example would be  when you want to check that for all records with <code>status=closed</code>, that <code>balance=0</code>, you would define a pre-filter like below:</p> JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validations(\n    validation().preFilter(fieldPreFilter(\"status\").isEqual(\"closed\")).field(\"balance\").isEqual(0)\n  );\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validations(\n    validation.preFilter(fieldPreFilter(\"status\").isEqual(\"closed\")).field(\"balance\").isEqual(0)\n  )  \n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      validations:\n        - expr: \"balance == 0\"\n          preFilterExpr: \"status == 'closed'\"\n</code></pre>"},{"location":"docs/validation/#wait-condition","title":"Wait Condition","text":"<p>Once data has been generated, you may want to wait for a certain condition to be met before starting the data validations. This can be via:</p> <ul> <li>Pause for seconds</li> <li>When file is available</li> <li>Data exists</li> <li>Webhook</li> </ul>"},{"location":"docs/validation/#pause","title":"Pause","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition().pause(1));\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition.pause(1))\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        pauseInSeconds: 1\n</code></pre>"},{"location":"docs/validation/#data-exists","title":"Data exists","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWaitDataExists(\"updated_date &gt; DATE('2023-01-01')\");\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWaitDataExists(\"updated_date &gt; DATE('2023-01-01')\")\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        dataSourceName: \"transactions\"\n        options:\n          path: \"/tmp/csv\"\n        expr: \"updated_date &gt; DATE('2023-01-01')\"\n</code></pre>"},{"location":"docs/validation/#webhook","title":"Webhook","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition().webhook(\"http://localhost:8080/finished\")); //by default, GET request successful when 200 status code\n\n//or\n\nvar csvTxnsWithStatusCodes = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition().webhook(\"http://localhost:8080/finished\", \"GET\", 200, 202));  //successful if 200 or 202 status code\n\n//or\n\nvar csvTxnsWithExistingHttpConnection = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition().webhook(\"my_http\", \"http://localhost:8080/finished\"));  //use connection configuration from existing 'my_http' connection definition\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition.webhook(\"http://localhost:8080/finished\"))  //by default, GET request successful when 200 status code\n\n//or\n\nval csvTxnsWithStatusCodes = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition.webhook(\"http://localhost:8080/finished\", \"GET\", 200, 202)) //successful if 200 or 202 status code\n\n//or\n\nval csvTxnsWithExistingHttpConnection = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition.webhook(\"my_http\", \"http://localhost:8080/finished\")) //use connection configuration from existing 'my_http' connection definition\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        url: \"http://localhost:8080/finished\" #by default, GET request successful when 200 status code\n\n#or\n\n---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        url: \"http://localhost:8080/finished\"\n        method: \"GET\"\n        statusCodes: [200, 202] #successful if 200 or 202 status code\n\n#or\n\n---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        dataSourceName: \"my_http\" #use connection configuration from existing 'my_http' connection definition\n        url: \"http://localhost:8080/finished\"\n</code></pre>"},{"location":"docs/validation/#file-exists","title":"File exists","text":"JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition().file(\"/tmp/json\"));\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\")\n  .validationWait(waitCondition.file(\"/tmp/json\"))\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    - options:\n        path: \"/tmp/csv\"\n      waitCondition:\n        path: \"/tmp/json\"\n</code></pre>"},{"location":"docs/validation/#report","title":"Report","text":"<p>Once run, it will produce a report like this.</p>"},{"location":"docs/generator/count/","title":"Record Count","text":"<p>There are options related to controlling the number of records generated that can help in generating the scenarios or data required.</p>"},{"location":"docs/generator/count/#record-count_1","title":"Record Count","text":"<p>Record count is the simplest as you define the total number of records you require for that particular step. For example, in the below step, it will generate 1000 records for the CSV file  </p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(1000);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(1000)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      records: 1000\n</code></pre>"},{"location":"docs/generator/count/#generated-count","title":"Generated Count","text":"<p>As like most things in Data Caterer, the count can be generated based on some metadata. For example, if I wanted to generate between 1000 and 2000 records, I could define that by the below configuration:</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(generator().min(1000).max(2000));\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(generator.min(1000).max(2000))\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      options:\n        min: 1000\n        max: 2000\n</code></pre>"},{"location":"docs/generator/count/#per-field-count","title":"Per Field Count","text":"<p>When defining a per field count, this allows you to generate records \"per set of fields\". This means that for a given set of fields, it will generate a particular amount of records per combination of values  for those fields.  </p> <p>One example of this would be when generating transactions relating to a customer, a customer may be defined by fields <code>account_id, name</code>. A number of transactions would be generated per <code>account_id, name</code>.  </p> <p>You can also use a combination of the above two methods to generate the number of records per field.</p>"},{"location":"docs/generator/count/#records","title":"Records","text":"<p>When defining a base number of records within the <code>perField</code> configuration, it translates to creating <code>(count.records * count.recordsPerField)</code> records. This is a fixed number of records that will be generated each time, with no variation between runs.</p> <p>In the example below, we have <code>count.records = 1000</code> and <code>count.recordsPerField = 2</code>. Which means that <code>1000 * 2 = 2000</code> records will be generated in total.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count()\n      .records(1000)\n      .recordsPerField(2, \"account_id\", \"name\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count\n      .records(1000)\n      .recordsPerField(2, \"account_id\", \"name\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      records: 1000\n      perField:\n        count: 2\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n</code></pre>"},{"location":"docs/generator/count/#generated","title":"Generated","text":"<p>You can also define a generator for the count per field. This can be used in scenarios where you want a variable number of records per set of fields.</p> <p>In the example below, it will generate between <code>(count.records * count.perFieldGenerator.generator.min) = (1000 * 1) = 1000</code> and <code>(count.records * count.perFieldGenerator.generator.max) = (1000 * 2) = 2000</code> records.</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count()\n      .records(1000)\n      .recordsPerFieldGenerator(generator().min(1).max(2), \"account_id\", \"name\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count\n      .records(1000)\n      .recordsPerFieldGenerator(generator.min(1).max(2), \"account_id\", \"name\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      records: 1000\n      perField:\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n        options:\n          min: 1\n          max: 2\n</code></pre>"},{"location":"docs/generator/count/#one-of","title":"One Of","text":"<p>You can also generate a number of records based on a prescribed set of values. For example, if you only want to generate 1,5 or 10 records per <code>account_id, name</code> you can have the following configuration:</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count()\n      .records(1000)\n      .recordsPerFieldGenerator(generator().oneOf(1, 5, 10), \"account_id\", \"name\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count\n      .records(1000)\n      .recordsPerFieldGenerator(generator.oneOf(1, 5, 10), \"account_id\", \"name\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      records: 1000\n      perField:\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n        options:\n          oneOf: [1, 5, 10]\n</code></pre>"},{"location":"docs/generator/count/#weighted","title":"Weighted","text":"<p>You can also generate a number of records based on a prescribed set of values with weights. For example, if you want to generate 1 record 50% of the time, 5 records 30% of the time and 10 records 20% of the time per <code>account_id, name</code> you can have the following configuration:</p> JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count()\n      .records(1000)\n      .recordsPerFieldGenerator(generator().oneOfWeighted(\"1-&gt;0.5\", \"5-&gt;0.3\", \"10-&gt;0.2\"), \"account_id\", \"name\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .count(\n    count\n      .records(1000)\n      .recordsPerFieldGenerator(generator.oneOfWeighted(\"1-&gt;0.5\", \"5-&gt;0.3\", \"10-&gt;0.2\"), \"account_id\", \"name\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    count:\n      records: 1000\n      perField:\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n        options:\n          oneOf: [\"1-&gt;0.5\", \"5-&gt;0.3\", \"10-&gt;0.2\"]\n</code></pre>"},{"location":"docs/generator/count/#distribution","title":"Distribution","text":"<p>You also have the option to alter the distribution of the record count per field by choosing either <code>normal</code> or <code>exponential</code>. By default, the distribution is <code>uniform</code>. Below are examples where you can define the <code>min</code> and <code>max</code> parameters for  each distribution (<code>exponential</code> also has a <code>rateParameter</code> argument to adjust the distribution).</p> JavaScalaYAML <pre><code>count().recordsPerFieldNormalDistribution(1, 10, \"account_id\")\ncount().recordsPerFieldExponentialDistribution(1, 10, 2.0, \"account_id\")\n</code></pre> <pre><code>count.recordsPerFieldNormalDistribution(1, 10, \"account_id\")\ncount.recordsPerFieldExponentialDistribution(1, 10, 2.0, \"account_id\")\n</code></pre> <pre><code>---\nname: \"simple_kafka\"\nsteps:\n- name: \"kafka_account\"\n  type: \"kafka\"\n  options:\n    topic: \"accounts\"\n  count:\n    perField:\n    fieldNames:\n      - \"account_id\"\n    options:\n      min: 1\n      max: 10\n      distribution: \"normal\"\n</code></pre>"},{"location":"docs/generator/count/#nested-field","title":"Nested Field","text":"<p>You can also generate a number of records based on a nested field via creating a temporary field at the top level. For  example, we have a Kafka message body that contains <code>account_id</code> and we want to generate multiple records per <code>account_id</code>. We create a field called <code>tmp_account_id</code> that has the same metadata as we want for <code>account_id</code>. Then we associate the two fields via <code>field().name(\"account_id\").sql(\"tmp_account_id\")</code>.</p> JavaScalaYAML <pre><code>var kafkaTask = kafka(\"my_kafka\", \"kafka:9092\")\n        .topic(\"accounts\")\n        .fields(field().name(\"tmp_account_id\").regex(\"ACC[0-9]{8}\").omit(true))\n        .fields(\n                field().messageBody(\n                        field().name(\"account_id\").sql(\"tmp_account_id\"),\n                        field().name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                )\n        )\n        .count(count().recordsPerFieldGenerator(generator.min(1).max(10), \"tmp_account_id\"));\n</code></pre> <pre><code>val kafkaTask = kafka(\"my_kafka\", \"kafka:9092\")\n  .topic(\"accounts\")\n  .fields(field.name(\"tmp_account_id\").regex(\"ACC[0-9]{8}\").omit(true))\n  .fields(\n    field.messageBody(\n      field.name(\"account_id\").sql(\"tmp_account_id\"),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n  )\n.count(count.recordsPerFieldGenerator(generator.min(1).max(10), \"tmp_account_id\"))\n</code></pre> <pre><code>---\nname: \"simple_kafka\"\nsteps:\n- name: \"kafka_account\"\n  type: \"kafka\"\n  options:\n    topic: \"accounts\"\n  count:\n    perField:\n    fieldNames:\n      - \"tmp_account_id\"\n    options:\n      min: 1\n      max: 10\n  fields:\n    - name: \"tmp_account_id\"\n      options:\n        regex: \"ACC[0-9]{8}\"\n        omit: true\n    - name: \"key\"\n      options:\n        sql: \"body.account_id\"\n    - name: \"messageBody\"\n      type: struct\n      fields:\n      - name: \"account_id\"\n        options:\n          sql: \"tmp_account_id\"\n      - name: \"account_status\"\n        options:\n          oneOf: [\"open\", \"closed\", \"suspended\", \"pending\"]\n</code></pre>"},{"location":"docs/generator/count/#all-combinations","title":"All Combinations","text":"<p>If you want to generate records for all combinations of values that your fields can obtain, you can set <code>allCombinations</code> to <code>true</code>.</p> <p>For example, if your dataset has fields: - <code>account_id</code>: Some string - <code>debit_credit</code>: Either <code>D</code> or <code>C</code> - <code>status</code>: Either <code>open</code>, <code>closed</code> or <code>suspended</code></p> <p>It can generate a dataset like below where all combinations of <code>debit_credit</code> and <code>status</code> are covered:</p> account_id debit_credit status ACC123 D open ACC124 D closed ACC125 D suspended ACC126 C open ACC127 C closed ACC128 C suspended JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"debit_creidt\").oneOf(\"D\", \"C\"),\n    field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\")\n  )\n  .allCombinations(true);\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  schema(\n    field.name(\"account_id\"),\n    field.name(\"debit_creidt\").oneOf(\"D\", \"C\"),\n    field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\")\n  )\n  .allCombinations(true)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n      allCombinations: \"true\"\n    fields:\n      - name: \"account_id\"\n      - name: \"debit_credit\"\n        options:\n          oneOf:\n            - \"D\"\n            - \"C\"\n      - name: \"status\"\n        options:\n          oneOf:\n            - \"open\"\n            - \"closed\"\n            - \"suspended\"\n</code></pre>"},{"location":"docs/generator/data-generator/","title":"Data Generators","text":""},{"location":"docs/generator/data-generator/#data-types","title":"Data Types","text":"<p>Below is a list of all supported data types for generating data:</p> Data Type Options string, StringType String options integer, IntegerType Integer options long, LongType Long options short, ShortType Short options decimal(precision, scale), DecimalType Decimal options double, DoubleType Double options float, FloatType Float options date, DateType Date options timestamp, TimestampType Timestamp options boolean, BooleanType binary, BinaryType Binary options byte, ByteType Byte options array, ArrayType Array options struct, StructType"},{"location":"docs/generator/data-generator/#options","title":"Options","text":""},{"location":"docs/generator/data-generator/#all-data-types","title":"All data types","text":"<p>Some options are available to use for all types of data generators. Below is the list along with example and descriptions:</p> Option Default Example Description <code>enableEdgeCase</code> false <code>enableEdgeCase: \"true\"</code> Enable/disable generated data to contain edge cases based on the data type. For example, integer data type has edge cases of (Int.MaxValue, Int.MinValue and 0) <code>edgeCaseProbability</code> 0.0 <code>edgeCaseProb: \"0.1\"</code> Probability of generating a random edge case value if <code>enableEdgeCase</code> is true <code>isUnique</code> false <code>isUnique: \"true\"</code> Enable/disable generated data to be unique for that field. Errors will be thrown when it is unable to generate unique data <code>regex</code> <code>regex: \"ACC[0-9]{10}\"</code> Regular expression to define pattern generated data should follow <code>seed</code> <code>seed: \"1\"</code> Defines the random seed for generating data for that particular field. It will override any seed defined at a global level <code>sql</code> <code>sql: \"CASE WHEN amount &lt; 10 THEN true ELSE false END\"</code> Define any SQL statement for generating that fields value. Computation occurs after all non-SQL fields are generated. This means any fields used in the SQL cannot be based on other SQL generated fields. Data type of generated value from SQL needs to match data type defined for the field <code>oneOf</code> <code>oneOf: [\"open\", \"closed\", \"suspended\"]</code> or <code>oneOf: [\"open-&gt;0.8\", \"closed-&gt;0.1\", \"suspended-&gt;0.1\"]</code> Field can only take one of the prescribed values. Chance of value being chosen is based on the weight assigned to it. Weight can be any double value. <code>omit</code> false <code>omit: \"true\"</code> If true, field will not be included in final data generated. Useful for intermediate transformations that are not included in final outcome"},{"location":"docs/generator/data-generator/#string","title":"String","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated strings have at least length <code>minLen</code> <code>maxLen</code> 10 <code>maxLen: \"15\"</code> Ensures that all generated strings have at most length <code>maxLen</code> <code>expression</code> <code>expression: \"#{Name.name}\"</code><code>expression:\"#{Address.city}/#{Demographic.maritalStatus}\"</code> Will generate a string based on the faker expression provided. All possible faker expressions can be found here Expression has to be in format <code>#{&lt;faker expression name&gt;}</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <code>uuid</code> <code>uuid: \"account_id\"</code> Generate a UUID value. If value is non-empty, UUID value will be generated based off column value <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", \"\u0130yi g\u00fcnler\", \"\u0421\u043f\u0430\u0441\u0438\u0431\u043e\", \"\u039a\u03b1\u03bb\u03b7\u03bc\u03ad\u03c1\u03b1\", \"\u0635\u0628\u0627\u062d \u0627\u0644\u062e\u064a\u0631\", \" F\u00f6rl\u00e5t\", \"\u4f60\u597d\u5417\", \"Nh\u00e0 v\u1ec7 sinh \u1edf \u0111\u00e2u\", \"\u3053\u3093\u306b\u3061\u306f\", \"\u0928\u092e\u0938\u094d\u0924\u0947\", \"\u0532\u0561\u0580\u0565\u0582\", \"\u0417\u0434\u0440\u0430\u0432\u0435\u0439\u0442\u0435\")</p>"},{"location":"docs/generator/data-generator/#sample","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field()\n      .name(\"customer_name\")\n      .type(StringType.instance())\n      .expression(\"#{Name.name}\")\n      .enableNull(true)\n      .nullProbability(0.1)\n      .minLength(4)\n      .maxLength(20),\n    field()\n      .name(\"account_id\")\n      .type(StringType.instance())\n      .regex(\"ACC[0-9]{10}\")\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field()\n      .name(\"status\")\n      .type(StringType.instance())\n      .oneOf(\"open\", \"closed\", \"suspended\"),\n    field()\n      .name(\"priority\")\n      .type(StringType.instance())\n      .oneOf(\"high-&gt;0.1\", \"medium-&gt;0.7\", \"low-&gt;0.2\"),\n    field()\n      .name(\"user_uuid\")\n      .type(StringType.instance())\n      .uuid(\"user_id\"),\n    field()\n      .name(\"address\")\n      .type(StringType.instance())\n      .expression(\"#{Address.city}/#{Demographic.maritalStatus}\")\n      .minLength(10)\n      .maxLength(50),\n    field()\n      .name(\"calculated_field\")\n      .type(StringType.instance())\n      .sql(\"CASE WHEN amount &lt; 10 THEN 'small' ELSE 'large' END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field\n      .name(\"customer_name\")\n      .`type`(StringType)\n      .expression(\"#{Name.name}\")\n      .enableNull(true)\n      .nullProbability(0.1)\n      .minLength(4)\n      .maxLength(20),\n    field\n      .name(\"account_id\")\n      .`type`(StringType)\n      .regex(\"ACC[0-9]{10}\")\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field\n      .name(\"status\")\n      .`type`(StringType)\n      .oneOf(\"open\", \"closed\", \"suspended\"),\n    field\n      .name(\"priority\")\n      .`type`(StringType)\n      .oneOf(\"high-&gt;0.1\", \"medium-&gt;0.7\", \"low-&gt;0.2\"),\n    field\n      .name(\"user_uuid\")\n      .`type`(StringType)\n      .uuid(\"user_id\"),\n    field\n      .name(\"address\")\n      .`type`(StringType)\n      .expression(\"#{Address.city}/#{Demographic.maritalStatus}\")\n      .minLength(10)\n      .maxLength(50),\n    field\n      .name(\"calculated_field\")\n      .`type`(StringType)\n      .sql(\"CASE WHEN amount &lt; 10 THEN 'small' ELSE 'large' END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"customer_name\"\n        type: \"string\"\n        options:\n          expression: \"#{Name.name}\"\n          enableNull: true\n          nullProb: 0.1\n          minLen: 4\n          maxLen: 20\n      - name: \"account_id\"\n        type: \"string\"\n        options:\n          regex: \"ACC[0-9]{10}\"\n          isUnique: true\n          enableEdgeCase: true\n          edgeCaseProb: 0.05\n      - name: \"status\"\n        type: \"string\"\n        options:\n          oneOf: [\"open\", \"closed\", \"suspended\"]\n      - name: \"priority\"\n        type: \"string\"\n        options:\n          oneOf: [\"high-&gt;0.1\", \"medium-&gt;0.7\", \"low-&gt;0.2\"]\n      - name: \"user_uuid\"\n        type: \"string\"\n        options:\n          uuid: \"user_id\"\n      - name: \"address\"\n        type: \"string\"\n        options:\n          expression: \"#{Address.city}/#{Demographic.maritalStatus}\"\n          minLen: 10\n          maxLen: 50\n      - name: \"calculated_field\"\n        type: \"string\"\n        options:\n          sql: \"CASE WHEN amount &lt; 10 THEN 'small' ELSE 'large' END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#numeric","title":"Numeric","text":"<p>For all the numeric data types, there are 4 options to choose from: min, max and maxValue. Generally speaking, you only need to define one of min or minValue, similarly with max or maxValue. The reason why there are 2 options for each is because of when metadata is automatically gathered, we gather the statistics of the observed min and max values. Also, it will attempt to gather any restriction on the min or max value as defined by the data source (i.e. max value as per database type).</p>"},{"location":"docs/generator/data-generator/#integerlongshort","title":"Integer/Long/Short","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <code>distribution</code> <code>distribution: \"exponential\"</code> Type of distribution of the data. Either <code>exponential</code> or <code>normal</code> <code>distributionRateParam</code> <code>distributionRateParam: \"1.0\"</code> If distribution is <code>exponential</code>, rate parameter to adjust exponential distribution <code>incremental</code> 1 <code>incremental: \"1\"</code> Values will be incremental. Define a start number to increment from <p>Edge cases Integer: (2147483647, -2147483648, 0) Edge cases Long: (9223372036854775807, -9223372036854775808, 0) Edge cases Short: (32767, -32768, 0)</p>"},{"location":"docs/generator/data-generator/#sample_1","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"year\").type(IntegerType.instance()).min(2020).max(2023),\n    field().name(\"customer_id\").type(LongType.instance()).incremental(1000),\n    field().name(\"customer_group\").type(ShortType.instance()).oneOf(\"1\", \"2\", \"3\"),\n    field().name(\"transaction_amount\").type(IntegerType.instance())\n      .min(1).max(1000)\n      .distribution(\"normal\")\n      .mean(100.0)\n      .stddev(25.0),\n    field().name(\"retry_count\").type(IntegerType.instance())\n      .min(0).max(10)\n      .distribution(\"exponential\")\n      .distributionRateParam(2.0),\n    field().name(\"unique_sequence\").type(IntegerType.instance())\n      .min(1).max(99999)\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.1),\n    field().name(\"priority_level\").type(ShortType.instance())\n      .oneOf(\"high-&gt;1\", \"medium-&gt;2\", \"low-&gt;3\")\n      .regex(\"[1-3]\"),\n    field().name(\"calculated_score\").type(IntegerType.instance())\n      .sql(\"CASE WHEN transaction_amount &gt; 500 THEN 100 ELSE 50 END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"year\").`type`(IntegerType).min(2020).max(2023),\n    field.name(\"customer_id\").`type`(LongType).incremental(1000),\n    field.name(\"customer_group\").`type`(ShortType).oneOf(\"1\", \"2\", \"3\"),\n    field.name(\"transaction_amount\").`type`(IntegerType)\n      .min(1).max(1000)\n      .distribution(\"normal\")\n      .mean(100.0)\n      .stddev(25.0),\n    field.name(\"retry_count\").`type`(IntegerType)\n      .min(0).max(10)\n      .distribution(\"exponential\")\n      .distributionRateParam(2.0),\n    field.name(\"unique_sequence\").`type`(IntegerType)\n      .min(1).max(99999)\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.1),\n    field.name(\"priority_level\").`type`(ShortType)\n      .oneOf(\"high-&gt;1\", \"medium-&gt;2\", \"low-&gt;3\")\n      .regex(\"[1-3]\"),\n    field.name(\"calculated_score\").`type`(IntegerType)\n      .sql(\"CASE WHEN transaction_amount &gt; 500 THEN 100 ELSE 50 END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"year\"\n        type: \"integer\"\n        options:\n          min: 2020\n          max: 2023\n      - name: \"customer_id\"\n        type: \"long\"\n        options:\n          incremental: 1000\n      - name: \"customer_group\"\n        type: \"short\"\n        options:\n          oneOf: [\"1\", \"2\", \"3\"]\n      - name: \"transaction_amount\"\n        type: \"integer\"\n        options:\n          min: 1\n          max: 1000\n          distribution: \"normal\"\n          mean: 100.0\n          stddev: 25.0\n      - name: \"retry_count\"\n        type: \"integer\"\n        options:\n          min: 0\n          max: 10\n          distribution: \"exponential\"\n          distributionRateParam: 2.0\n      - name: \"unique_sequence\"\n        type: \"integer\"\n        options:\n          min: 1\n          max: 99999\n          isUnique: true\n          enableEdgeCase: true\n          edgeCaseProb: 0.1\n      - name: \"priority_level\"\n        type: \"short\"\n        options:\n          oneOf: [\"high-&gt;1\", \"medium-&gt;2\", \"low-&gt;3\"]\n          regex: \"[1-3]\"\n      - name: \"calculated_score\"\n        type: \"integer\"\n        options:\n          sql: \"CASE WHEN transaction_amount &gt; 500 THEN 100 ELSE 50 END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#decimal","title":"Decimal","text":"Option Default Example Description <code>min</code> 0 <code>min: \"2\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000 <code>max: \"25\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <code>numericPrecision</code> 10 <code>precision: \"25\"</code> The maximum number of digits <code>numericScale</code> 0 <code>scale: \"25\"</code> The number of digits on the right side of the decimal point (has to be less than or equal to precision) <code>distribution</code> <code>distribution: \"exponential\"</code> Type of distribution of the data. Either <code>exponential</code> or <code>normal</code> <code>distributionRateParam</code> <code>distributionRateParam: \"1.0\"</code> If distribution is <code>exponential</code>, rate parameter to adjust exponential distribution <p>Edge cases Decimal: (9223372036854775807, -9223372036854775808, 0)</p>"},{"location":"docs/generator/data-generator/#sample_2","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"account_balance\").type(DecimalType.instance())\n      .numericPrecision(10).numericScale(2)\n      .min(new BigDecimal(\"0.00\")).max(new BigDecimal(\"99999.99\")),\n    field().name(\"interest_rate\").type(DecimalType.instance())\n      .numericPrecision(5).numericScale(4)\n      .min(new BigDecimal(\"0.0001\")).max(new BigDecimal(\"0.2500\"))\n      .distribution(\"normal\")\n      .mean(0.05)\n      .stddev(0.02),\n    field().name(\"commission_rate\").type(DecimalType.instance())\n      .numericPrecision(6).numericScale(3)\n      .oneOf(\"0.025\", \"0.050\", \"0.075\"),\n    field().name(\"bonus_multiplier\").type(DecimalType.instance())\n      .numericPrecision(3).numericScale(1)\n      .distribution(\"exponential\")\n      .distributionRateParam(1.5)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field().name(\"unique_transaction_fee\").type(DecimalType.instance())\n      .numericPrecision(8).numericScale(2)\n      .min(new BigDecimal(\"1.00\")).max(new BigDecimal(\"999.99\"))\n      .isUnique(true),\n    field().name(\"calculated_total\").type(DecimalType.instance())\n      .numericPrecision(12).numericScale(2)\n      .sql(\"CASE WHEN account_balance &gt; 1000 THEN account_balance * 1.1 ELSE account_balance END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"account_balance\").`type`(DecimalType)\n      .numericPrecision(10).numericScale(2)\n      .min(new java.math.BigDecimal(\"0.00\")).max(new java.math.BigDecimal(\"99999.99\")),\n    field.name(\"interest_rate\").`type`(DecimalType)\n      .numericPrecision(5).numericScale(4)\n      .min(new java.math.BigDecimal(\"0.0001\")).max(new java.math.BigDecimal(\"0.2500\"))\n      .distribution(\"normal\")\n      .mean(0.05)\n      .stddev(0.02),\n    field.name(\"commission_rate\").`type`(DecimalType)\n      .numericPrecision(6).numericScale(3)\n      .oneOf(\"0.025\", \"0.050\", \"0.075\"),\n    field.name(\"bonus_multiplier\").`type`(DecimalType)\n      .numericPrecision(3).numericScale(1)\n      .distribution(\"exponential\")\n      .distributionRateParam(1.5)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field.name(\"unique_transaction_fee\").`type`(DecimalType)\n      .numericPrecision(8).numericScale(2)\n      .min(new java.math.BigDecimal(\"1.00\")).max(new java.math.BigDecimal(\"999.99\"))\n      .isUnique(true),\n    field.name(\"calculated_total\").`type`(DecimalType)\n      .numericPrecision(12).numericScale(2)\n      .sql(\"CASE WHEN account_balance &gt; 1000 THEN account_balance * 1.1 ELSE account_balance END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"account_balance\"\n        type: \"decimal\"\n        options:\n          precision: 10\n          scale: 2\n          min: \"0.00\"\n          max: \"99999.99\"\n      - name: \"interest_rate\"\n        type: \"decimal\"\n        options:\n          precision: 5\n          scale: 4\n          min: \"0.0001\"\n          max: \"0.2500\"\n          distribution: \"normal\"\n          mean: 0.05\n          stddev: 0.02\n      - name: \"commission_rate\"\n        type: \"decimal\"\n        options:\n          precision: 6\n          scale: 3\n          oneOf: [\"0.025\", \"0.050\", \"0.075\"]\n      - name: \"bonus_multiplier\"\n        type: \"decimal\"\n        options:\n          precision: 3\n          scale: 1\n          distribution: \"exponential\"\n          distributionRateParam: 1.5\n          enableEdgeCase: true\n          edgeCaseProb: 0.05\n      - name: \"unique_transaction_fee\"\n        type: \"decimal\"\n        options:\n          precision: 8\n          scale: 2\n          min: \"1.00\"\n          max: \"999.99\"\n          isUnique: true\n      - name: \"calculated_total\"\n        type: \"decimal\"\n        options:\n          precision: 12\n          scale: 2\n          sql: \"CASE WHEN account_balance &gt; 1000 THEN account_balance * 1.1 ELSE account_balance END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#doublefloat","title":"Double/Float","text":"Option Default Example Description <code>min</code> 0.0 <code>min: \"2.1\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> 1000.0 <code>max: \"25.9\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>round</code> N/A <code>round: \"2\"</code> Round to particular number of decimal places <code>stddev</code> 1.0 <code>stddev: \"2.0\"</code> Standard deviation for normal distributed data <code>mean</code> <code>max - min</code> <code>mean: \"5.0\"</code> Mean for normal distributed data <code>round</code> <code>round: \"2\"</code> Number of decimal places to round to (round up) <code>distribution</code> <code>distribution: \"exponential\"</code> Type of distribution of the data. Either <code>exponential</code> or <code>normal</code> <code>distributionRateParam</code> <code>distributionRateParam: \"1.0\"</code> If distribution is <code>exponential</code>, rate parameter to adjust exponential distribution <p>Edge cases Double: (+infinity, 1.7976931348623157e+308, 4.9e-324, 0.0, -0.0, -1.7976931348623157e+308, -infinity, NaN) Edge cases Float: (+infinity, 3.4028235e+38, 1.4e-45, 0.0, -0.0, -3.4028235e+38, -infinity, NaN)</p>"},{"location":"docs/generator/data-generator/#sample_3","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"transaction_amount\").type(DoubleType.instance())\n      .min(1.0).max(10000.0)\n      .round(2),\n    field().name(\"processing_fee\").type(FloatType.instance())\n      .min(0.5f).max(99.99f)\n      .round(2)\n      .distribution(\"normal\")\n      .mean(5.0)\n      .stddev(2.0),\n    field().name(\"exchange_rate\").type(DoubleType.instance())\n      .min(0.1).max(5.0)\n      .round(4)\n      .distribution(\"exponential\")\n      .distributionRateParam(0.8),\n    field().name(\"discount_percentage\").type(FloatType.instance())\n      .oneOf(\"0.05\", \"0.10\", \"0.15\", \"0.25\"),\n    field().name(\"unique_score\").type(DoubleType.instance())\n      .min(0.0).max(100.0)\n      .round(3)\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.02),\n    field().name(\"weighted_value\").type(DoubleType.instance())\n      .oneOf(\"low-&gt;10.5\", \"medium-&gt;25.75\", \"high-&gt;50.0\")\n      .round(2),\n    field().name(\"calculated_ratio\").type(FloatType.instance())\n      .sql(\"CASE WHEN transaction_amount &gt; 1000 THEN 1.5 ELSE 1.0 END\")\n      .round(1)\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"transaction_amount\").`type`(DoubleType)\n      .min(1.0).max(10000.0)\n      .round(2),\n    field.name(\"processing_fee\").`type`(FloatType)\n      .min(0.5f).max(99.99f)\n      .round(2)\n      .distribution(\"normal\")\n      .mean(5.0)\n      .stddev(2.0),\n    field.name(\"exchange_rate\").`type`(DoubleType)\n      .min(0.1).max(5.0)\n      .round(4)\n      .distribution(\"exponential\")\n      .distributionRateParam(0.8),\n    field.name(\"discount_percentage\").`type`(FloatType)\n      .oneOf(\"0.05\", \"0.10\", \"0.15\", \"0.25\"),\n    field.name(\"unique_score\").`type`(DoubleType)\n      .min(0.0).max(100.0)\n      .round(3)\n      .isUnique(true)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.02),\n    field.name(\"weighted_value\").`type`(DoubleType)\n      .oneOf(\"low-&gt;10.5\", \"medium-&gt;25.75\", \"high-&gt;50.0\")\n      .round(2),\n    field.name(\"calculated_ratio\").`type`(FloatType)\n      .sql(\"CASE WHEN transaction_amount &gt; 1000 THEN 1.5 ELSE 1.0 END\")\n      .round(1)\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"transaction_amount\"\n        type: \"double\"\n        options:\n          min: 1.0\n          max: 10000.0\n          round: 2\n      - name: \"processing_fee\"\n        type: \"float\"\n        options:\n          min: 0.5\n          max: 99.99\n          round: 2\n          distribution: \"normal\"\n          mean: 5.0\n          stddev: 2.0\n      - name: \"exchange_rate\"\n        type: \"double\"\n        options:\n          min: 0.1\n          max: 5.0\n          round: 4\n          distribution: \"exponential\"\n          distributionRateParam: 0.8\n      - name: \"discount_percentage\"\n        type: \"float\"\n        options:\n          oneOf: [\"0.05\", \"0.10\", \"0.15\", \"0.25\"]\n      - name: \"unique_score\"\n        type: \"double\"\n        options:\n          min: 0.0\n          max: 100.0\n          round: 3\n          isUnique: true\n          enableEdgeCase: true\n          edgeCaseProb: 0.02\n      - name: \"weighted_value\"\n        type: \"double\"\n        options:\n          oneOf: [\"low-&gt;10.5\", \"medium-&gt;25.75\", \"high-&gt;50.0\"]\n          round: 2\n      - name: \"calculated_ratio\"\n        type: \"float\"\n        options:\n          sql: \"CASE WHEN transaction_amount &gt; 1000 THEN 1.5 ELSE 1.0 END\"\n          round: 1\n</code></pre>"},{"location":"docs/generator/data-generator/#date","title":"Date","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01, 1582-10-15, 1970-01-01, 9999-12-31) (reference)</p>"},{"location":"docs/generator/data-generator/#sample_4","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"created_date\").type(DateType.instance())\n      .min(java.sql.Date.valueOf(\"2020-01-01\"))\n      .max(java.sql.Date.valueOf(\"2023-12-31\")),\n    field().name(\"birth_date\").type(DateType.instance())\n      .min(java.sql.Date.valueOf(\"1950-01-01\"))\n      .max(java.sql.Date.valueOf(\"2005-12-31\"))\n      .enableNull(true)\n      .nullProbability(0.05),\n    field().name(\"expiry_date\").type(DateType.instance())\n      .oneOf(\"2024-01-01\", \"2024-06-01\", \"2024-12-31\"),\n    field().name(\"random_date_with_edges\").type(DateType.instance())\n      .min(java.sql.Date.valueOf(\"2022-01-01\"))\n      .max(java.sql.Date.valueOf(\"2024-01-01\"))\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.1),\n    field().name(\"unique_event_date\").type(DateType.instance())\n      .min(java.sql.Date.valueOf(\"2023-01-01\"))\n      .max(java.sql.Date.valueOf(\"2023-12-31\"))\n      .isUnique(true),\n    field().name(\"calculated_date\").type(DateType.instance())\n      .sql(\"CASE WHEN created_date &lt; '2022-01-01' THEN '2022-01-01' ELSE created_date END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"created_date\").`type`(DateType)\n      .min(java.sql.Date.valueOf(\"2020-01-01\"))\n      .max(java.sql.Date.valueOf(\"2023-12-31\")),\n    field.name(\"birth_date\").`type`(DateType)\n      .min(java.sql.Date.valueOf(\"1950-01-01\"))\n      .max(java.sql.Date.valueOf(\"2005-12-31\"))\n      .enableNull(true)\n      .nullProbability(0.05),\n    field.name(\"expiry_date\").`type`(DateType)\n      .oneOf(\"2024-01-01\", \"2024-06-01\", \"2024-12-31\"),\n    field.name(\"random_date_with_edges\").`type`(DateType)\n      .min(java.sql.Date.valueOf(\"2022-01-01\"))\n      .max(java.sql.Date.valueOf(\"2024-01-01\"))\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.1),\n    field.name(\"unique_event_date\").`type`(DateType)\n      .min(java.sql.Date.valueOf(\"2023-01-01\"))\n      .max(java.sql.Date.valueOf(\"2023-12-31\"))\n      .isUnique(true),\n    field.name(\"calculated_date\").`type`(DateType)\n      .sql(\"CASE WHEN created_date &lt; '2022-01-01' THEN '2022-01-01' ELSE created_date END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"created_date\"\n        type: \"date\"\n        options:\n          min: \"2020-01-01\"\n          max: \"2023-12-31\"\n      - name: \"birth_date\"\n        type: \"date\"\n        options:\n          min: \"1950-01-01\"\n          max: \"2005-12-31\"\n          enableNull: true\n          nullProb: 0.05\n      - name: \"expiry_date\"\n        type: \"date\"\n        options:\n          oneOf: [\"2024-01-01\", \"2024-06-01\", \"2024-12-31\"]\n      - name: \"random_date_with_edges\"\n        type: \"date\"\n        options:\n          min: \"2022-01-01\"\n          max: \"2024-01-01\"\n          enableEdgeCase: true\n          edgeCaseProb: 0.1\n      - name: \"unique_event_date\"\n        type: \"date\"\n        options:\n          min: \"2023-01-01\"\n          max: \"2023-12-31\"\n          isUnique: true\n      - name: \"calculated_date\"\n        type: \"date\"\n        options:\n          sql: \"CASE WHEN created_date &lt; '2022-01-01' THEN '2022-01-01' ELSE created_date END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#timestamp","title":"Timestamp","text":"Option Default Example Description <code>min</code> now() - 365 days <code>min: \"2023-01-31 23:10:10\"</code> Ensures that all generated values are greater than or equal to <code>min</code> <code>max</code> now() <code>max: \"2023-12-31 23:10:10\"</code> Ensures that all generated values are less than or equal to <code>max</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (0001-01-01 00:00:00, 1582-10-15 23:59:59, 1970-01-01 00:00:00, 9999-12-31 23:59:59)</p>"},{"location":"docs/generator/data-generator/#sample_5","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"created_time\").type(TimestampType.instance())\n      .min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\")),\n    field().name(\"last_login\").type(TimestampType.instance())\n      .min(java.sql.Timestamp.valueOf(\"2023-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\"))\n      .enableNull(true)\n      .nullProbability(0.2),\n    field().name(\"scheduled_time\").type(TimestampType.instance())\n      .oneOf(\"2024-01-01 09:00:00\", \"2024-01-01 12:00:00\", \"2024-01-01 17:00:00\"),\n    field().name(\"event_timestamp\").type(TimestampType.instance())\n      .min(java.sql.Timestamp.valueOf(\"2023-06-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\"))\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field().name(\"unique_activity_time\").type(TimestampType.instance())\n      .min(java.sql.Timestamp.valueOf(\"2023-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-01-31 23:59:59\"))\n      .isUnique(true),\n    field().name(\"calculated_timestamp\").type(TimestampType.instance())\n      .sql(\"CASE WHEN created_time &lt; '2022-01-01 00:00:00' THEN '2022-01-01 00:00:00' ELSE created_time END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"created_time\").`type`(TimestampType)\n      .min(java.sql.Timestamp.valueOf(\"2020-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\")),\n    field.name(\"last_login\").`type`(TimestampType)\n      .min(java.sql.Timestamp.valueOf(\"2023-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\"))\n      .enableNull(true)\n      .nullProbability(0.2),\n    field.name(\"scheduled_time\").`type`(TimestampType)\n      .oneOf(\"2024-01-01 09:00:00\", \"2024-01-01 12:00:00\", \"2024-01-01 17:00:00\"),\n    field.name(\"event_timestamp\").`type`(TimestampType)\n      .min(java.sql.Timestamp.valueOf(\"2023-06-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-12-31 23:59:59\"))\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field.name(\"unique_activity_time\").`type`(TimestampType)\n      .min(java.sql.Timestamp.valueOf(\"2023-01-01 00:00:00\"))\n      .max(java.sql.Timestamp.valueOf(\"2023-01-31 23:59:59\"))\n      .isUnique(true),\n    field.name(\"calculated_timestamp\").`type`(TimestampType)\n      .sql(\"CASE WHEN created_time &lt; '2022-01-01 00:00:00' THEN '2022-01-01 00:00:00' ELSE created_time END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"created_time\"\n        type: \"timestamp\"\n        options:\n          min: \"2020-01-01 00:00:00\"\n          max: \"2023-12-31 23:59:59\"\n      - name: \"last_login\"\n        type: \"timestamp\"\n        options:\n          min: \"2023-01-01 00:00:00\"\n          max: \"2023-12-31 23:59:59\"\n          enableNull: true\n          nullProb: 0.2\n      - name: \"scheduled_time\"\n        type: \"timestamp\"\n        options:\n          oneOf: [\"2024-01-01 09:00:00\", \"2024-01-01 12:00:00\", \"2024-01-01 17:00:00\"]\n      - name: \"event_timestamp\"\n        type: \"timestamp\"\n        options:\n          min: \"2023-06-01 00:00:00\"\n          max: \"2023-12-31 23:59:59\"\n          enableEdgeCase: true\n          edgeCaseProb: 0.05\n      - name: \"unique_activity_time\"\n        type: \"timestamp\"\n        options:\n          min: \"2023-01-01 00:00:00\"\n          max: \"2023-01-31 23:59:59\"\n          isUnique: true\n      - name: \"calculated_timestamp\"\n        type: \"timestamp\"\n        options:\n          sql: \"CASE WHEN created_time &lt; '2022-01-01 00:00:00' THEN '2022-01-01 00:00:00' ELSE created_time END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#binary","title":"Binary","text":"Option Default Example Description <code>minLen</code> 1 <code>minLen: \"2\"</code> Ensures that all generated array of bytes have at least length <code>minLen</code> <code>maxLen</code> 20 <code>maxLen: \"15\"</code> Ensures that all generated array of bytes have at most length <code>maxLen</code> <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true <p>Edge cases: (\"\", \"\\n\", \"\\r\", \"\\t\", \" \", \"\\u0000\", \"\\ufff\", -128, 127)</p>"},{"location":"docs/generator/data-generator/#sample_6","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"message_payload\").type(BinaryType.instance())\n      .minLength(10)\n      .maxLength(100),\n    field().name(\"encrypted_data\").type(BinaryType.instance())\n      .minLength(32)\n      .maxLength(256)\n      .enableNull(true)\n      .nullProbability(0.1),\n    field().name(\"signature\").type(BinaryType.instance())\n      .minLength(64)\n      .maxLength(128)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field().name(\"unique_hash\").type(BinaryType.instance())\n      .minLength(32)\n      .maxLength(32)\n      .isUnique(true),\n    field().name(\"calculated_checksum\").type(BinaryType.instance())\n      .sql(\"CASE WHEN LENGTH(message_payload) &gt; 50 THEN UNHEX('DEADBEEF') ELSE UNHEX('CAFEBABE') END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"message_payload\").`type`(BinaryType)\n      .minLength(10)\n      .maxLength(100),\n    field.name(\"encrypted_data\").`type`(BinaryType)\n      .minLength(32)\n      .maxLength(256)\n      .enableNull(true)\n      .nullProbability(0.1),\n    field.name(\"signature\").`type`(BinaryType)\n      .minLength(64)\n      .maxLength(128)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field.name(\"unique_hash\").`type`(BinaryType)\n      .minLength(32)\n      .maxLength(32)\n      .isUnique(true),\n    field.name(\"calculated_checksum\").`type`(BinaryType)\n      .sql(\"CASE WHEN LENGTH(message_payload) &gt; 50 THEN UNHEX('DEADBEEF') ELSE UNHEX('CAFEBABE') END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"message_payload\"\n        type: \"binary\"\n        options:\n          minLen: 10\n          maxLen: 100\n      - name: \"encrypted_data\"\n        type: \"binary\"\n        options:\n          minLen: 32\n          maxLen: 256\n          enableNull: true\n          nullProb: 0.1\n      - name: \"signature\"\n        type: \"binary\"\n        options:\n          minLen: 64\n          maxLen: 128\n          enableEdgeCase: true\n          edgeCaseProb: 0.05\n      - name: \"unique_hash\"\n        type: \"binary\"\n        options:\n          minLen: 32\n          maxLen: 32\n          isUnique: true\n      - name: \"calculated_checksum\"\n        type: \"binary\"\n        options:\n          sql: \"CASE WHEN LENGTH(message_payload) &gt; 50 THEN UNHEX('DEADBEEF') ELSE UNHEX('CAFEBABE') END\"\n</code></pre>"},{"location":"docs/generator/data-generator/#array","title":"Array","text":"Option Default Example Description <code>arrayMinLen</code> 0 <code>arrayMinLen: \"2\"</code> Ensures that all generated arrays have at least length <code>arrayMinLen</code> <code>arrayMaxLen</code> 5 <code>arrayMaxLen: \"15\"</code> Ensures that all generated arrays have at most length <code>arrayMaxLen</code> <code>arrayType</code> <code>arrayType: \"double\"</code> Inner data type of the array. Optional when using Java/Scala API. Allows for nested data types to be defined like struct <code>enableNull</code> false <code>enableNull: \"true\"</code> Enable/disable null values being generated <code>nullProbability</code> 0.0 <code>nullProb: \"0.1\"</code> Probability to generate null values if <code>enableNull</code> is true"},{"location":"docs/generator/data-generator/#sample_7","title":"Sample","text":"JavaScalaYAML <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field().name(\"transaction_amounts\").type(ArrayType.instance())\n      .arrayType(\"double\")\n      .arrayMinLength(1)\n      .arrayMaxLength(10),\n    field().name(\"tags\").type(ArrayType.instance())\n      .arrayType(\"string\")\n      .arrayMinLength(0)\n      .arrayMaxLength(5)\n      .enableNull(true)\n      .nullProbability(0.1),\n    field().name(\"priority_scores\").type(ArrayType.instance())\n      .arrayType(\"integer\")\n      .arrayMinLength(3)\n      .arrayMaxLength(3)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field().name(\"unique_identifiers\").type(ArrayType.instance())\n      .arrayType(\"string\")\n      .arrayMinLength(2)\n      .arrayMaxLength(8)\n      .isUnique(true),\n    field().name(\"calculated_values\").type(ArrayType.instance())\n      .arrayType(\"double\")\n      .sql(\"CASE WHEN SIZE(transaction_amounts) &gt; 3 THEN ARRAY(1.0, 2.0, 3.0) ELSE ARRAY(0.5, 1.0) END\")\n  );\n</code></pre> <pre><code>csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"transaction_amounts\").`type`(ArrayType)\n      .arrayType(\"double\")\n      .arrayMinLength(1)\n      .arrayMaxLength(10),\n    field.name(\"tags\").`type`(ArrayType)\n      .arrayType(\"string\")\n      .arrayMinLength(0)\n      .arrayMaxLength(5)\n      .enableNull(true)\n      .nullProbability(0.1),\n    field.name(\"priority_scores\").`type`(ArrayType)\n      .arrayType(\"integer\")\n      .arrayMinLength(3)\n      .arrayMaxLength(3)\n      .enableEdgeCase(true)\n      .edgeCaseProbability(0.05),\n    field.name(\"unique_identifiers\").`type`(ArrayType)\n      .arrayType(\"string\")\n      .arrayMinLength(2)\n      .arrayMaxLength(8)\n      .isUnique(true),\n    field.name(\"calculated_values\").`type`(ArrayType)\n      .arrayType(\"double\")\n      .sql(\"CASE WHEN SIZE(transaction_amounts) &gt; 3 THEN ARRAY(1.0, 2.0, 3.0) ELSE ARRAY(0.5, 1.0) END\")\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"app/src/test/resources/sample/csv/transactions\"\n    fields:\n      - name: \"transaction_amounts\"\n        type: \"array&lt;double&gt;\"\n        options:\n          arrayMinLen: 1\n          arrayMaxLen: 10\n      - name: \"tags\"\n        type: \"array&lt;string&gt;\"\n        options:\n          arrayMinLen: 0\n          arrayMaxLen: 5\n          enableNull: true\n          nullProb: 0.1\n      - name: \"priority_scores\"\n        type: \"array&lt;integer&gt;\"\n        options:\n          arrayMinLen: 3\n          arrayMaxLen: 3\n          enableEdgeCase: true\n          edgeCaseProb: 0.05\n      - name: \"unique_identifiers\"\n        type: \"array&lt;string&gt;\"\n        options:\n          arrayMinLen: 2\n          arrayMaxLen: 8\n          isUnique: true\n      - name: \"calculated_values\"\n        type: \"array&lt;double&gt;\"\n        options:\n          sql: \"CASE WHEN SIZE(transaction_amounts) &gt; 3 THEN ARRAY(1.0, 2.0, 3.0) ELSE ARRAY(0.5, 1.0) END\"\n</code></pre>"},{"location":"docs/generator/foreign-key/","title":"Relationships/Foreign Keys","text":"<p>Foreign keys can be defined to represent the relationships between datasets where values are required to match for particular fields.</p>"},{"location":"docs/generator/foreign-key/#single-field","title":"Single field","text":"<p>Define a field in one data source to match against another field. Below example shows a <code>postgres</code> data source with two tables, <code>accounts</code> and <code>transactions</code> that have a foreign key for <code>account_id</code>.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"name\"),\n    ...\n  );\nvar postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"full_name\"),\n    ...\n  );\n\nplan().addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List.of(Map.entry(postgresTxn, \"account_id\"))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"name\"),\n    ...\n  )\nval postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"full_name\"),\n    ...\n  )\n\nplan.addForeignKeyRelationship(\n  postgresAcc, \"account_id\",\n  List(postgresTxn -&gt; \"account_id\")\n)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_id\"\n      - name: \"name\"\n  - name: \"transactions\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"full_name\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"postgres\"\n        step: \"accounts\"\n        fields: [\"account_id\"]\n      generate:\n        - dataSource: \"postgres\"\n          step: \"transactions\"\n          fields: [\"account_id\"]\n</code></pre>"},{"location":"docs/generator/foreign-key/#multiple-fields","title":"Multiple fields","text":"<p>You may have a scenario where multiple fields need to be aligned. From the same example, we want <code>account_id</code> and <code>name</code> from <code>accounts</code> to match with <code>account_id</code> and <code>full_name</code> to match in <code>transactions</code> respectively.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"name\"),\n    ...\n  );\nvar postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"full_name\"),\n    ...\n  );\n\nplan().addForeignKeyRelationship(\n  postgresAcc, List.of(\"account_id\", \"name\"),\n  List.of(Map.entry(postgresTxn, List.of(\"account_id\", \"full_name\")))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"name\"),\n    ...\n  )\nval postgresTxn = postgres(postgresAcc)\n  .table(\"public.transactions\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"full_name\"),\n    ...\n  )\n\nplan.addForeignKeyRelationship(\n  postgresAcc, List(\"account_id\", \"name\"),\n  List(postgresTxn -&gt; List(\"account_id\", \"full_name\"))\n)\n</code></pre> <pre><code>---\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_id\"\n      - name: \"name\"\n  - name: \"transactions\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"full_name\"\n---\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"postgres\"\n        step: \"accounts\"\n        fields: [\"account_id\", \"name\"]\n      generate:\n        - dataSource: \"postgres\"\n          step: \"transactions\"\n          fields: [\"account_id\", \"full_name\"]\n</code></pre>"},{"location":"docs/generator/foreign-key/#transformed-field","title":"Transformed field","text":"<p>Scenarios exist where there are relationships defined by certain transformations being applied to the source data.</p> <p>For example, there may be accounts created with a field <code>account_number</code> that contains records like <code>123456</code>. Then another data source contains <code>account_id</code> which is a concatenation of <code>ACC</code> with <code>account_number</code> to have values like <code>ACC123456</code>.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_number\"),\n    field().name(\"name\"),\n    ...\n  );\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n  .fields(\n    field().name(\"account_id\").sql(\"CONCAT('ACC', account_number)\"),\n    field().name(\"account_number\").omit(true),  #using this field for intermediate calculation, not included in final result with omit=true\n    ...\n  );\n\nplan().addForeignKeyRelationship(\n  postgresAcc, List.of(\"account_number\"),\n  List.of(Map.entry(jsonTask, List.of(\"account_number\")))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_number\"),\n    field.name(\"name\"),\n    ...\n  )\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n  .fields(\n    field.name(\"account_id\").sql(\"CONCAT('ACC', account_number)\"),\n    field.name(\"account_number\").omit(true),  #using this field for intermediate calculation, not included in final result with omit=true\n    ...\n  )\n\nplan.addForeignKeyRelationship(\n  postgresAcc, List(\"account_number\"),\n  List(jsonTask -&gt; List(\"account_number\"))\n)\n</code></pre> <pre><code>---\n#postgres task yaml\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_number\"\n      - name: \"name\"\n---\n#json task yaml\nname: \"json_data\"\nsteps:\n  - name: \"transactions\"\n    type: \"json\"\n    options:\n      dbtable: \"account.transactions\"\n    fields:\n      - name: \"account_id\"\n        options:\n          sql: \"CONCAT('ACC', account_number)\"\n      - name: \"account_number\"\n        options:\n          omit: true\n\n---\n#plan yaml\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n  - name: \"json_data\"\n    dataSourceName: \"my_json\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"my_postgres\"\n        step: \"accounts\"\n        fields: [\"account_number\"]\n      generate:\n        - dataSource: \"my_json\"\n          step: \"transactions\"\n          fields: [\"account_number\"]\n</code></pre>"},{"location":"docs/generator/foreign-key/#nested-field","title":"Nested field","text":"<p>Your schema structure can have nested fields which can also be referenced as foreign keys. But to do so, you need to create a proxy field that gets omitted from the final saved data.</p> <p>In the example below, the nested <code>customer_details.name</code> field inside the <code>json</code> task needs to match with <code>name</code> from <code>postgres</code>. A new field in the <code>json</code> called <code>_txn_name</code> is used as a temporary field to facilitate the foreign key definition.</p> JavaScalaYAML <pre><code>var postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"name\"),\n    ...\n  );\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").sql(\"_txn_name\"), #nested field will get value from '_txn_name'\n        ...\n      ),\n    field().name(\"_txn_name\").omit(true)       #value will not be included in output\n  );\n\nplan().addForeignKeyRelationship(\n  postgresAcc, List.of(\"account_id\", \"name\"),\n  List.of(Map.entry(jsonTask, List.of(\"account_id\", \"_txn_name\")))\n);\n</code></pre> <pre><code>val postgresAcc = postgres(\"my_postgres\", \"jdbc:...\")\n  .table(\"public.accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"name\"),\n    ...\n  )\nvar jsonTask = json(\"my_json\", \"/tmp/json\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").sql(\"_txn_name\"), #nested field will get value from '_txn_name'\n        ...\n      ), \n    field.name(\"_txn_name\").omit(true)       #value will not be included in output\n  )\n\nplan.addForeignKeyRelationship(\n  postgresAcc, List(\"account_id\", \"name\"),\n  List(jsonTask -&gt; List(\"account_id\", \"_txn_name\"))\n)\n</code></pre> <pre><code>---\n#postgres task yaml\nname: \"postgres_data\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n      - name: \"account_id\"\n      - name: \"name\"\n---\n#json task yaml\nname: \"json_data\"\nsteps:\n  - name: \"transactions\"\n    type: \"json\"\n    options:\n      dbtable: \"account.transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"_txn_name\"\n        options:\n          omit: true\n      - name: \"cusotmer_details\"\n        fields:\n          name: \"name\"\n          options:\n            sql: \"_txn_name\"\n\n---\n#plan yaml\nname: \"customer_create_plan\"\ndescription: \"Create customers in JDBC\"\ntasks:\n  - name: \"postgres_data\"\n    dataSourceName: \"my_postgres\"\n  - name: \"json_data\"\n    dataSourceName: \"my_json\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"my_postgres\"\n        step: \"accounts\"\n        fields: [\"account_id\", \"name\"]\n      generate:\n        - dataSource: \"my_json\"\n          step: \"transactions\"\n          fields: [\"account_id\", \"_txn_name\"]\n</code></pre>"},{"location":"docs/generator/foreign-key/#ordering","title":"Ordering","text":"<p>When defining relationships/foreign keys, the order matters. The source of the foreign key is generated first, then the children  foreign keys are generated. This is to ensure that the source data is available for the children to reference.</p> <p>When using the HTTP data sources, it gives you the opportunity to define the order in which the requests are executed. For example, you want the following order:</p> <ul> <li>Create a pet with <code>id</code></li> <li>Get pet with <code>id</code></li> <li>Delete pet with <code>id</code></li> </ul> <p>Below is how you can define the order of the HTTP data sources.</p> JavaScalaYAML <pre><code>var httpTask = http(\"my_http\")\n        .fields(metadataSource().openApi(\"/opt/app/mount/http/petstore.json\"))\n        .count(count().records(2));\n\nvar myPlan = plan().addForeignKeyRelationship(\n        foreignField(\"my_http\", \"POST/pets\", \"body.id\"),\n        foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\"),\n        foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\")\n);\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(metadataSource.openApi(\"/opt/app/mount/http/petstore.json\"))\n  .count(count.records(2))\n\nval myPlan = plan.addForeignKeyRelationship(\n  foreignField(\"my_http\", \"POST/pets\", \"body.id\"),\n  foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\"),\n  foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\"),\n)\n</code></pre> <p>In <code>docker/data/custom/task/http/openapi-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    options:\n      metadataSourceType: \"openapi\"\n      schemaLocation: \"/opt/app/mount/http/petstore.json\"\n</code></pre></p> <p>In <code>docker/data/custom/plan/my-http.yaml</code>: <pre><code>name: \"my_http_plan\"\ndescription: \"Create pet data via HTTP from OpenAPI metadata\"\ntasks:\n  - name: \"http_task\"\n    dataSourceName: \"my_http\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"my_http\"\n        step: \"POST/pets\"\n        fields: [\"body.id\"]\n      generate:\n        - dataSource: \"my_http\"\n          step: \"GET/pets/{id}\"\n          fields: [\"pathParamid\"]\n        - dataSource: \"my_http\"\n          step: \"DELETE/pets/{id}\"\n          fields: [\"pathParamid\"]\n</code></pre></p>"},{"location":"docs/generator/foreign-key/#fast-relationships","title":"Fast Relationships","text":"<p>You may want to generate a large number of records whilst retaining relationships across datasets. This consumes a lot of memory as Data Caterer will keep track of generated values and will check for global uniqueness.</p> <p>There are some tactics that can be used to avoid defining a relationships but still maintain the same values across datasets by leveraging incremental values. When you define an incremental value, it will be globally unique across the  data generated for that field. Below is an example where you have <code>accounts</code> and <code>transactions</code> where the same <code>id</code> values should appear in both datasets.</p> JavaScalaYAML <pre><code>var accountTask = csv(\"accounts\", \"app/src/test/resources/sample/csv/accounts\")\n    .fields(\n        field().name(\"id\").type(LongType.instance()).incremental()\n    );\n\nvar transactionTask = csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n    .fields(\n        field().name(\"id\").type(LongType.instance()).incremental()\n    );\n\nvar config = configuration()\n  .enableCount(false)\n  .enableSinkMetadata(false)\n  .enableUniqueCheckOnlyInBatch(true);\n</code></pre> <pre><code>val accountTask = csv(\"accounts\", \"app/src/test/resources/sample/csv/accounts\")\n  .fields(\n    field.name(\"id\").`type`(LongType).incremental()\n  )\n\nval transactionTask = csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"id\").`type`(LongType).incremental()\n  )\n\nval config = configuration\n  .enableCount(false)\n  .enableSinkMetadata(false)\n  .enableUniqueCheckOnlyInBatch(true)\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"accounts\"\n    ...\n    fields:\n      - name: \"id\"\n        type: \"long\"\n        options:\n          incremental: 1\n  - name: \"transactions\"\n    ...\n    fields:\n      - name: \"id\"\n        type: \"long\"\n        options:\n          incremental: 1\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n    enableCount = false\n    enableCount = ${?ENABLE_COUNT}\n    enableSinkMetadata = false\n    enableSinkMetadata = ${?ENABLE_SINK_METADATA}\n    enableUniqueCheckOnlyInBatch = true\n    enableUniqueCheckOnlyInBatch = ${?ENABLE_UNIQUE_CHECK_ONLY_IN_BATCH}\n}\n</code></pre></p>"},{"location":"docs/generator/foreign-key/#uuid","title":"UUID","text":"<p>If you require UUID values to match across datasets, you can also leverage <code>incremental</code> with <code>uuid</code>.</p> JavaScalaYAML <pre><code>var accountTask = csv(\"accounts\", \"app/src/test/resources/sample/csv/accounts\")\n    .fields(\n        field().name(\"id\").incremental().uuid()\n    );\n\nvar transactionTask = csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n    .fields(\n        field().name(\"id\").incremental().uuid()\n    );\n</code></pre> <pre><code>val accountTask = csv(\"accounts\", \"app/src/test/resources/sample/csv/accounts\")\n  .fields(\n    field.name(\"id\").incremental().uuid()\n  )\n\nval transactionTask = csv(\"transactions\", \"app/src/test/resources/sample/csv/transactions\")\n  .fields(\n    field.name(\"id\").incremental().uuid()\n  )\n</code></pre> <pre><code>name: \"csv_file\"\nsteps:\n  - name: \"accounts\"\n    ...\n    fields:\n      - name: \"id\"\n        options:\n          incremental: 1\n          uuid: \"\"\n  - name: \"transactions\"\n    ...\n    fields:\n      - name: \"id\"\n        options:\n          incremental: 1\n          uuid: \"\"\n</code></pre>"},{"location":"docs/guide/","title":"Guides","text":"<p>Below are a list of guides you can follow to create your first Data Catering job for your use case.</p>"},{"location":"docs/guide/#data-sources","title":"Data Sources","text":""},{"location":"docs/guide/#databases","title":"Databases","text":"<ul> <li> BigQuery - Generate/validate data for BigQuery</li> <li> Cassandra - Generate/validate data for Cassandra</li> <li> MySQL - Generate/validate data for MySQL</li> <li> Postgres - Generate/validate data for Postgres</li> </ul>"},{"location":"docs/guide/#files","title":"Files","text":"<ul> <li> CSV - Generate/validate data for CSV</li> <li> Delta Lake - Generate/validate data for Delta Lake</li> <li> Iceberg - Generate/validate data for Iceberg tables</li> <li> JSON - Generate/validate data for JSON</li> <li> ORC - Generate/validate data for ORC</li> <li> Parquet - Generate/validate data for Parquet</li> </ul>"},{"location":"docs/guide/#http","title":"HTTP","text":"<ul> <li> REST API - Generate data for REST APIs</li> </ul>"},{"location":"docs/guide/#messaging","title":"Messaging","text":"<ul> <li> Kafka - Generate data for Kafka topics</li> <li> Rabbitmq - Generate data for Rabbitmq</li> <li> Solace - Generate data for Solace</li> </ul>"},{"location":"docs/guide/#metadata","title":"Metadata","text":"<ul> <li> Data Contract CLI - Generate data based on metadata in data contract files in Data Contract CLI format</li> <li> Great Expectations - Use validations from Great Expectations for testing</li> <li> Marquez - Generate data based on metadata in Marquez</li> <li> OpenMetadata - Generate data based on metadata in OpenMetadata</li> <li> Open Data Contract Standard (ODCS) - Generate data based on metadata in data contract files in ODCS format</li> </ul>"},{"location":"docs/guide/#scenarios","title":"Scenarios","text":"<ul> <li>Auto Generate From Data Connection - Automatically generating data from just defining data sources</li> <li>Data Generation - Generate production-like data</li> <li>Data Validations - Run data validations after generating data</li> <li>Delete Generated Data - Delete the generated data whilst leaving other data</li> <li>First Data Generation - If you are new, this is the place to start</li> <li>Foreign Keys Across Data Sources - Generate matching values across generated data sets</li> <li>Generate Batch and Event Data - Generate matching batch and event data</li> <li>Multiple Records Per Field Value - How you can generate multiple records per set of fields</li> </ul>"},{"location":"docs/guide/#yaml-files","title":"YAML Files","text":""},{"location":"docs/guide/#base-concept","title":"Base Concept","text":"<p>The execution of the data generator is based on the concept of plans and tasks. A plan represent the set of tasks that need to be executed, along with other information that spans across tasks, such as foreign keys between data sources. A task represent the component(s) of a data source and its associated metadata so that it understands what the data should look like and how many steps (sub data sources) there are (i.e. tables in a database, topics in Kafka). Tasks can define one or more steps.</p>"},{"location":"docs/guide/#plan","title":"Plan","text":""},{"location":"docs/guide/#foreign-keys","title":"Foreign Keys","text":"<p>Define foreign keys across data sources in your plan to ensure generated data can match Link to associated task 1 Link to associated task 2</p>"},{"location":"docs/guide/#task","title":"Task","text":"Data Source Type Data Source Sample Task Notes Database Postgres Sample Database MySQL Sample Database Cassandra Sample File CSV Sample File JSON Sample Contains nested schemas and use of SQL for generated values File Parquet Sample Partition by year field Messaging System Kafka Sample Specific base schema to be used, define headers, key, value, etc. Messaging System Solace Sample JSON formatted message HTTP PUT Sample JSON formatted PUT body"},{"location":"docs/guide/#configuration","title":"Configuration","text":"<p>Basic configuration</p>"},{"location":"docs/guide/#docker-compose","title":"Docker-compose","text":"<p>To see how it runs against different data sources, you can run using <code>docker-compose</code> and set <code>DATA_SOURCE</code> like below</p> <pre><code>./gradlew build\ncd docker\nDATA_SOURCE=postgres docker-compose up -d datacaterer\n</code></pre> <p>Can set it to one of the following:</p> <ul> <li>postgres</li> <li>mysql</li> <li>cassandra</li> <li>solace</li> <li>kafka</li> <li>http</li> </ul>"},{"location":"docs/guide/data-source/database/bigquery/","title":"BigQuery","text":"<p>Creating a data generator for BigQuery. You will build a Docker image that will be able to populate data in BigQuery for the tables you configure.</p>"},{"location":"docs/guide/data-source/database/bigquery/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> <li>Google Cloud Platform (GCP)<ul> <li>BigQuery</li> <li>Google Cloud Storage (GCS)</li> </ul> </li> </ul>"},{"location":"docs/guide/data-source/database/bigquery/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a BigQuery instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/database/bigquery/#bigquery-setup","title":"BigQuery Setup","text":"<p>Next, let's make sure you have an instance of BigQuery up and running in GCP. Make sure you have the following:</p> <ul> <li>Created a dataset</li> <li>Created a table<ul> <li><code>sql   CREATE TABLE IF NOT EXISTS data_caterer_test.accounts (     account_id STRING,     account_status STRING,     balance FLOAT64   )</code></li> </ul> </li> <li>Create a GCS bucket for temporary storage</li> </ul>"},{"location":"docs/guide/data-source/database/bigquery/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyBigQueryJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyBigQueryPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-bigquery.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyBigQueryJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyBigQueryPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-bigquery.yaml</code>: <pre><code>name: \"my_bigquery_plan\"\ndescription: \"Create account data via BigQuery\"\ntasks:\n  - name: \"bigquery_task\"\n    dataSourceName: \"my_bigquery\"\n</code></pre></p> BigQuery not supported in UI <p>BigQuery is not supported in the UI at this time. It will be added soon!</p> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/database/bigquery/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to BigQuery. Follows same configuration as defined by the Spark BigQuery Connector as found here. By default, it will use <code>indirect</code> as the <code>writeMethod</code> which involves writing to GCS first before loading into BigQuery.</p> JavaScalaYAMLUI <pre><code>bigquery(\n    \"customer_bigquery\",   //name\n    \"gs://my-test-bucket\", //temporaryGcsBucket\n    Map.of()               //optional additional connection options\n)\n</code></pre> <pre><code>bigquery(\n  \"customer_bigquery\",   //name\n  \"gs://my-test-bucket\", //temporaryGcsBucket\n  Map()                  //optional additional connection options\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>bigquery {\n    customer_bigquery {\n        temporaryGcsBucket = \"gs://my-test-bucket\"\n        temporaryGcsBucket = ${?BIGQUERY_TEMPORARY_GCS_BUCKET}\n    }\n}\n</code></pre></p> BigQuery not supported in UI <p>BigQuery is not supported in the UI at this time. It will be added soon!</p>"},{"location":"docs/guide/data-source/database/bigquery/#authentication","title":"Authentication","text":"<p>To setup authentication, follow the options found here.</p>"},{"location":"docs/guide/data-source/database/bigquery/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>account.accounts</code> and <code>account.balances</code> tables as defined under<code>docker/data/sql/bigquery/customer.cql</code>. This table should already be setup for you if you followed this step.</p> <p>Trimming the connection details to work with the docker-compose BigQuery, we have a base BigQuery connection to define the table and schema required. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code> which corresponds to <code>text</code> in BigQuery.</p> JavaScalaYAMLUI <pre><code>{\n    var accountTask = bigquery(\"customer_bigquery\", \"gs://&lt;my-bucket-name&gt;/temp-data-gen\")\n            .table(\"&lt;project&gt;.data_caterer_test.accounts\")\n            .fields(\n                    field().name(\"account_number\"),\n                    field().name(\"amount\").type(DoubleType.instance()),\n                    field().name(\"created_by\"),\n                    field().name(\"created_by_fixed_length\"),\n                    field().name(\"open_timestamp\").type(TimestampType.instance()),\n                    field().name(\"account_status\")\n            );\n}\n</code></pre> <pre><code>val accountTask = bigquery(\"customer_bigquery\", \"gs://&lt;my-bucket-name&gt;/temp-data-gen\")\n  .table(\"&lt;project&gt;.data_caterer_test.accounts\")\n  .fields(\n    field.name(\"account_number\"),\n    field.name(\"amount\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"created_by_fixed_length\"),\n    field.name(\"open_timestamp\").`type`(TimestampType),\n    field.name(\"account_status\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/bigquery/bigquery-task.yaml</code>: <pre><code>name: \"bigquery_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"bigquery\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n    - name: \"account_number\"\n    - name: \"amount\"\n      type: \"double\"\n    - name: \"created_by\"\n    - name: \"created_by_fixed_length\"\n    - name: \"open_timestamp\"\n      type: \"timestamp\"\n    - name: \"account_status\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>account_number</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>amount</code></li> <li>Click on <code>Select data type</code> and select <code>double</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by_fixed_length</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>open_timestamp</code></li> <li>Click on <code>Select data type</code> and select <code>timestamp</code></li> <li>Click on <code>+ Field</code> and add name as <code>account_status</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> </ol> <p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply   enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/database/bigquery/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/database/bigquery/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScalaYAMLUI <pre><code>public class MyBigQueryJavaPlan extends PlanRun {\n    {\n        var accountTask = bigquery(\"customer_bigquery\", \"gs://&lt;my-bucket-name&gt;/temp-data-gen\")\n                .table(\"&lt;project&gt;.data_caterer_test.accounts\")\n                .fields(\n                        field().name(\"account_number\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").expression(\"#{Name.name}\"),\n                        field().name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"open_timestamp\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MyBigQueryPlan extends PlanRun {\n  val accountTask = bigquery(\"customer_bigquery\", \"gs://&lt;my-bucket-name&gt;/temp-data-gen\")\n    .table(\"&lt;project&gt;.data_caterer_test.accounts\")\n    .fields(\n      field.name(\"account_number\").primaryKey(true),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").expression(\"#{Name.name}\"),\n      field.name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"open_timestamp\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  execute(config, accountTask)\n}\n</code></pre> <p>No additional steps for YAML.</p> <p>You can save your plan via the <code>Save</code> button at the top.</p>"},{"location":"docs/guide/data-source/database/bigquery/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyBigQueryJavaPlan\n</code></pre> <pre><code>./run.sh MyBigQueryPlan\n</code></pre> <pre><code>./run.sh my-bigquery.yaml\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <pre><code> count\n-------\n   100\n(1 row)\n\n id | account_number | account_status |     created_by      | created_by_fixed_length | customer_id_int |     open_timestamp      \n----+----------------+----------------+---------------------+-------------------------+-----------------+-------------------------\n  1 | 0499572486     | closed         | Stewart Hartmann    | eod                     |             951 | 2023-12-02 12:30:37.602 \n  4 | 0777698075     | closed         | Shauna Huels        | eod                     |             225 | 2023-08-07 01:25:32.732 \n  2 | 1011209228     | suspended      | Miss Yu Torp        | event                   |             301 | 2024-03-07 08:33:03.031 \n  6 | 0759166208     | closed         | Mrs. Alesha Koelpin | eod                     |             778 | 2024-04-18 13:23:43.861 \n  5 | 1151247273     | closed         | Eugenio Corkery     | eod                     |             983 | 2024-05-03 22:44:22.816 \n  7 | 3909668884     | suspended      | Deandra Ratke       | event                   |             891 | 2024-05-01 13:11:05.498 \n  8 | 5396749742     | suspended      | Grant Moen          | event                   |              46 | 2024-02-22 14:43:31.294 \n  9 | 4269791821     | suspended      | Kenton Romaguera    | event                   |             735 | 2024-05-16 16:40:55.781 \n 10 | 6095315531     | closed         | Crystle Hintz       | eod                     |             279 | 2024-02-18 07:40:21.088 \n 11 | 6625684008     | open           | Miss Edelmira Rath  | eod                     |             200 | 2024-05-12 17:17:55.86  \n(10 rows)\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/data-source/database/bigquery/#validation","title":"Validation","text":"<p>If you want to validate data from BigQuery, follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/database/cassandra/","title":"Cassandra","text":"<p>Creating a data generator for Cassandra. You will build a Docker image that will be able to populate data in Cassandra for the tables you configure.</p>"},{"location":"docs/guide/data-source/database/cassandra/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> <li>Cassandra</li> </ul>"},{"location":"docs/guide/data-source/database/cassandra/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a Cassandra instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/database/cassandra/#cassandra-setup","title":"Cassandra Setup","text":"<p>Next, let's make sure you have an instance of Cassandra up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d cassandra\n</code></pre>"},{"location":"docs/guide/data-source/database/cassandra/#permissions","title":"Permissions","text":"<p>Let's make a new user that has the required permissions needed to push data into the Cassandra tables we want.</p> CQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO data_caterer_user;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and fields from the below tables.</p> CQL Permission Statements <pre><code>GRANT SELECT ON system_schema.tables TO data_caterer_user;\nGRANT SELECT ON system_schema.columns TO data_caterer_user;\n</code></pre>"},{"location":"docs/guide/data-source/database/cassandra/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedCassandraJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedCassandraPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyAdvancedCassandraJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyAdvancedCassandraPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-cassandra.yaml</code>: <pre><code>name: \"my_cassandra_plan\"\ndescription: \"Create account data via Cassandra\"\ntasks:\n  - name: \"cassandra_task\"\n    dataSourceName: \"my_cassandra\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_cassandra</code></li> <li>Click on <code>Select data source type..</code> and select <code>Cassandra</code></li> <li>Set URL as <code>localhost:9042</code></li> <li>Set username as <code>cassandra</code></li> <li>Set password as <code>cassandra</code><ol> <li>Optionally, we could set a keyspace and table name but if you have more than keyspace or table, you would have to create new connection for each</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_cassandra</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_cassandra_plan</code></li> <li>Set task name to <code>cassandra_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_cassandra</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/database/cassandra/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to Cassandra.</p> JavaScalaYAMLUI <pre><code>var accountTask = cassandra(\n    \"customer_cassandra\",   //name\n    \"localhost:9042\",       //url\n    \"cassandra\",            //username\n    \"cassandra\",            //password\n    Map.of()                //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <pre><code>val accountTask = cassandra(\n    \"customer_cassandra\",   //name\n    \"localhost:9042\",       //url\n    \"cassandra\",            //username\n    \"cassandra\",            //password\n    Map()                   //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>org.apache.spark.sql.cassandra {\n    cassandra {\n        spark.cassandra.connection.host = \"localhost\"\n        spark.cassandra.connection.host = ${?CASSANDRA_HOST}\n        spark.cassandra.connection.port = \"9042\"\n        spark.cassandra.connection.port = ${?CASSANDRA_PORT}\n        spark.cassandra.auth.username = \"cassandra\"\n        spark.cassandra.auth.username = ${?CASSANDRA_USERNAME}\n        spark.cassandra.auth.password = \"cassandra\"\n        spark.cassandra.auth.password = ${?CASSANDRA_PASSWORD}\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/database/cassandra/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>account.accounts</code> and <code>account.account_status_history</code> tables as defined under<code>docker/data/cql/customer.cql</code>. This table should already be setup for you if you followed this step. We can check if the table is set up already via the following command:</p> <pre><code>docker exec docker-cassandraserver-1 cqlsh -e 'describe account.accounts; describe account.account_status_history;'\n</code></pre> <p>Here we should see some output that looks like the below. This tells us what schema we need to follow when generating data. We need to define that alongside any metadata that is useful to add constraints on what are possible values the generated data should contain.</p> <pre><code>CREATE TABLE account.accounts (\n    account_id text PRIMARY KEY,\n    amount double,\n    created_by text,\n    name text,\n    open_time timestamp,\n    status text\n)...\n\nCREATE TABLE account.account_status_history (\n    account_id text,\n    eod_date date,\n    status text,\n    updated_by text,\n    updated_time timestamp,\n    PRIMARY KEY (account_id, eod_date)\n)...\n</code></pre> <p>Trimming the connection details to work with the docker-compose Cassandra, we have a base Cassandra connection to define the table and schema required. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code> which corresponds to <code>text</code> in Cassandra.</p> JavaScalaYAMLUI <pre><code>{\n    var accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n            .table(\"account\", \"accounts\")\n            .fields(\n                    field().name(\"account_id\"),\n                    field().name(\"amount\").type(DoubleType.instance()),\n                    field().name(\"created_by\"),\n                    field().name(\"name\"),\n                    field().name(\"open_time\").type(TimestampType.instance()),\n                    field().name(\"status\")\n            );\n}\n</code></pre> <pre><code>val accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n  .table(\"account\", \"accounts\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"amount\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"name\"),\n    field.name(\"open_time\").`type`(TimestampType),\n    field.name(\"status\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/cassandra/cassandra-task.yaml</code>: <pre><code>name: \"cassandra_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"cassandra\"\n    options:\n      keyspace: \"account\"\n      table: \"accounts\"\n    fields:\n    - name: \"account_number\"\n    - name: \"amount\"\n      type: \"double\"\n    - name: \"created_by\"\n    - name: \"open_time\"\n      type: \"timestamp\"\n    - name: \"status\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>account_number</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>amount</code></li> <li>Click on <code>Select data type</code> and select <code>double</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>open_time</code></li> <li>Click on <code>Select data type</code> and select <code>timestamp</code></li> <li>Click on <code>+ Field</code> and add name as <code>status</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> </ol> <p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/database/cassandra/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/database/cassandra/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScalaYAMLUI <pre><code>public class MyAdvancedCassandraJavaPlan extends PlanRun {\n    {\n        var accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n                .table(\"account\", \"accounts\")\n                .fields(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"name\").expression(\"#{Name.name}\"),\n                        field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MyAdvancedCassandraPlan extends PlanRun {\n  val accountTask = cassandra(\"customer_cassandra\", \"host.docker.internal:9042\")\n    .table(\"account\", \"accounts\")\n    .fields(\n      field.name(\"account_id\").primaryKey(true),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  execute(config, accountTask)\n}\n</code></pre> <p>No additional steps for YAML.</p> <p>You can save your plan via the <code>Save</code> button at the top.</p>"},{"location":"docs/guide/data-source/database/cassandra/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedCassandraJavaPlan\ndocker exec docker-cassandraserver-1 cqlsh -e 'select count(1) from account.accounts;select * from account.accounts limit 10;'\n</code></pre> <pre><code>./run.sh MyAdvancedCassandraPlan\ndocker exec docker-cassandraserver-1 cqlsh -e 'select count(1) from account.accounts;select * from account.accounts limit 10;'\n</code></pre> <pre><code>./run.sh my-cassandra.yaml\ndocker exec docker-cassandraserver-1 cqlsh -e 'select count(1) from account.accounts;select * from account.accounts limit 10;'\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <pre><code> count\n-------\n  1000\n\n(1 rows)\n\nWarnings :\nAggregation query used without partition key\n\n\n account_id  | amount    | created_by         | name                   | open_time                       | status\n-------------+-----------+--------------------+------------------------+---------------------------------+-----------\n ACC13554145 | 917.00418 | zb CVvbBTTzitjo5fK |          Jan Sanford I | 2023-06-21 21:50:10.463000+0000 | suspended\n ACC19154140 |  46.99177 |             VH88H9 |       Clyde Bailey PhD | 2023-07-18 11:33:03.675000+0000 |      open\n ACC50587836 |  774.9872 |         GENANwPm t |           Sang Monahan | 2023-03-21 00:16:53.308000+0000 |    closed\n ACC67619387 | 452.86706 |       5msTpcBLStTH |         Jewell Gerlach | 2022-10-18 19:13:07.606000+0000 | suspended\n ACC69889784 |  14.69298 |           WDmOh7NT |          Dale Schulist | 2022-10-25 12:10:52.239000+0000 | suspended\n ACC41977254 |  51.26492 |          J8jAKzvj2 |           Norma Nienow | 2023-08-19 18:54:39.195000+0000 | suspended\n ACC40932912 | 349.68067 |   SLcJgKZdLp5ALMyg | Vincenzo Considine III | 2023-05-16 00:22:45.991000+0000 |    closed\n ACC20642011 | 658.40713 |          clyZRD4fI |  Lannie McLaughlin DDS | 2023-05-11 23:14:30.249000+0000 |      open\n ACC74962085 | 970.98218 |       ZLETTSnj4NpD |          Ima Jerde DVM | 2023-05-07 10:01:56.218000+0000 |   pending\n ACC72848439 | 481.64267 |                 cc |        Kyla Deckow DDS | 2023-08-16 13:28:23.362000+0000 | suspended\n\n(10 rows)\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/data-source/database/cassandra/#validation","title":"Validation","text":"<p>If you want to validate data from Cassandra, follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/database/mysql/","title":"MySQL","text":"<p>Creating a data generator for MySQL. You will build a Docker image that will be able to populate data in MySQL for the tables you configure.</p>"},{"location":"docs/guide/data-source/database/mysql/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/database/mysql/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a MySQL instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/database/mysql/#mysql-setup","title":"MySQL Setup","text":"<p>Next, let's make sure you have an instance of MySQL up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d mysql\n</code></pre>"},{"location":"docs/guide/data-source/database/mysql/#permissions","title":"Permissions","text":"<p>Let's make a new user that has the required permissions needed to push data into the MySQL tables we want.</p> SQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO data_caterer_user;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and fields from the below tables.</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"docs/guide/data-source/database/mysql/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MySQLJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MySQLPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-mysql.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MySQLJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MySQLPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-mysql.yaml</code>: <pre><code>name: \"my_mysql_plan\"\ndescription: \"Create account data via MySQL\"\ntasks:\n  - name: \"mysql_task\"\n    dataSourceName: \"my_mysql\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_mysql</code></li> <li>Click on <code>Select data source type..</code> and select <code>MySQL</code></li> <li>Set URL as <code>jdbc:mysql://localhost:3306/customer</code></li> <li>Set username as <code>root</code></li> <li>Set password as <code>root</code><ol> <li>Optionally, we could set a schema and table name but if you have more than schema or table, you would have to create new connection for each</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_mysql</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_mysql_plan</code></li> <li>Set task name to <code>mysql_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_mysql</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/database/mysql/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to MySQL.</p> JavaScalaYAMLUI <pre><code>var accountTask = mysql(\n    \"customer_mysql\",                                   //name\n    \"jdbc:mysql://host.docker.internal:3306/customer\",  //url\n    \"root\",                                             //username\n    \"root\",                                             //password\n    Map.of()                                            //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <pre><code>val accountTask = mysql(\n    \"customer_mysql\",                                   //name\n    \"jdbc:mysql://host.docker.internal:3306/customer\",  //url\n    \"root\",                                             //username\n    \"root\",                                             //password\n    Map()                                               //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jdbc {\n    customer_mysql {\n        url = \"jdbc:mysql://jdbc:mysql://host.docker.internal:3306/customer/customer\"\n        user = \"root\"\n        password = \"root\"\n        driver = \"com.mysql.cj.jdbc.Driver\"\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/database/mysql/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>customer.accounts</code> and <code>customer.balances</code> tables as defined under<code>docker/data/sql/mysql/customer.cql</code>. This table should already be setup for you if you followed this step.</p> <p>Trimming the connection details to work with the docker-compose MySQL, we have a base MySQL connection to define the table and schema required. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code> which corresponds to <code>text</code> in MySQL.</p> JavaScalaYAMLUI <pre><code>{\n    var accountTask = mysql(\"customer_mysql\", \"jdbc:mysql://host.docker.internal:3306/customer\")\n            .table(\"customer\", \"accounts\")\n            .fields(\n                    field().name(\"account_number\"),\n                    field().name(\"amount\").type(DoubleType.instance()),\n                    field().name(\"created_by\"),\n                    field().name(\"created_by_fixed_length\"),\n                    field().name(\"open_timestamp\").type(TimestampType.instance()),\n                    field().name(\"account_status\")\n            );\n}\n</code></pre> <pre><code>val accountTask = mysql(\"customer_mysql\", \"jdbc:mysql://host.docker.internal:3306/customer\")\n  .table(\"customer\", \"accounts\")\n  .fields(\n    field.name(\"account_number\"),\n    field.name(\"amount\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"created_by_fixed_length\"),\n    field.name(\"open_timestamp\").`type`(TimestampType),\n    field.name(\"account_status\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/mysql/mysql-task.yaml</code>: <pre><code>name: \"mysql_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"mysql\"\n    options:\n      dbtable: \"customer.accounts\"\n    fields:\n    - name: \"account_number\"\n    - name: \"amount\"\n      type: \"double\"\n    - name: \"created_by\"\n    - name: \"created_by_fixed_length\"\n    - name: \"open_timestamp\"\n      type: \"timestamp\"\n    - name: \"account_status\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>account_number</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>amount</code></li> <li>Click on <code>Select data type</code> and select <code>double</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by_fixed_length</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>open_timestamp</code></li> <li>Click on <code>Select data type</code> and select <code>timestamp</code></li> <li>Click on <code>+ Field</code> and add name as <code>account_status</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> </ol> <p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply   enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/database/mysql/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/database/mysql/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScalaYAMLUI <pre><code>public class MySQLJavaPlan extends PlanRun {\n    {\n        var accountTask = mysql(\"customer_mysql\", \"jdbc:mysql://host.docker.internal:3306/customer\")\n                .table(\"customer\", \"accounts\")\n                .fields(\n                        field().name(\"account_number\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").expression(\"#{Name.name}\"),\n                        field().name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"open_timestamp\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MySQLPlan extends PlanRun {\n  val accountTask = mysql(\"customer_mysql\", \"jdbc:mysql://host.docker.internal:3306/customer\")\n    .table(\"customer\", \"accounts\")\n    .fields(\n      field.name(\"account_number\").primaryKey(true),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").expression(\"#{Name.name}\"),\n      field.name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"open_timestamp\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  execute(config, accountTask)\n}\n</code></pre> <p>No additional steps for YAML.</p> <p>You can save your plan via the <code>Save</code> button at the top.</p>"},{"location":"docs/guide/data-source/database/mysql/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MySQLJavaPlan\ndocker exec docker-mysql-1  mysql -u root \"-proot\" \"customer\" -e \"SELECT COUNT(1) FROM customer.accounts; SELECT * FROM customer.accounts LIMIT 10;\"\n</code></pre> <pre><code>./run.sh MySQLPlan\ndocker exec docker-mysql-1  mysql -u root \"-proot\" \"customer\" -e \"SELECT COUNT(1) FROM customer.accounts; SELECT * FROM customer.accounts LIMIT 10;\"\n</code></pre> <pre><code>./run.sh my-mysql.yaml\ndocker exec docker-mysql-1  mysql -u root \"-proot\" \"customer\" -e \"SELECT COUNT(1) FROM customer.accounts; SELECT * FROM customer.accounts LIMIT 10;\"\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <pre><code>COUNT(1)\n10\nid  account_number  account_status  created_by  created_by_fixed_length customer_id_int customer_id_smallint    customer_id_bigint  customer_id_decimal customer_id_real    customer_id_double  open_date   open_timestamp  last_opened_time    payload_bytes\n1   0507581306  suspended   Pete Pouros event   510 NULL    NULL    NULL    NULL    NULL    NULL    2023-08-08 06:55:09 NULL    NULL\n2   0998204877  pending Kraig Balistreri    event   987 NULL    NULL    NULL    NULL    NULL    NULL    2023-09-04 10:09:27 NULL    NULL\n3   1491488574  pending Mrs. Ali DuBuque    event   43  NULL    NULL    NULL    NULL    NULL    NULL    2024-05-11 06:43:07 NULL    NULL\n4   5209805789  suspended   Lorilee Gislason    event   975 NULL    NULL    NULL    NULL    NULL    NULL    2023-08-30 20:54:40 NULL    NULL\n5   5901422604  closed  Elbert Johnston eod 369 NULL    NULL    NULL    NULL    NULL    NULL    2023-08-07 13:46:58 NULL    NULL\n6   7930880350  pending Charlie McCullough  event   797 NULL    NULL    NULL    NULL    NULL    NULL    2024-02-05 15:16:46 NULL    NULL\n7   8248715689  pending Mila Becker event   236 NULL    NULL    NULL    NULL    NULL    NULL    2024-06-07 08:51:29 NULL    NULL\n8   8709384015  closed  Dinah Zemlak    eod 965 NULL    NULL    NULL    NULL    NULL    NULL    2024-01-14 20:58:51 NULL    NULL\n9   9463221576  closed  Les Hettinger   eod 83  NULL    NULL    NULL    NULL    NULL    NULL    2023-06-27 16:05:22 NULL    NULL\n10  9903967165  closed  Ms. Napoleon Walker eod 795 NULL    NULL    NULL    NULL    NULL    NULL    2023-10-07 13:42:24 NULL    NULL\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/data-source/database/mysql/#validation","title":"Validation","text":"<p>If you want to validate data from MySQL, follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/database/postgres/","title":"Postgres","text":"<p>Creating a data generator for Postgres. You will build a Docker image that will be able to populate data in Postgres for the tables you configure.</p>"},{"location":"docs/guide/data-source/database/postgres/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/database/postgres/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a Postgres instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/database/postgres/#postgres-setup","title":"Postgres Setup","text":"<p>Next, let's make sure you have an instance of Postgres up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d mysql\n</code></pre>"},{"location":"docs/guide/data-source/database/postgres/#permissions","title":"Permissions","text":"<p>Let's make a new user that has the required permissions needed to push data into the Postgres tables we want.</p> SQL Permission Statements <pre><code>GRANT INSERT ON &lt;schema&gt;.&lt;table&gt; TO data_caterer_user;\n</code></pre> <p>Following permissions are required when enabling <code>configuration.enableGeneratePlanAndTasks(true)</code> as it will gather metadata information about tables and fields from the below tables.</p> SQL Permission Statements <pre><code>GRANT SELECT ON information_schema.columns TO &lt; user &gt;;\nGRANT SELECT ON information_schema.statistics TO &lt; user &gt;;\nGRANT SELECT ON information_schema.key_column_usage TO &lt; user &gt;;\n</code></pre>"},{"location":"docs/guide/data-source/database/postgres/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyPostgresJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyPostgresPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-postgres.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyPostgresJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyPostgresPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-postgres.yaml</code>: <pre><code>name: \"my_postgres_plan\"\ndescription: \"Create account data via Postgres\"\ntasks:\n  - name: \"postgres_task\"\n    dataSourceName: \"my_postgres\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_postgres</code></li> <li>Click on <code>Select data source type..</code> and select <code>Postgres</code></li> <li>Set URL as <code>jdbc:postgresql://localhost:5432/customer</code></li> <li>Set username as <code>postgres</code></li> <li>Set password as <code>postgres</code><ol> <li>Optionally, we could set a schema and table name but if you have more than schema or table, you would have to create new connection for each</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_postgres</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_postgres_plan</code></li> <li>Set task name to <code>postgres_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_postgres</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/database/postgres/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to Postgres.</p> JavaScalaYAMLUI <pre><code>var accountTask = postgres(\n    \"customer_postgres\",                                    //name\n    \"jdbc:postgresql://host.docker.internal:5432/customer\", //url\n    \"postgres\",                                             //username\n    \"postgres\",                                             //password\n    Map.of()                                                //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <pre><code>val accountTask = postgres(\n    \"customer_postgres\",                                    //name\n    \"jdbc:postgresql://host.docker.internal:5432/customer\", //url\n    \"postgres\",                                             //username\n    \"postgres\",                                             //password\n    Map()                                                   //optional additional connection options\n)\n</code></pre> <p>Additional options such as SSL configuration, etc can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jdbc {\n    customer_postgres {\n        url = \"jdbc:mysql://jdbc:postgresql://host.docker.internal:5432/customer/customer\"\n        user = \"postgres\"\n        password = \"postgres\"\n        driver = \"org.postgresql.Driver\"\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/database/postgres/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>account.accounts</code> and <code>account.balances</code> tables as defined under<code>docker/data/sql/postgres/customer.cql</code>. This table should already be setup for you if you followed this step.</p> <p>Trimming the connection details to work with the docker-compose Postgres, we have a base Postgres connection to define the table and schema required. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code> which corresponds to <code>text</code> in Postgres.</p> JavaScalaYAMLUI <pre><code>{\n    var accountTask = postgres(\"customer_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")\n            .table(\"account\", \"accounts\")\n            .fields(\n                    field().name(\"account_number\"),\n                    field().name(\"amount\").type(DoubleType.instance()),\n                    field().name(\"created_by\"),\n                    field().name(\"created_by_fixed_length\"),\n                    field().name(\"open_timestamp\").type(TimestampType.instance()),\n                    field().name(\"account_status\")\n            );\n}\n</code></pre> <pre><code>val accountTask = postgres(\"customer_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")\n  .table(\"account\", \"accounts\")\n  .fields(\n    field.name(\"account_number\"),\n    field.name(\"amount\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"created_by_fixed_length\"),\n    field.name(\"open_timestamp\").`type`(TimestampType),\n    field.name(\"account_status\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/postgres/postgres-task.yaml</code>: <pre><code>name: \"postgres_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"postgres\"\n    options:\n      dbtable: \"account.accounts\"\n    fields:\n    - name: \"account_number\"\n    - name: \"amount\"\n      type: \"double\"\n    - name: \"created_by\"\n    - name: \"created_by_fixed_length\"\n    - name: \"open_timestamp\"\n      type: \"timestamp\"\n    - name: \"account_status\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>account_number</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>amount</code></li> <li>Click on <code>Select data type</code> and select <code>double</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by_fixed_length</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>open_timestamp</code></li> <li>Click on <code>Select data type</code> and select <code>timestamp</code></li> <li>Click on <code>+ Field</code> and add name as <code>account_status</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> </ol> <p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply   enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/database/postgres/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/database/postgres/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScalaYAMLUI <pre><code>public class MyPostgresJavaPlan extends PlanRun {\n    {\n        var accountTask = postgres(\"customer_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")\n                .table(\"account\", \"accounts\")\n                .fields(\n                        field().name(\"account_number\").regex(\"ACC[0-9]{8}\").primaryKey(true),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").expression(\"#{Name.name}\"),\n                        field().name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"open_timestamp\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MyPostgresPlan extends PlanRun {\n  val accountTask = postgres(\"customer_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")\n    .table(\"account\", \"accounts\")\n    .fields(\n      field.name(\"account_number\").primaryKey(true),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").expression(\"#{Name.name}\"),\n      field.name(\"created_by_fixed_length\").sql(\"CASE WHEN account_status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"open_timestamp\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  execute(config, accountTask)\n}\n</code></pre> <p>No additional steps for YAML.</p> <p>You can save your plan via the <code>Save</code> button at the top.</p>"},{"location":"docs/guide/data-source/database/postgres/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyPostgresJavaPlan\ndocker exec docker-postgresserver-1  psql -Upostgres -d customer -c \"SELECT COUNT(1) FROM account.accounts; SELECT * FROM account.accounts LIMIT 10;\"\n</code></pre> <pre><code>./run.sh MyPostgresPlan\ndocker exec docker-postgresserver-1  psql -Upostgres -d customer -c \"SELECT COUNT(1) FROM account.accounts; SELECT * FROM account.accounts LIMIT 10;\"\n</code></pre> <pre><code>./run.sh my-postgres.yaml\ndocker exec docker-postgresserver-1  psql -Upostgres -d customer -c \"SELECT COUNT(1) FROM account.accounts; SELECT * FROM account.accounts LIMIT 10;\"\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <pre><code> count\n-------\n   100\n(1 row)\n\n id | account_number | account_status |     created_by      | created_by_fixed_length | customer_id_int | customer_id_smallint | customer_id_bigint | customer_id_decimal | customer_id_real | customer_id_double | open_date |     open_timestamp      | last_opened_time | payload_bytes\n----+----------------+----------------+---------------------+-------------------------+-----------------+----------------------+--------------------+---------------------+------------------+--------------------+-----------+-------------------------+------------------+---------------\n  1 | 0499572486     | closed         | Stewart Hartmann    | eod                     |             951 |                      |                    |                     |                  |                    |           | 2023-12-02 12:30:37.602 |                  |\n  4 | 0777698075     | closed         | Shauna Huels        | eod                     |             225 |                      |                    |                     |                  |                    |           | 2023-08-07 01:25:32.732 |                  |\n  2 | 1011209228     | suspended      | Miss Yu Torp        | event                   |             301 |                      |                    |                     |                  |                    |           | 2024-03-07 08:33:03.031 |                  |\n  6 | 0759166208     | closed         | Mrs. Alesha Koelpin | eod                     |             778 |                      |                    |                     |                  |                    |           | 2024-04-18 13:23:43.861 |                  |\n  5 | 1151247273     | closed         | Eugenio Corkery     | eod                     |             983 |                      |                    |                     |                  |                    |           | 2024-05-03 22:44:22.816 |                  |\n  7 | 3909668884     | suspended      | Deandra Ratke       | event                   |             891 |                      |                    |                     |                  |                    |           | 2024-05-01 13:11:05.498 |                  |\n  8 | 5396749742     | suspended      | Grant Moen          | event                   |              46 |                      |                    |                     |                  |                    |           | 2024-02-22 14:43:31.294 |                  |\n  9 | 4269791821     | suspended      | Kenton Romaguera    | event                   |             735 |                      |                    |                     |                  |                    |           | 2024-05-16 16:40:55.781 |                  |\n 10 | 6095315531     | closed         | Crystle Hintz       | eod                     |             279 |                      |                    |                     |                  |                    |           | 2024-02-18 07:40:21.088 |                  |\n 11 | 6625684008     | open           | Miss Edelmira Rath  | eod                     |             200 |                      |                    |                     |                  |                    |           | 2024-05-12 17:17:55.86  |                  |\n(10 rows)\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/data-source/database/postgres/#validation","title":"Validation","text":"<p>If you want to validate data from Postgres, follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/csv/","title":"CSV","text":"<p>Creating a data generator for CSV. You will have the ability to generate and validate CSV files via Docker.</p>"},{"location":"docs/guide/data-source/file/csv/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/csv/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/csv/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyCSVJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyCSVPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-csv.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyCSVJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyCSVPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-csv.yaml</code>: <pre><code>name: \"my_csv_plan\"\ndescription: \"Create account data in CSV\"\ntasks:\n  - name: \"csv_task\"\n    dataSourceName: \"my_csv\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_csv</code></li> <li>Click on <code>Select data source type..</code> and select <code>CSV</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/csv/accounts</code><ol> <li>Optionally, we could set the number of partitions and columns to partition by</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_csv</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_csv_plan</code></li> <li>Set task name to <code>csv_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_csv</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/csv/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to CSV.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\n    \"customer_accounts\",              //name\n    \"/opt/app/data/customer/account\", //path\n    Map.of(\"header\", \"true\")          //additional options\n);\n</code></pre> <p>Additional options such as including a header row, etc can be found here.</p> <pre><code>val accountTask = csv(\n  \"customer_accounts\",              //name         \n  \"/opt/app/data/customer/account\", //path\n  Map(\"header\" -&gt; \"true\")           //additional options\n)\n</code></pre> <p>Additional options such as including a header row, etc can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>csv {\n    my_csv {\n        \"header\": \"true\"\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/csv/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/file/csv/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/csv/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyCSVJavaPlan\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <pre><code>./run.sh MyCSVPlan\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <pre><code>./run.sh my-csv.yaml\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>CSVJavaPlan.java</code> or <code>CSVPlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/csv/#validation","title":"Validation","text":"<p>If you want to validate data from a CSV source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/delta-lake/","title":"Delta Lake","text":"<p>Creating a data generator for Delta Lake. You will have the ability to generate and validate Delta Lake files.</p>"},{"location":"docs/guide/data-source/file/delta-lake/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/delta-lake/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/delta-lake/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyDeltaLakeJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyDeltaLakePlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-delta-lake.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyDeltaLakeJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyDeltaLakePlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-delta-lake.yaml</code>: <pre><code>name: \"my_delta_lake_plan\"\ndescription: \"Create account data in Delta Lake\"\ntasks:\n  - name: \"delta_lake_task\"\n    dataSourceName: \"my_delta_lake\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_delta_lake</code></li> <li>Click on <code>Select data source type..</code> and select <code>Delta Lake</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/delta_lake/accounts</code><ol> <li>Optionally, we could set the number of partitions and columns to partition by</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_delta_lake</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_delta_lake_plan</code></li> <li>Set task name to <code>delta_lake_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_delta_lake</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/delta-lake/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to Delta Lake.</p> JavaScalaYAMLUI <pre><code>var accountTask = delta(\n        \"customer_accounts\",            //name\n        \"/opt/app/data/customer/delta\", //path\n        Map.of()                        //additional options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = delta(\n  \"customer_accounts\",            //name\n  \"/opt/app/data/customer/delta\", //path\n  Map()                           //additional options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>delta_lake {\n    my_delta_lake {\n        \"spark.databricks.delta.properties.defaults.appendOnly\" = \"true\"\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/delta-lake/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/file/delta-lake/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/delta-lake/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyDeltaLakeJavaPlan\n</code></pre> <pre><code>./run.sh MyDeltaLakePlan\n</code></pre> <pre><code>./run.sh my-delta-lake.yaml\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>DeltaLakeJavaPlan.java</code> or <code>DeltaLakePlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/delta-lake/#validation","title":"Validation","text":"<p>If you want to validate data from a Delta Lake source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/iceberg/","title":"Iceberg","text":"<p>Data testing for Iceberg. You will have the ability to generate and validate Iceberg tables.</p>"},{"location":"docs/guide/data-source/file/iceberg/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/iceberg/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/iceberg/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyIcebergJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyIcebergPlan.scala</code></li> <li>YAML: <code>docker/data/customer/plan/my-iceberg.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyIcebergJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyIcebergPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-iceberg.yaml</code>: <pre><code>name: \"my_iceberg_plan\"\ndescription: \"Create account data in Iceberg table\"\ntasks:\n  - name: \"iceberg_task\"\n    dataSourceName: \"my_iceberg\"\n</code></pre></p> <ol> <li>Go to <code>Connection</code> tab in the top bar</li> <li>Select data source as <code>Iceberg</code><ol> <li>Enter in data source name <code>my_iceberg</code></li> <li>Select catalog type <code>hadoop</code></li> <li>Enter warehouse path as <code>/opt/app/data/customer/iceberg</code></li> </ol> </li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/iceberg/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to Iceberg.</p> JavaScalaYAMLUI <pre><code>var accountTask = iceberg(\n        \"customer_accounts\",              //name\n        \"account.accounts\",               //table name\n        \"/opt/app/data/customer/iceberg\", //warehouse path\n        \"hadoop\",                         //catalog type\n        \"\",                               //catalog uri\n        Map.of()                          //additional options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = iceberg(\n  \"customer_accounts\",              //name\n  \"account.accounts\",               //table name\n  \"/opt/app/data/customer/iceberg\", //warehouse path\n  \"hadoop\",                         //catalog type\n  \"\",                               //catalog uri\n  Map()                             //additional options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>iceberg {\n  my_iceberg {\n    path = \"/opt/app/data/customer/iceberg\"\n    path = ${?ICEBERG_WAREHOUSE_PATH}\n    catalogType = \"hadoop\"\n    catalogType = ${?ICEBERG_CATALOG_TYPE}\n    catalogUri = \"\"\n    catalogUri = ${?ICEBERG_CATALOG_URI}\n  }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/iceberg/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/file/iceberg/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/iceberg/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyIcebergJavaPlan\n</code></pre> <pre><code>./run.sh MyIcebergPlan\n</code></pre> <pre><code>./run.sh my-iceberg.yaml\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>IcebergJavaPlan.java</code> or <code>IcebergPlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/iceberg/#validation","title":"Validation","text":"<p>If you want to validate data from an Iceberg source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/json/","title":"JSON","text":"<p>Creating a data generator for JSON. You will have the ability to generate and validate JSON files via Docker.</p>"},{"location":"docs/guide/data-source/file/json/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/json/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/json/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyJSONJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyJSONPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-json.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyJSONJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyJSONPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-json.yaml</code>: <pre><code>name: \"my_json_plan\"\ndescription: \"Create account data in JSON\"\ntasks:\n  - name: \"json_task\"\n    dataSourceName: \"my_json\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_json</code></li> <li>Click on <code>Select data source type..</code> and select <code>JSON</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/json/accounts</code><ol> <li>Optionally, we could set the number of partitions and columns to partition by</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_json</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_json_plan</code></li> <li>Set task name to <code>json_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_json</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/json/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to JSON.</p> JavaScalaYAMLUI <pre><code>var accountTask = json(\n    \"customer_accounts\",                    //name\n    \"/opt/app/data/customer/account_json\",  //path\n    Map.of()                                //additional options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = json(\n  \"customer_accounts\",                    //name         \n  \"/opt/app/data/customer/account_json\",  //path\n  Map()                                   //additional options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>json {\n    my_json {\n        \"dateFormat\": \"dd-MM-yyyy\"\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/json/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul> <p>Let's create a task for generating data as <code>accounts</code> and then generate data for <code>transactions</code>, which will be related  to the accounts generated.</p> JavaScalaYAMLUI <pre><code>var accountTask = json(\"customer_accounts\", \"/opt/app/data/customer/account_json\")\n        .fields(\n            field().name(\"account_id\"),\n            field().name(\"balance\").type(new DecimalType(5, 2)),\n            field().name(\"created_by\"),\n            field().name(\"open_time\").type(TimestampType.instance()),\n            field().name(\"status\"),\n            field().name(\"customer_details\")\n                .fields(\n                    field().name(\"name\"),\n                    field().name(\"age\").type(IntegerType.instance()),\n                    field().name(\"city\")\n                )\n        );\n</code></pre> <pre><code>val accountTask = json(\"customer_accounts\", \"/opt/app/data/customer/account_json\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"balance\").`type`(new DecimalType(5, 2)),\n    field.name(\"created_by\"),\n    field.name(\"open_time\").`type`(TimestampType),\n    field.name(\"status\"),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\"),\n        field.name(\"age\").`type`(IntegerType),\n        field.name(\"city\")\n      )\n  )\n</code></pre> <p>In <code>docker/data/custom/task/json/json-task.yaml</code>: <pre><code>name: \"json_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"json\"\n    options:\n      path: \"/opt/app/data/customer/account_json\"\n    fields:\n      - name: \"account_id\"\n      - name: \"balance\"\n        type: \"double\"\n      - name: \"created_by\"\n      - name: \"open_time\"\n        type: \"timestamp\"\n      - name: \"status\"\n      - name: \"customer_details\"\n        type: \"struct\"\n        fields:\n          - name: \"name\"\n          - name: \"age\"\n            type: \"integer\"\n          - name: \"city\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>account_id</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>balance</code></li> <li>Click on <code>Select data type</code> and select <code>double</code></li> <li>Click on <code>+ Field</code> and add name as <code>created_by</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>open_time</code></li> <li>Click on <code>Select data type</code> and select <code>timestamp</code></li> <li>Click on <code>+ Field</code> and add name as <code>status</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>customer_details</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code><ol> <li>Under <code>customer_details</code>, click on <code>+ Field</code> and add name <code>name</code></li> <li>Under <code>customer_details</code>, click on <code>+ Field</code> and add name <code>age</code>, set data type as <code>integer</code></li> <li>Under <code>customer_details</code>, click on <code>+ Field</code> and add name <code>city</code></li> </ol> </li> </ol>"},{"location":"docs/guide/data-source/file/json/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/json/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyJSONJavaPlan\naccount=$(head -1 docker/sample/customer/account_json/part-00000-* | sed -nr 's/.*account_id\":\"(.+)\",\"balance.*/\\1/p')\necho \"Head account record:\"\nhead -1 docker/sample/customer/account_json/part-00000-*\necho $account\necho \"Transaction records:\"\ncat docker/sample/customer/transaction_json/part-0000* | grep $account\n</code></pre> <pre><code>./run.sh MyJSONPlan\naccount=$(head -1 docker/sample/customer/account_json/part-00000-* | sed -nr 's/.*account_id\":\"(.+)\",\"balance.*/\\1/p')\necho \"Head account record:\"\nhead -1 docker/sample/customer/account_json/part-00000-*\necho $account\necho \"Transaction records:\"\ncat docker/sample/customer/transaction_json/part-0000* | grep $account\n</code></pre> <pre><code>./run.sh my-json.yaml\naccount=$(head -1 docker/sample/customer/account_json/part-00000-* | sed -nr 's/.*account_id\":\"(.+)\",\"balance.*/\\1/p')\necho \"Head account record:\"\nhead -1 docker/sample/customer/account_json/part-00000-*\necho $account\necho \"Transaction records:\"\ncat docker/sample/customer/transaction_json/part-0000* | grep $account\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>It should look something like this.</p> <pre><code>Head account record:\n{\"account_id\":\"ACC00047541\",\"balance\":445.62,\"created_by\":\"event\",\"open_time\":\"2024-03-13T00:31:38.836Z\",\"status\":\"suspended\",\"customer_details\":{\"name\":\"Joey Gaylord\",\"age\":44,\"city\":\"Lake Jose\"}}\nACC00047541\nTransaction records:\n{\"account_id\":\"ACC00047541\",\"full_name\":\"Joey Gaylord\",\"amount\":31.485424217447527,\"time\":\"2023-11-07T04:50:20.875Z\",\"date\":\"2023-11-07\"}\n{\"account_id\":\"ACC00047541\",\"full_name\":\"Joey Gaylord\",\"amount\":79.22177964401857,\"time\":\"2024-02-01T15:15:38.289Z\",\"date\":\"2024-02-01\"}\n{\"account_id\":\"ACC00047541\",\"full_name\":\"Joey Gaylord\",\"amount\":56.06230355456882,\"time\":\"2024-02-29T21:42:42.473Z\",\"date\":\"2024-02-29\"}\n</code></pre> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>JsonJavaPlan.java</code> or <code>JsonPlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/json/#validation","title":"Validation","text":"<p>If you want to validate data from a JSON source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/orc/","title":"ORC","text":"<p>Creating a data generator for ORC. You will have the ability to generate and validate ORC files via Docker.</p>"},{"location":"docs/guide/data-source/file/orc/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/orc/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/orc/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyORCJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyORCPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-orc.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyORCJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyORCPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-orc.yaml</code>: <pre><code>name: \"my_orc_plan\"\ndescription: \"Create account data in ORC format\"\ntasks:\n  - name: \"orc_task\"\n    dataSourceName: \"my_orc\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_orc</code></li> <li>Click on <code>Select data source type..</code> and select <code>ORC</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/orc/accounts</code><ol> <li>Optionally, we could set the number of partitions and columns to partition by</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_orc</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_orc_plan</code></li> <li>Set task name to <code>orc_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_orc</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/orc/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to ORC.</p> JavaScalaYAMLUI <pre><code>var accountTask = orc(\n    \"customer_accounts\",                  //name\n    \"/opt/app/data/customer/account_orc\", //path\n    Map.of()                              //additional options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = orc(\n  \"customer_accounts\",                  //name         \n  \"/opt/app/data/customer/account_orc\", //path\n  Map()                                 //additional options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>orc {\n    my_orc {\n        \"spark.sql.orc.mergeSchema\": \"false\"\n    }\n}\n</code></pre></p> <p>Additional options can be found here.</p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/orc/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/file/orc/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/orc/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyORCJavaPlan\n</code></pre> <pre><code>./run.sh MyORCPlan\n</code></pre> <pre><code>./run.sh my-orc.yaml\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>ORCJavaPlan.java</code> or <code>ORCPlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/orc/#validation","title":"Validation","text":"<p>If you want to validate data from a ORC source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/file/parquet/","title":"Parquet","text":"<p>Creating a data generator for Parquet. You will have the ability to generate and validate Parquet files via Docker.</p>"},{"location":"docs/guide/data-source/file/parquet/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/file/parquet/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/file/parquet/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyParquetJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyParquetPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-parquet.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyParquetJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyParquetPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-parquet.yaml</code>: <pre><code>name: \"my_parquet_plan\"\ndescription: \"Create account data in Parquet format\"\ntasks:\n  - name: \"parquet_task\"\n    dataSourceName: \"my_parquet\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_parquet</code></li> <li>Click on <code>Select data source type..</code> and select <code>Parquet</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/parquet/accounts</code><ol> <li>Optionally, we could set the number of partitions and columns to partition by</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_parquet</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_parquet_plan</code></li> <li>Set task name to <code>parquet_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_parquet</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/file/parquet/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to Parquet.</p> JavaScalaYAMLUI <pre><code>var accountTask = parquet(\n    \"customer_accounts\",                      //name\n    \"/opt/app/data/customer/account_parquet\", //path\n    Map.of()                                  //additional options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = parquet(\n  \"customer_accounts\",                      //name         \n  \"/opt/app/data/customer/account_parquet\", //path\n  Map()                                     //additional options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>parquet {\n    my_parquet {\n        \"spark.sql.parquet.mergeSchema\": \"true\"\n    }\n}\n</code></pre></p> <p>Additional options can be found here.</p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/file/parquet/#schema","title":"Schema","text":"<p>Depending on how you want to define the schema, follow the below:</p> <ul> <li>Manual schema guide</li> <li>Automatically detect schema from the data source, you can simply enable <code>configuration.enableGeneratePlanAndTasks(true)</code></li> <li>Automatically detect schema from a metadata source</li> </ul>"},{"location":"docs/guide/data-source/file/parquet/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n\nexecute(myPlan, config, accountTask, transactionTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n\nexecute(myPlan, config, accountTask, transactionTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/file/parquet/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyParquetJavaPlan\n</code></pre> <pre><code>./run.sh MyParquetPlan\n</code></pre> <pre><code>./run.sh my-parquet.yaml\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>ParquetJavaPlan.java</code> or <code>ParquetPlan.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/data-source/file/parquet/#validation","title":"Validation","text":"<p>If you want to validate data from a Parquet source,  follow the validation documentation found here to help guide you.</p>"},{"location":"docs/guide/data-source/http/http/","title":"HTTP Source","text":"<p>Creating a data generator based on an OpenAPI/Swagger document.</p> <p></p>"},{"location":"docs/guide/data-source/http/http/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/http/http/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/http/http/#http-setup","title":"HTTP Setup","text":"<p>We will be using the http-bin docker image to help simulate a service with HTTP endpoints.</p> <p>Start it via:</p> <pre><code>cd docker\ndocker-compose up -d http\ndocker ps\n</code></pre>"},{"location":"docs/guide/data-source/http/http/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedHttpJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedHttpPlanRun.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-http.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedHttpJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedHttpPlanRun extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-http.yaml</code>: <pre><code>name: \"my_http_plan\"\ndescription: \"Create account data via HTTP from OpenAPI metadata\"\ntasks:\n  - name: \"http_task\"\n    dataSourceName: \"my_http\"\n</code></pre></p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGeneratePlanAndTasks = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/http/http/#schema","title":"Schema","text":"<p>We can point the schema of a data source to a OpenAPI/Swagger document or URL. For this example, we will use the OpenAPI document found under <code>docker/mount/http/petstore.json</code> in the data-caterer-example repo. This is a simplified version of the original OpenAPI spec that can be found here.</p> <p>We have kept the following endpoints to test out:</p> <ul> <li>GET /pets - get all pets</li> <li>POST /pets - create a new pet</li> <li>GET /pets/{id} - get a pet by id</li> <li>DELETE /pets/{id} - delete a pet by id</li> </ul> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\")\n        .fields(metadataSource().openApi(\"/opt/app/mount/http/petstore.json\"))\n        .count(count().records(2));\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(metadataSource.openApi(\"/opt/app/mount/http/petstore.json\"))\n  .count(count.records(2))\n</code></pre> <p>In <code>docker/data/custom/task/http/openapi-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    options:\n      metadataSourceType: \"openapi\"\n      schemaLocation: \"/opt/app/mount/http/petstore.json\"\n    count:\n      records: 2\n</code></pre></p> <ol> <li>Click on <code>Connection</code> tab at the top<ol> <li>Click on <code>Select data source type</code> and select <code>OpenAPI/Swagger</code></li> <li>Provide a <code>Name</code> and <code>Schema Location</code> pointing to where you have stored your OpenAPI specification file</li> <li>Click <code>Create</code></li> </ol> </li> <li>Click on <code>Home</code> tab at the top</li> <li>Click on <code>Generation</code> and tick the <code>Auto from metadata source</code> checkbox<ol> <li>Click on <code>Select metadata source</code> and select the OpenAPI metadata source you just created</li> </ol> </li> </ol> <p>The above defines that the schema will come from an OpenAPI document found on the pathway defined. It will then generate 2 requests per request method and endpoint combination.</p>"},{"location":"docs/guide/data-source/http/http/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedHttpJavaPlanRun or MyAdvancedHttpPlanRun\n#after completing\ndocker logs -f docker-http-1\n</code></pre> <p>It should look something like this.</p> <pre><code>172.21.0.1 [06/Nov/2023:01:06:53 +0000] GET /anything/pets?tags%3DeXQxFUHVja+EYm%26limit%3D33895 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:06:53 +0000] GET /anything/pets?tags%3DSXaFvAqwYGF%26tags%3DjdNRFONA%26limit%3D40975 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:06:56 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:06:56 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:07:00 +0000] GET /anything/pets/kbH8D7rDuq HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:07:00 +0000] GET /anything/pets/REsa0tnu7dvekGDvxR HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:07:03 +0000] DELETE /anything/pets/EqrOr1dHFfKUjWb HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:07:03 +0000] DELETE /anything/pets/7WG7JHPaNxP HTTP/1.1 200 Host: host.docker.internal}\n</code></pre> <p>Looks like we have some data now. But we can do better and add some enhancements to it.</p>"},{"location":"docs/guide/data-source/http/http/#foreign-keys","title":"Foreign keys","text":"<p>The four different requests that get sent could have the same <code>id</code> passed across to each of them if we define a foreign key relationship. This will make it more realistic to a real life scenario as pets get created and queried by a particular <code>id</code> value. We note that the <code>id</code> value is first used when a pet is created in the body of the POST request. Then it gets used as a path parameter in the DELETE and GET requests.</p> <p>To link them all together, we must follow a particular pattern when referring to request body, query parameter or path parameter fields.</p> HTTP Type Field Prefix Example Request Body <code>body</code> <code>body.id</code> Path Parameter <code>pathParam</code> <code>pathParamid</code> Query Parameter <code>queryParam</code> <code>queryParamid</code> Header <code>header</code> <code>headerContent_Type</code> <p>Also note, that when creating a foreign field definition for a HTTP data source, to refer to a specific endpoint and method, we have to follow the pattern of <code>{http method}{http path}</code>. For example, <code>POST/pets</code>. Let's apply this knowledge to link all the <code>id</code> values together.</p> JavaScalaYAMLUI <pre><code>var myPlan = plan().addForeignKeyRelationship(\n        foreignField(\"my_http\", \"POST/pets\", \"body.id\"),     //source of foreign key value\n        foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\"),\n        foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\")\n);\n\nexecute(myPlan, conf, httpTask);\n</code></pre> <pre><code>val myPlan = plan.addForeignKeyRelationship(\n  foreignField(\"my_http\", \"POST/pets\", \"body.id\"),     //source of foreign key value\n  foreignField(\"my_http\", \"DELETE/pets/{id}\", \"pathParamid\"),\n  foreignField(\"my_http\", \"GET/pets/{id}\", \"pathParamid\")\n)\n\nexecute(myPlan, conf, httpTask)\n</code></pre> <p>In <code>docker/data/custom/plan/my-http.yaml</code>: <pre><code>name: \"my_http_plan\"\ndescription: \"Create account data via HTTP from OpenAPI metadata\"\ntasks:\n  - name: \"http_task\"\n    dataSourceName: \"my_http\"\n\nsinkOptions:\n  foreignKeys:\n    - source:\n        dataSource: \"my_http\"\n        step: \"POST/pets\"\n        fields: [\"body.id\"]\n      generate:\n        - dataSource: \"my_http\"\n          step: \"DELETE/pets/{id}\"\n          fields: [\"pathParamid\"]\n        - dataSource: \"my_http\"\n          step: \"GET/pets/{id}\"\n          fields: [\"pathParamid\"]\n</code></pre></p> <p></p> <p>Let's test it out by running it again</p> <pre><code>./run.sh\n#input class MyAdvancedHttpJavaPlanRun or MyAdvancedHttpPlanRun\ndocker logs -f docker-http-1\n</code></pre> <pre><code>172.21.0.1 [06/Nov/2023:01:33:59 +0000] GET /anything/pets?limit%3D45971 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:00 +0000] GET /anything/pets?limit%3D62015 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:04 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:05 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:09 +0000] DELETE /anything/pets/5e HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:09 +0000] DELETE /anything/pets/IHPm2 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:14 +0000] GET /anything/pets/IHPm2 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:34:14 +0000] GET /anything/pets/5e HTTP/1.1 200 Host: host.docker.internal}\n</code></pre> <p>Now we have the same <code>id</code> values being produced across the POST, DELETE and GET requests! What if we knew that the <code>id</code> values should follow a particular pattern?</p>"},{"location":"docs/guide/data-source/http/http/#custom-metadata","title":"Custom metadata","text":"<p>So given that we have defined a foreign key where the root of the foreign key values is from the POST request, we can update the metadata of the <code>id</code> field for the POST request and it will proliferate to the other endpoints as well. Given the <code>id</code> field is a nested field as noted in the foreign key, we can alter its metadata via the following:</p> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\")\n        .fields(metadataSource().openApi(\"/opt/app/mount/http/petstore.json\"))\n        .fields(field().name(\"body\").fields(field().name(\"id\").regex(\"ID[0-9]{8}\")))\n        .count(count().records(2));\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(metadataSource.openApi(\"/opt/app/mount/http/petstore.json\"))\n  .fields(field.name(\"body\").fields(field.name(\"id\").regex(\"ID[0-9]{8}\")))\n  .count(count.records(2))\n</code></pre> <p>In <code>docker/data/custom/task/http/openapi-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    options:\n      metadataSourceType: \"openapi\"\n      schemaLocation: \"/opt/app/mount/http/petstore.json\"\n    count:\n      records: 2\n    fields:\n      - name: \"body\"\n        fields:\n          - name: \"id\"\n            options:\n              regex: \"ID[0-9]{8}\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add name as <code>body</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>id</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Regex</code>. Then enter <code>ID[0-9]{8}</code></li> </ol> </li> </ol> <p>We first get the field <code>body</code>, then get the nested schema and get the field <code>id</code> and add metadata stating that <code>id</code> should follow the patter <code>ID[0-9]{8}</code>.</p> <p>Let's try run again, and hopefully we should see some proper ID values.</p> <pre><code>./run.sh\n#input class MyAdvancedHttpJavaPlanRun or MyAdvancedHttpPlanRun\ndocker logs -f docker-http-1\n</code></pre> <pre><code>172.21.0.1 [06/Nov/2023:01:45:45 +0000] GET /anything/pets?tags%3D10fWnNoDz%26limit%3D66804 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:46 +0000] GET /anything/pets?tags%3DhyO6mI8LZUUpS HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:50 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:51 +0000] POST /anything/pets HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:52 +0000] DELETE /anything/pets/ID55185420 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:52 +0000] DELETE /anything/pets/ID20618951 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:57 +0000] GET /anything/pets/ID55185420 HTTP/1.1 200 Host: host.docker.internal}\n172.21.0.1 [06/Nov/2023:01:45:57 +0000] GET /anything/pets/ID20618951 HTTP/1.1 200 Host: host.docker.internal}\n</code></pre> <p>Great! Now we have replicated a production-like flow of HTTP requests.</p>"},{"location":"docs/guide/data-source/http/http/#no-openapiswagger","title":"No OpenAPI/Swagger","text":"<p>You may want to create your own HTTP requests that are hand-crafted with your requirements. Below is how we can achieve this with some helper methods.</p>"},{"location":"docs/guide/data-source/http/http/#http-url","title":"HTTP URL","text":"<p>There are 4 different parts of creating an HTTP URL. At minimum, you need a base URL and HTTP method. - Base URL - HTTP method (i.e. GET, POST, etc.) - Path parameters - Query parameters</p> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\")\n        .fields(\n                field().httpUrl(\n                        \"http://host.docker.internal:80/anything/pets/{id}\",    //url\n                        HttpMethodEnum.GET(),                                       //method\n                        List.of(field().name(\"id\")),                                //path parameter\n                        List.of(field().name(\"limit\").type(IntegerType.instance()).min(1).max(10))  //query parameter\n                )\n        )\n        .count(count().records(2));\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(\n    field.httpUrl(\n      \"http://host.docker.internal:80/anything/pets/{id}\",  //url\n      HttpMethodEnum.GET,                                   //method\n      List(field.name(\"id\")),                               //path parameter\n      List(field.name(\"limit\").`type`(IntegerType).min(1).max(10))  //query parameter\n    ): _*\n  )\n  .count(count.records(2))\n</code></pre> <p>In <code>docker/data/custom/task/http/http-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    count:\n      records: 2\n    fields:\n      - name: \"httpUrl\"\n        fields:\n          - name: \"url\"\n            static: \"http://localhost:80/anything/{id}\"\n          - name: \"method\"\n            static: \"GET\"\n          - name: \"pathParam\"\n            fields:\n              - name: \"id\"\n          - name: \"queryParam\"\n            fields:\n              - name: \"limit\"\n                type: \"integer\"\n                options:\n                  min: 1\n                  max: 10\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add name as <code>httpUrl</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>httpUrl</code></li> <li>Add name as <code>url</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>Static</code> and enter <code>http://localhost:80/anything/{id}</code></li> <li>Click on <code>+ Field</code> and add name as <code>method</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Static</code>. Then enter <code>GET</code></li> <li>Click on <code>+ Field</code> and add name as <code>pathParam</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>pathParam</code> and add name as <code>id</code></li> <li>Click on <code>+ Field</code> and add name as <code>queryParam</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>queryParam</code> and add name as <code>limit</code></li> <li>Click <code>+</code> next to data type and select <code>Min</code> and enter <code>1</code>. Similarly, select <code>Max</code> and enter <code>10</code></li> </ol> </li> </ol>"},{"location":"docs/guide/data-source/http/http/#http-headers","title":"HTTP Headers","text":"<p>HTTP headers can also be generated and have values that are based on the request payload.</p> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\")\n        .fields(\n                field().httpHeader(\"Content-Type\").staticValue(\"application/json\"),\n                field().httpHeader(\"Content-Length\"), //automatically calculated for you\n                field().httpHeader(\"X-Account-Id\").sql(\"body.account_id\")\n        )\n        ...\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(\n    field.httpHeader(\"Content-Type\").static(\"application/json\"),\n    field.httpHeader(\"Content-Length\"), //automatically calculated for you\n    field.httpHeader(\"X-Account-Id\").sql(\"body.account_id\")\n  )\n  ...\n</code></pre> <p>In <code>docker/data/custom/task/http/http-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    count:\n      records: 2\n    fields:\n      - name: \"httpHeaders\"\n        fields:\n          - name: \"Content-Type\"\n            static: \"application/json\"\n          - name: \"Content-Length\"\n          - name: \"X-Account-Id\"\n            options:\n              sql: \"body.account_id\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code></li> <li>Add name as <code>httpHeader</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>httpHeader</code></li> <li>Add name as <code>Content-Type</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>Static</code> and enter <code>application/json</code></li> <li>Click on <code>+ Field</code> and add name as <code>Content-Length</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click on <code>+ Field</code> and add name as <code>X-Account-Id</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Sql</code> and enter <code>body.account_id</code></li> </ol>"},{"location":"docs/guide/data-source/http/http/#http-body","title":"HTTP Body","text":"<p>HTTP body can be currently formed as a JSON structure that is generated from the metadata you define.</p> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\")\n        .fields(\n                field().httpBody(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n                        field().name(\"details\").fields(\n                                field().name(\"name\").expression(\"#{Name.name}\"),\n                                field().name(\"age\").type(IntegerType.instance()).max(100)\n                        )\n                )\n        )\n        ...\n</code></pre> <pre><code>val httpTask = http(\"my_http\")\n  .fields(\n    field.httpBody(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n      field.name(\"details\").fields(\n        field.name(\"name\").expression(\"#{Name.name}\"),\n        field.name(\"age\").`type`(IntegerType).max(100)\n      )\n    )\n  )\n  ...\n</code></pre> <p>In <code>docker/data/custom/task/http/http-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    count:\n      records: 2\n    fields:\n      - name: \"httpBody\"\n        fields:\n          - name: \"account_id\"\n            options:\n              regex: \"ACC[0-9]{8}\"\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n                options:\n                  expression: \"#{Name.name}\"\n              - name: \"age\"\n                type: Integer\n                options:\n                  max: 100\n</code></pre></p> <ol> <li>Click on <code>+ Field</code></li> <li>Add name as <code>httpBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>httpBody</code></li> <li>Add name as <code>account_id</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Regex</code> and enter <code>ACC[0-9]{8}</code></li> <li>Click on <code>+ Field</code> and add name as <code>details</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>details</code></li> <li>Add name as <code>name</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Faker expression</code> and enter <code>#{Name.name}</code></li> <li>Click on <code>+ Field</code> under <code>details</code></li> <li>Add name as <code>age</code></li> <li>Click on <code>Select data type</code> and select <code>integer</code></li> <li>Click <code>+</code> next to data type and select <code>Max</code> and enter <code>100</code></li> </ol>"},{"location":"docs/guide/data-source/http/http/#ordering","title":"Ordering","text":"<p>If you wanted to change the ordering of the requests, you can alter the order from within the OpenAPI/Swagger document. This is particularly useful when you want to simulate the same flow that users would take when utilising your application (i.e. create account, query account, update account).</p>"},{"location":"docs/guide/data-source/http/http/#rows-per-second","title":"Rows per second","text":"<p>By default, Data Caterer will push requests per method and endpoint at a rate of around 5 requests per second. If you want to alter this value, you can do so via the below configuration. The lowest supported requests per second is 1.</p> JavaScalaYAML <pre><code>import io.github.datacatering.datacaterer.api.model.Constants;\n\n...\nvar httpTask = http(\"my_http\", Map.of(Constants.ROWS_PER_SECOND(), \"1\"))\n        ...\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.model.Constants.ROWS_PER_SECOND\n\n...\nval httpTask = http(\"my_http\", options = Map(ROWS_PER_SECOND -&gt; \"1\"))\n  ...\n</code></pre> <p>In <code>docker/data/custom/task/http/openapi-task.yaml</code>: <pre><code>name: \"http_task\"\nsteps:\n  - name: \"my_petstore\"\n    options:\n      metadataSourceType: \"openapi\"\n      schemaLocation: \"/opt/app/mount/http/petstore.json\"\n      rowsPerSecond: \"1\"\n    ...\n</code></pre></p>"},{"location":"docs/guide/data-source/http/http/#validation","title":"Validation","text":"<p>Once you have generated HTTP requests, you may also want to validate the responses to ensure your service is responding as expected.</p> <p>The following fields are made available to you to validate against:</p> Field Inner Field Data Type Example request method String GET request url String http://localhost:8080/my/path request headers Map[String, String] Content-Length -&gt; 200 request body String my-body request startTime Long 1733408207499 response contentType String application/json response headers Map[String, String] Content-Length -&gt; 200 response body String my-body response statusCode Int 200 response statusText String OK response timeTakenMs Long 120 JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\", Map.of(Constants.VALIDATION_IDENTIFIER(), \"POST/pets\"))\n        .fields(\n                ...\n        )\n        .validations(\n                validation().field(\"request.method\").isEqual(\"POST\"),\n                validation().field(\"response.statusCode\").isEqual(200),\n                validation().field(\"response.timeTakenMs\").lessThan(100),\n                validation().field(\"response.headers.Content-Length\").greaterThan(0),\n                validation().field(\"response.headers.Content-Type\").isEqual(\"application/json\")\n        )\n</code></pre> <pre><code>val httpTask = http(\"my_http\", options = Map(VALIDATION_IDENTIFIER -&gt; \"POST/pets\"))\n  .fields(\n    ...\n  )\n  .validations(\n    validation.field(\"request.method\").isEqual(\"POST\"),\n    validation.field(\"response.statusCode\").isEqual(200),\n    validation.field(\"response.timeTakenMs\").lessThan(100),\n    validation.field(\"response.headers.Content-Length\").greaterThan(0),\n    validation.field(\"response.headers.Content-Type\").isEqual(\"application/json\"),\n  )\n</code></pre> <p>In <code>docker/data/custom/validation/http/http-validation.yaml</code>: <pre><code>name: \"http_checks\"\ndataSources:\n  my_http:\n    - options:\n        validationIdentifier: \"POST/pets\"\n      validations:\n        - expr: \"request.method == 'POST'\"\n        - expr: \"response.statusCode == 200\"\n        - expr: \"response.timeTakenMs &lt; 100\"\n        - expr: \"response.headers.Content-Length &gt; 0\"\n        - expr: \"response.headers.Content-Type == 'application/json'\"\n</code></pre></p> <ol> <li>Open <code>Validation</code></li> <li>Click on <code>Manual</code> checkbox</li> <li>Click on <code>+ Validation</code> button and click <code>Select validation type</code> and select <code>Field</code></li> <li>Enter <code>request.method</code> in the <code>Field</code> text box</li> <li>Click on <code>+</code> next to <code>Operator</code> and select <code>Equal</code></li> <li>Enter <code>POST</code> in the <code>Equal</code> text box</li> <li>Continue adding validations for <code>response.statusCode</code>, <code>response.timeTakenMs</code>, <code>response.headers.Content-Length</code> and <code>response.headers.Content-Type</code></li> </ol> <p>If you want to validate data from an HTTP source, follow the validation documentation found here to help guide you.</p> <p>Check out the full example under <code>HttpPlanRun</code> in the example repo.</p>"},{"location":"docs/guide/data-source/messaging/kafka/","title":"Kafka","text":"<p>Creating a data generator for Kafka. You will build a Docker image that will be able to populate data in kafka for the topics you configure.</p>"},{"location":"docs/guide/data-source/messaging/kafka/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/messaging/kafka/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a Kafka instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/messaging/kafka/#kafka-setup","title":"Kafka Setup","text":"<p>Next, let's make sure you have an instance of Kafka up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d kafka\n</code></pre>"},{"location":"docs/guide/data-source/messaging/kafka/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedKafkaJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedKafkaPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-kafka.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyAdvancedKafkaJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyAdvancedKafkaPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-kafka.yaml</code>: <pre><code>name: \"my_kafka_plan\"\ndescription: \"Create account data via Kafka\"\ntasks:\n  - name: \"kafka_task\"\n    dataSourceName: \"my_kafka\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_kafka</code></li> <li>Click on <code>Select data source type..</code> and select <code>Kafka</code></li> <li>Set URL as <code>localhost:9092</code><ol> <li>Optionally, we could set a topic name but if you have more than 1 topic, you would have to create new connection for each topic</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_kafka</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_kafka_plan</code></li> <li>Set task name to <code>kafka_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_kafka</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/messaging/kafka/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to Kafka.</p> JavaScalaYAMLUI <pre><code>var accountTask = kafka(\n    \"my_kafka\",       //name\n    \"localhost:9092\", //url\n    Map.of()          //optional additional connection options\n);\n</code></pre> <p>Additional options can be found here.</p> <pre><code>val accountTask = kafka(\n    \"my_kafka\",       //name\n    \"localhost:9092\", //url\n    Map()             //optional additional connection options\n)\n</code></pre> <p>Additional options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>kafka {\n    my_kafka {\n        kafka.bootstrap.servers = \"localhost:9092\"\n        kafka.bootstrap.servers = ${?KAFKA_BOOTSTRAP_SERVERS}\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/messaging/kafka/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>account-topic</code> that is already defined under<code>docker/data/kafka/setup_kafka.sh</code>. This topic should already be setup for you if you followed this step. We can check if the topic is set up already via the following command:</p> <pre><code>docker exec docker-kafkaserver-1 kafka-topics --bootstrap-server localhost:9092 --list\n</code></pre> <p>Trimming the connection details to work with the docker-compose Kafka, we have a base Kafka connection to define the topic we will publish to. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScalaYAMLUI <pre><code>{\n    var kafkaTask = kafka(\"my_kafka\", \"kafkaserver:29092\")\n            .topic(\"account-topic\")\n            .fields(\n                    field().name(\"key\").sql(\"body.account_id\"),\n                    //field().name(\"partition\").type(IntegerType.instance()),   //can define message partition here\n                    field().messageHeaders(\n                            field().messageHeader(\"account-id\", \"body.account_id\"),\n                            field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n                    )\n            ).fields(\n                    field().messageBody(\n                            field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n                            field().name(\"year\").type(IntegerType.instance()).min(2021).max(2023),\n                            field().name(\"amount\").type(DoubleType.instance()),\n                            field().name(\"details\")\n                                    .fields(\n                                            field().name(\"name\").expression(\"#{Name.name}\"),\n                                            field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n                                            field().name(\"updated_by\")\n                                                    .fields(\n                                                            field().name(\"user\"),\n                                                            field().name(\"time\").type(TimestampType.instance())\n                                                    )\n                                    ),\n                            field().name(\"transactions\").type(ArrayType.instance())\n                                    .fields(\n                                            field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                                            field().name(\"amount\").type(DoubleType.instance())\n                                    )\n                    )\n            )\n}\n</code></pre> <pre><code>val kafkaTask = kafka(\"my_kafka\", \"kafkaserver:29092\")\n  .topic(\"account-topic\")\n  .fields(\n    field.name(\"key\").sql(\"body.account_id\"),\n    //field.name(\"partition\").type(IntegerType),  can define partition here\n    field.messageHeaders(\n      field.messageHeader(\"account-id\", \"body.account_id\"),\n      field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n    )\n  )\n  .fields(\n    field.messageBody(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n      field.name(\"year\").`type`(IntegerType).min(2021).max(2023),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\"),\n      field.name(\"amount\").`type`(DoubleType),\n      field.name(\"details\").`type`(StructType)\n        .fields(\n          field.name(\"name\").expression(\"#{Name.name}\"),\n          field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n          field.name(\"updated_by\").`type`(StructType)\n            .fields(\n              field.name(\"user\"),\n              field.name(\"time\").`type`(TimestampType),\n            ),\n        ),\n      field.name(\"transactions\").`type`(ArrayType)\n        .fields(\n          field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n          field.name(\"amount\").`type`(DoubleType),\n        )\n    )\n  )\n</code></pre> <p>In <code>docker/data/custom/task/kafka/kafka-task.yaml</code>: <pre><code>name: \"kafka_task\"\nsteps:\n  - name: \"kafka_account\"\n    options:\n      topic: \"account-topic\"\n    fields:\n      - name: \"key\"\n        options:\n          sql: \"body.account_id\"\n      - name: \"messageBody\"\n        fields:\n          - name: \"account_id\"\n          - name: \"year\"\n            type: \"int\"\n            options:\n              min: \"2021\"\n              max: \"2022\"\n          - name: \"amount\"\n            type: \"double\"\n            options:\n              min: \"10.0\"\n              max: \"100.0\"\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add name as <code>key</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Sql</code>. Then enter <code>body.account_id</code></li> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageBody</code> with your own metadata</li> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol> </li> </ol>"},{"location":"docs/guide/data-source/messaging/kafka/#fields","title":"Fields","text":"<p>The schema defined for Kafka has a format that needs to be followed as noted above. Specifically, the required fields are: - <code>messageBody</code></p> <p>Whilst, the other fields are optional: - <code>key</code> - <code>partition</code> - <code>messageHeaders</code></p>"},{"location":"docs/guide/data-source/messaging/kafka/#message-headers","title":"Message Headers","text":"<p>If your messages contain headers, you can follow the details below on generating header values. These can be based off  values contained within you message body or could be static values, just like any other generated field. The main restriction imposed here is that the <code>key</code> of the message header is static and the <code>value</code> has to be a valid SQL expression.</p> JavaScalaYAMLUI <pre><code>field().messageHeaders(\n        field().messageHeader(\"account-id\", \"body.account_id\"),\n        field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n)\n</code></pre> <pre><code>field.messageHeaders(\n  field.messageHeader(\"account-id\", \"body.account_id\"),\n  field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n)\n</code></pre> <p>In <code>docker/data/custom/task/kafka/kafka-task.yaml</code>: <pre><code>name: \"kafka_task\"\nsteps:\n  - name: \"kafka_account\"\n    options:\n      topic: \"account-topic\"\n    fields:\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/kafka/#transactions","title":"transactions","text":"<p><code>transactions</code> is an array that contains an inner structure of <code>txn_date</code> and <code>amount</code>. The size of the array generated can be controlled via <code>arrayMinLength</code> and <code>arrayMaxLength</code>.</p> JavaScalaYAMLUI <pre><code>field().name(\"transactions\").type(ArrayType.instance())\n        .fields(\n                field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                field().name(\"amount\").type(DoubleType.instance())\n        )\n</code></pre> <pre><code>field.name(\"transactions\").`type`(ArrayType)\n  .fields(\n    field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n    field.name(\"amount\").`type`(DoubleType),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/kafka/kafka-task.yaml</code>: <pre><code>name: \"kafka_task\"\nsteps:\n  - name: \"kafka_account\"\n    options:\n      topic: \"account-topic\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n</code></pre></p> <p>Warning</p> <p>Inner field definition for array type is currently not supported from the UI. Will be added in the near future!</p>"},{"location":"docs/guide/data-source/messaging/kafka/#details","title":"details","text":"<p><code>details</code> is another example of a nested schema structure where it also has a nested structure itself in <code>updated_by</code>. One thing to note here is the <code>first_txn_date</code> field has a reference to the <code>body.transactions</code> array where it will  sort the array by <code>txn_date</code> and get the first element.</p> JavaScalaYAMLUI <pre><code>field().name(\"details\")\n        .fields(\n                field().name(\"name\").expression(\"#{Name.name}\"),\n                field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n                field().name(\"updated_by\")\n                        .fields(\n                                field().name(\"user\"),\n                                field().name(\"time\").type(TimestampType.instance())\n                        )\n        )\n</code></pre> <pre><code>field.name(\"details\")\n  .fields(\n    field.name(\"name\").expression(\"#{Name.name}\"),\n    field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n    field.name(\"updated_by\")\n      .fields(\n        field.name(\"user\"),\n        field.name(\"time\").`type`(TimestampType),\n      ),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/kafka/kafka-task.yaml</code>: <pre><code>name: \"kafka_task\"\nsteps:\n  - name: \"kafka_account\"\n    options:\n      topic: \"account-topic\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>details</code></li> <li>Add additional fields under <code>details</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/kafka/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\");\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>folders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/messaging/kafka/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh AdvancedKafkaJavaPlanRun\ndocker exec docker-kafkaserver-1 kafka-console-consumer --bootstrap-server localhost:9092 --topic account-topic --from-beginning\n</code></pre> <pre><code>./run.sh AdvancedKafkaPlanRun\ndocker exec docker-kafkaserver-1 kafka-console-consumer --bootstrap-server localhost:9092 --topic account-topic --from-beginning\n</code></pre> <pre><code>./run.sh my-kafka.yaml\ndocker exec docker-kafkaserver-1 kafka-console-consumer --bootstrap-server localhost:9092 --topic account-topic --from-beginning\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <pre><code>{\"account_id\":\"ACC56292178\",\"year\":2022,\"amount\":18338.627721151555,\"details\":{\"name\":\"Isaias Reilly\",\"first_txn_date\":\"2021-01-22\",\"updated_by\":{\"user\":\"FgYXbKDWdhHVc3\",\"time\":\"2022-12-30T13:49:07.309Z\"}},\"transactions\":[{\"txn_date\":\"2021-01-22\",\"amount\":30556.52125487579},{\"txn_date\":\"2021-10-29\",\"amount\":39372.302259554635},{\"txn_date\":\"2021-10-29\",\"amount\":61887.31389495968}]}\n{\"account_id\":\"ACC37729457\",\"year\":2022,\"amount\":96885.31758764731,\"details\":{\"name\":\"Randell Witting\",\"first_txn_date\":\"2021-06-30\",\"updated_by\":{\"user\":\"HCKYEBHN8AJ3TB\",\"time\":\"2022-12-02T02:05:01.144Z\"}},\"transactions\":[{\"txn_date\":\"2021-06-30\",\"amount\":98042.09647765031},{\"txn_date\":\"2021-10-06\",\"amount\":41191.43564742036},{\"txn_date\":\"2021-11-16\",\"amount\":78852.08184809204},{\"txn_date\":\"2021-10-09\",\"amount\":13747.157653571106}]}\n{\"account_id\":\"ACC23127317\",\"year\":2023,\"amount\":81164.49304198896,\"details\":{\"name\":\"Jed Wisozk\",\"updated_by\":{\"user\":\"9MBFZZ\",\"time\":\"2023-07-12T05:56:52.397Z\"}},\"transactions\":[]}\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/","title":"RabbitMQ","text":"<p>Creating a data generator for RabbitMQ. You will build a Docker image that will be able to populate data in RabbitMQ for the queues/topics you configure.</p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a RabbitMQ instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#rabbitmq-setup","title":"RabbitMQ Setup","text":"<p>Next, let's make sure you have an instance of RabbitMQ up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d rabbitmq\n</code></pre> <p>Open up localhost:15672/#/queues and login with <code>guest:guest</code>. Create a new queue  with name <code>accounts</code>.</p> <p></p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedRabbitMQJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedRabbitMQPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-rabbitmq.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyAdvancedRabbitMQJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyAdvancedRabbitMQPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-rabbitmq.yaml</code>: <pre><code>name: \"my_rabbitmq_plan\"\ndescription: \"Create account data in RabbitMQ\"\ntasks:\n  - name: \"rabbitmq_task\"\n    dataSourceName: \"my_rabbitmq\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_rabbitmq</code></li> <li>Click on <code>Select data source type..</code> and select <code>RabbitMQ</code></li> <li>Set <code>URL</code> as <code>ampq://host.docker.internal:5672</code><ol> <li>Optionally, we could set the JNDI destination (queue or topic) but we would have to create a new connection for each queue or topic</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_rabbitmq</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_rabbitmq_plan</code></li> <li>Set task name to <code>rabbitmq_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_rabbitmq</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to RabbitMQ.</p> JavaScalaYAMLUI <pre><code>var accountTask = rabbitmq(\n    \"my_rabbitmq\",                        //name\n    \"ampq://host.docker.internal:5672\", //url\n    Map.of()                            //optional additional connection options\n);\n</code></pre> <p>Additional connection options can be found here.</p> <pre><code>val accountTask = rabbitmq(\n    \"my_rabbitmq\",                        //name\n    \"ampq://host.docker.internal:5672\", //url\n    Map()                               //optional additional connection options\n)\n</code></pre> <p>Additional connection options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jms {\n    rabbitmq {\n        initialContextFactory = \"com.rabbitmqsystems.jndi.SolJNDIInitialContextFactory\"\n        initialContextFactory = ${?SOLACE_INITIAL_CONTEXT_FACTORY}\n        connectionFactory = \"/jms/cf/default\"\n        connectionFactory = ${?SOLACE_CONNECTION_FACTORY}\n        url = \"smf://rabbitmqserver:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>rest_test_queue</code> or <code>rest_test_topic</code> that is already created for us from this step.</p> <p>Trimming the connection details to work with the docker-compose RabbitMQ, we have a base RabbitMQ connection to define the JNDI destination we will publish to. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScalaYAMLUI <pre><code>{\n    var rabbitmqTask = rabbitmq(\"my_rabbitmq\", \"ampq://host.docker.internal:5672\")\n            .destination(\"accounts\")\n            .fields(\n                    //field().name(\"partition\").type(IntegerType.instance()),   can define JMS priority here\n                    field().messageHeaders(   //set message properties via headers field\n                            field().messageHeader(\"account-id\", \"body.account_id\"),\n                            field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n                    )\n            ).fields(\n                    field().messageBody(\n                            field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n                            field().name(\"year\").type(IntegerType.instance()).min(2021).max(2023),\n                            field().name(\"amount\").type(DoubleType.instance()),\n                            field().name(\"details\")\n                                    .fields(\n                                            field().name(\"name\").expression(\"#{Name.name}\"),\n                                            field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n                                            field().name(\"updated_by\")\n                                                    .fields(\n                                                            field().name(\"user\"),\n                                                            field().name(\"time\").type(TimestampType.instance())\n                                                    )\n                                    ),\n                            field().name(\"transactions\").type(ArrayType.instance())\n                                    .fields(\n                                            field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                                            field().name(\"amount\").type(DoubleType.instance())\n                                    )\n                    )\n            )\n            .count(count().records(10));\n}\n</code></pre> <pre><code>val rabbitmqTask = rabbitmq(\"my_rabbitmq\", \"ampq://host.docker.internal:5672\")\n  .destination(\"accounts\")\n  .fields(\n    //field.name(\"partition\").type(IntegerType),  can define JMS priority here\n    field.messageHeaders(                         //set message properties via headers field\n      field.messageHeader(\"account-id\", \"body.account_id\"),\n      field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n    )\n  )\n  .fields(\n    field.messageBody(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n      field.name(\"year\").`type`(IntegerType).min(2021).max(2023),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\"),\n      field.name(\"amount\").`type`(DoubleType),\n      field.name(\"details\").`type`(StructType)\n        .fields(\n          field.name(\"name\").expression(\"#{Name.name}\"),\n          field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n          field.name(\"updated_by\").`type`(StructType)\n            .fields(\n              field.name(\"user\"),\n              field.name(\"time\").`type`(TimestampType),\n            ),\n        ),\n      field.name(\"transactions\").`type`(ArrayType)\n        .fields(\n          field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n          field.name(\"amount\").`type`(DoubleType),\n        )\n    )\n  )\n  .count(count.records(10))\n</code></pre> <p>In <code>docker/data/custom/task/rabbitmq/rabbitmq-task.yaml</code>: <pre><code>name: \"rabbitmq_task\"\nsteps:\n  - name: \"rabbitmq_account\"\n    options:\n      destinationName: \"accounts\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"account_id\"\n          - name: \"year\"\n            type: \"int\"\n            options:\n              min: \"2021\"\n              max: \"2022\"\n          - name: \"amount\"\n            type: \"double\"\n            options:\n              min: \"10.0\"\n              max: \"100.0\"\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add name as <code>key</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Sql</code>. Then enter <code>body.account_id</code></li> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageBody</code> with your own metadata</li> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol> </li> </ol>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#fields","title":"Fields","text":"<p>The schema defined for RabbitMQ has a format that needs to be followed as noted above. Specifically, the required fields are: - <code>messageBody</code></p> <p>Whilst, the other fields are optional:</p> <ul> <li><code>partition</code> - refers to JMS priority of the message</li> <li><code>messageHeaders</code> - refers to JMS message properties</li> </ul>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#message-headers","title":"Message Headers","text":"<p>If your messages contain headers, you can follow the details below on generating header values. These can be based off values contained within you message body or could be static values, just like any other generated field. The main restriction imposed here is that the <code>key</code> of the message header is static and the <code>value</code> has to be a valid SQL  expression.</p> JavaScalaYAMLUI <pre><code>field().messageHeaders(\n        field().messageHeader(\"account-id\", \"body.account_id\"),\n        field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n)\n</code></pre> <pre><code>field.messageHeaders(\n  field.messageHeader(\"account-id\", \"body.account_id\"),\n  field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n)\n</code></pre> <p>In <code>docker/data/custom/task/rabbitmq/rabbitmq-task.yaml</code>: <pre><code>name: \"rabbitmq_task\"\nsteps:\n  - name: \"rabbitmq_account\"\n    options:\n      destinationName: \"accounts\"\n    fields:\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#transactions","title":"transactions","text":"<p><code>transactions</code> is an array that contains an inner structure of <code>txn_date</code> and <code>amount</code>. The size of the array generated can be controlled via <code>arrayMinLength</code> and <code>arrayMaxLength</code>.</p> JavaScalaYAMLUI <pre><code>field().name(\"transactions\").type(ArrayType.instance())\n        .fields(\n                field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                field().name(\"amount\").type(DoubleType.instance())\n        )\n</code></pre> <pre><code>field.name(\"transactions\").`type`(ArrayType)\n  .fields(\n    field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n    field.name(\"amount\").`type`(DoubleType),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/rabbitmq/rabbitmq-task.yaml</code>: <pre><code>name: \"rabbitmq_task\"\nsteps:\n  - name: \"rabbitmq_account\"\n    options:\n      destinationName: \"accounts\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n</code></pre></p> <p>Warning</p> <p>Inner field definition for array type is currently not supported from the UI. Will be added in the near future!</p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#details","title":"details","text":"<p><code>details</code> is another example of a nested schema structure where it also has a nested structure itself in <code>updated_by</code>. One thing to note here is the <code>first_txn_date</code> field has a reference to the <code>content.transactions</code> array where it will sort the array by <code>txn_date</code> and get the first element.</p> JavaScalaYAMLUI <pre><code>field().name(\"details\")\n        .fields(\n                field().name(\"name\").expression(\"#{Name.name}\"),\n                field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(content.transactions.txn_date), 1)\"),\n                field().name(\"updated_by\")\n                        .fields(\n                                field().name(\"user\"),\n                                field().name(\"time\").type(TimestampType.instance())\n                        )\n        )\n</code></pre> <pre><code>field.name(\"details\")\n  .fields(\n    field.name(\"name\").expression(\"#{Name.name}\"),\n    field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(content.transactions.txn_date), 1)\"),\n    field.name(\"updated_by\")\n      .fields(\n        field.name(\"user\"),\n        field.name(\"time\").`type`(TimestampType),\n      ),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/rabbitmq/rabbitmq-task.yaml</code>: <pre><code>name: \"rabbitmq_task\"\nsteps:\n  - name: \"rabbitmq_account\"\n    options:\n      destinationName: \"accounts\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>details</code></li> <li>Add additional fields under <code>details</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\");\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>folders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>rabbitmqTask</code>, we have to call <code>execute</code>.</p>"},{"location":"docs/guide/data-source/messaging/rabbitmq/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh AdvancedRabbitMQJavaPlanRun\n#after completing, check http://localhost:15672/#/queues/%2F/accounts from browser\n</code></pre> <pre><code>./run.sh AdvancedRabbitMQPlanRun\n#after completing, check http://localhost:15672/#/queues/%2F/accounts from browser\n</code></pre> <pre><code>./run.sh my-rabbitmq.yaml\n#after completing, check http://localhost:15672/#/queues/%2F/accounts from browser\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <p></p> <p>You can check the message payload by clicking on <code>Get Message(s)</code>. Once you copy the payload, you can run a command  like: <code>echo \"&lt;payload&gt;\" | base64 -D</code> to see the actual message.</p> <p></p> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed. Or view the sample report found here.</p>"},{"location":"docs/guide/data-source/messaging/solace/","title":"Solace","text":"<p>Creating a data generator for Solace. You will build a Docker image that will be able to populate data in Solace for the queues/topics you configure.</p> <p></p>"},{"location":"docs/guide/data-source/messaging/solace/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/messaging/solace/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p> <p>If you already have a Solace instance running, you can skip to this step.</p>"},{"location":"docs/guide/data-source/messaging/solace/#solace-setup","title":"Solace Setup","text":"<p>Next, let's make sure you have an instance of Solace up and running in your local environment. This will make it easy for us to iterate and check our changes.</p> <pre><code>cd docker\ndocker-compose up -d solace\n</code></pre> <p>Open up localhost:8080 and login with <code>admin:admin</code> and check there is the <code>default</code> VPN like below. Notice there is 2 queues/topics created. If you do not see 2 created, try to run the script found under <code>docker/data/solace/setup_solace.sh</code> and change the <code>host</code> to <code>localhost</code>.</p> <p></p>"},{"location":"docs/guide/data-source/messaging/solace/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedSolaceJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedSolacePlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-solace.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyAdvancedSolaceJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyAdvancedSolacePlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-solace.yaml</code>: <pre><code>name: \"my_solace_plan\"\ndescription: \"Create account data in Solace\"\ntasks:\n  - name: \"solace_task\"\n    dataSourceName: \"my_solace\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_solace</code></li> <li>Click on <code>Select data source type..</code> and select <code>Solace</code></li> <li>Set <code>URL</code> as <code>smf://host.docker.internal:55554</code><ol> <li>Optionally, we could set the JNDI destination (queue or topic) but we would have to create a new connection for each queue or topic</li> </ol> </li> <li>Click on <code>Create</code></li> <li>You should see your connection <code>my_solace</code> show under <code>Existing connections</code></li> <li>Click on <code>Home</code> towards the top of the screen</li> <li>Set plan name to <code>my_solace_plan</code></li> <li>Set task name to <code>solace_task</code></li> <li>Click on <code>Select data source..</code> and select <code>my_solace</code></li> </ol> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/data-source/messaging/solace/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to connect to Solace.</p> JavaScalaYAMLUI <pre><code>var accountTask = solace(\n    \"my_solace\",                        //name\n    \"smf://host.docker.internal:55554\", //url\n    Map.of()                            //optional additional connection options\n);\n</code></pre> <p>Additional connection options can be found here.</p> <pre><code>val accountTask = solace(\n    \"my_solace\",                        //name\n    \"smf://host.docker.internal:55554\", //url\n    Map()                               //optional additional connection options\n)\n</code></pre> <p>Additional connection options can be found here.</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>jms {\n    solace {\n        initialContextFactory = \"com.solacesystems.jndi.SolJNDIInitialContextFactory\"\n        initialContextFactory = ${?SOLACE_INITIAL_CONTEXT_FACTORY}\n        connectionFactory = \"/jms/cf/default\"\n        connectionFactory = ${?SOLACE_CONNECTION_FACTORY}\n        url = \"smf://solaceserver:55555\"\n        url = ${?SOLACE_URL}\n        user = \"admin\"\n        user = ${?SOLACE_USER}\n        password = \"admin\"\n        password = ${?SOLACE_PASSWORD}\n        vpnName = \"default\"\n        vpnName = ${?SOLACE_VPN}\n    }\n}\n</code></pre></p> <ol> <li>We have already created the connection details in this step</li> </ol>"},{"location":"docs/guide/data-source/messaging/solace/#schema","title":"Schema","text":"<p>Let's create a task for inserting data into the <code>rest_test_queue</code> or <code>rest_test_topic</code> that is already created for us from this step.</p> <p>Trimming the connection details to work with the docker-compose Solace, we have a base Solace connection to define the JNDI destination we will publish to. Let's define each field along with their corresponding data type. You will notice that the <code>text</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScalaYAMLUI <pre><code>{\n    var solaceTask = solace(\"my_solace\", \"smf://host.docker.internal:55554\")\n            .destination(\"/JNDI/Q/rest_test_queue\")\n            .fields(\n                    //field().name(\"partition\").type(IntegerType.instance()),   can define JMS priority here\n                    field().messageHeaders(   //set message properties via headers field\n                            field().messageHeader(\"account-id\", \"body.account_id\"),\n                            field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n                    )\n            ).fields(\n                    field().messageBody(\n                            field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n                            field().name(\"year\").type(IntegerType.instance()).min(2021).max(2023),\n                            field().name(\"amount\").type(DoubleType.instance()),\n                            field().name(\"details\")\n                                    .fields(\n                                            field().name(\"name\").expression(\"#{Name.name}\"),\n                                            field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n                                            field().name(\"updated_by\")\n                                                    .fields(\n                                                            field().name(\"user\"),\n                                                            field().name(\"time\").type(TimestampType.instance())\n                                                    )\n                                    ),\n                            field().name(\"transactions\").type(ArrayType.instance())\n                                    .fields(\n                                            field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                                            field().name(\"amount\").type(DoubleType.instance())\n                                    )\n                    )\n            )\n            .count(count().records(10));\n}\n</code></pre> <pre><code>val solaceTask = solace(\"my_solace\", \"smf://host.docker.internal:55554\")\n  .destination(\"/JNDI/Q/rest_test_queue\")\n  .fields(\n    //field.name(\"partition\").type(IntegerType),  can define JMS priority here\n    field.messageHeaders(                         //set message properties via headers field\n      field.messageHeader(\"account-id\", \"body.account_id\"),\n      field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n    )\n  )\n  .fields(\n    field.messageBody(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n      field.name(\"year\").`type`(IntegerType).min(2021).max(2023),\n      field.name(\"account_status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\"),\n      field.name(\"amount\").`type`(DoubleType),\n      field.name(\"details\").`type`(StructType)\n        .fields(\n          field.name(\"name\").expression(\"#{Name.name}\"),\n          field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"),\n          field.name(\"updated_by\").`type`(StructType)\n            .fields(\n              field.name(\"user\"),\n              field.name(\"time\").`type`(TimestampType),\n            ),\n        ),\n      field.name(\"transactions\").`type`(ArrayType)\n        .fields(\n          field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n          field.name(\"amount\").`type`(DoubleType),\n        )\n    )\n  )\n  .count(count.records(10))\n</code></pre> <p>In <code>docker/data/custom/task/solace/solace-task.yaml</code>: <pre><code>name: \"solace_task\"\nsteps:\n  - name: \"solace_account\"\n    options:\n      destinationName: \"/JNDI/Q/rest_test_queue\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"account_id\"\n          - name: \"year\"\n            type: \"int\"\n            options:\n              min: \"2021\"\n              max: \"2022\"\n          - name: \"amount\"\n            type: \"double\"\n            options:\n              min: \"10.0\"\n              max: \"100.0\"\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add name as <code>key</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Click <code>+</code> next to data type and select <code>Sql</code>. Then enter <code>body.account_id</code></li> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageBody</code> with your own metadata</li> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol> </li> </ol>"},{"location":"docs/guide/data-source/messaging/solace/#fields","title":"Fields","text":"<p>The schema defined for Solace has a format that needs to be followed as noted above. Specifically, the required fields are: - <code>messageBody</code></p> <p>Whilst, the other fields are optional:</p> <ul> <li><code>partition</code> - refers to JMS priority of the message</li> <li><code>messageHeaders</code> - refers to JMS message properties</li> </ul>"},{"location":"docs/guide/data-source/messaging/solace/#message-headers","title":"Message Headers","text":"<p>If your messages contain headers, you can follow the details below on generating header values. These can be based off values contained within you message body or could be static values, just like any other generated field. The main restriction imposed here is that the <code>key</code> of the message header is static and the <code>value</code> has to be a valid SQL  expression.</p> JavaScalaYAMLUI <pre><code>field().messageHeaders(\n        field().messageHeader(\"account-id\", \"body.account_id\"),\n        field().messageHeader(\"updated\", \"body.details.updated_by-time\")\n)\n</code></pre> <pre><code>field.messageHeaders(\n  field.messageHeader(\"account-id\", \"body.account_id\"),\n  field.messageHeader(\"updated\", \"body.details.updated_by.time\"),\n)\n</code></pre> <p>In <code>docker/data/custom/task/solace/solace-task.yaml</code>: <pre><code>name: \"solace_task\"\nsteps:\n  - name: \"solace_account\"\n    options:\n      destinationName: \"/JNDI/Q/rest_test_queue\"\n    fields:\n      - name: \"messageHeaders\"\n        fields:\n          - name: \"account-id\"\n            options:\n              sql: \"body.account_id\"\n          - name: \"updated\"\n            options:\n              sql: \"body.details.update_by.time\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageHeaders</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageHeaders</code> and add name as <code>account_id</code></li> <li>Add additional fields under <code>messageHeaders</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/solace/#transactions","title":"transactions","text":"<p><code>transactions</code> is an array that contains an inner structure of <code>txn_date</code> and <code>amount</code>. The size of the array generated can be controlled via <code>arrayMinLength</code> and <code>arrayMaxLength</code>.</p> JavaScalaYAMLUI <pre><code>field().name(\"transactions\").type(ArrayType.instance())\n        .fields(\n                field().name(\"txn_date\").type(DateType.instance()).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n                field().name(\"amount\").type(DoubleType.instance())\n        )\n</code></pre> <pre><code>field.name(\"transactions\").`type`(ArrayType)\n  .fields(\n    field.name(\"txn_date\").`type`(DateType).min(Date.valueOf(\"2021-01-01\")).max(\"2021-12-31\"),\n    field.name(\"amount\").`type`(DoubleType),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/solace/solace-task.yaml</code>: <pre><code>name: \"solace_task\"\nsteps:\n  - name: \"solace_account\"\n    options:\n      destinationName: \"/JNDI/Q/rest_test_queue\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"transactions\"\n            type: \"array\"\n            fields:\n              - name: \"txn_date\"\n                type: \"date\"\n              - name: \"amount\"\n                type: \"double\"\n</code></pre></p> <p>Warning</p> <p>Inner field definition for array type is currently not supported from the UI. Will be added in the near future!</p>"},{"location":"docs/guide/data-source/messaging/solace/#details","title":"details","text":"<p><code>details</code> is another example of a nested schema structure where it also has a nested structure itself in <code>updated_by</code>. One thing to note here is the <code>first_txn_date</code> field has a reference to the <code>content.transactions</code> array where it will sort the array by <code>txn_date</code> and get the first element.</p> JavaScalaYAMLUI <pre><code>field().name(\"details\")\n        .fields(\n                field().name(\"name\").expression(\"#{Name.name}\"),\n                field().name(\"first_txn_date\").type(DateType.instance()).sql(\"ELEMENT_AT(SORT_ARRAY(content.transactions.txn_date), 1)\"),\n                field().name(\"updated_by\")\n                        .fields(\n                                field().name(\"user\"),\n                                field().name(\"time\").type(TimestampType.instance())\n                        )\n        )\n</code></pre> <pre><code>field.name(\"details\")\n  .fields(\n    field.name(\"name\").expression(\"#{Name.name}\"),\n    field.name(\"first_txn_date\").`type`(DateType).sql(\"ELEMENT_AT(SORT_ARRAY(content.transactions.txn_date), 1)\"),\n    field.name(\"updated_by\")\n      .fields(\n        field.name(\"user\"),\n        field.name(\"time\").`type`(TimestampType),\n      ),\n  )\n</code></pre> <p>In <code>docker/data/custom/task/solace/solace-task.yaml</code>: <pre><code>name: \"solace_task\"\nsteps:\n  - name: \"solace_account\"\n    options:\n      destinationName: \"/JNDI/Q/rest_test_queue\"\n    fields:\n      - name: \"messageBody\"\n        fields:\n          - name: \"details\"\n            fields:\n              - name: \"name\"\n              - name: \"first_txn_date\"\n                type: \"date\"\n                options:\n                  sql: \"ELEMENT_AT(SORT_ARRAY(body.transactions.txn_date), 1)\"\n              - name: \"updated_by\"\n                fields:\n                  - name: \"user\"\n                  - name: \"time\"\n                    type: \"timestamp\"\n</code></pre></p> <ol> <li>Click on <code>+ Field</code> and add name as <code>messageBody</code></li> <li>Click on <code>Select data type</code> and select <code>struct</code></li> <li>Click on <code>+ Field</code> under <code>messageBody</code> and add name as <code>details</code></li> <li>Add additional fields under <code>details</code> with your own metadata</li> </ol>"},{"location":"docs/guide/data-source/messaging/solace/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\");\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>folders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/messaging/solace/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>kafkaTask</code>, we have to call <code>execute</code>.</p>"},{"location":"docs/guide/data-source/messaging/solace/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh AdvancedSolaceJavaPlanRun\n#after completing, check http://localhost:8080 from browser\n</code></pre> <pre><code>./run.sh AdvancedSolacePlanRun\n#after completing, check http://localhost:8080 from browser\n</code></pre> <pre><code>./run.sh my-solace.yaml\n#after completing, check http://localhost:8080 from browser\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>Your output should look like this.</p> <p></p> <p>Unfortunately, there is no easy way to see the message content. You can check the message content from your application or service that consumes these messages.</p> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed. Or view the sample report found here.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/","title":"Data Contract CLI Source","text":"<p>Creating a data generator for a CSV file based on metadata stored in Data Contract CLI.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#data-contract-cli-setup","title":"Data Contract CLI Setup","text":"<p>We will be using the following Data Contract CLI file for this example.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedDataContractCliJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedDataContractCliPlanRun.scala</code></li> <li>YAML: <code>docker/data/customer/plan/my-datacontract-cli.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedDataContractCliJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedDataContractCliPlanRun extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-datacontract-cli.yaml</code>: <pre><code>name: \"my_datacontract_cli_plan\"\ndescription: \"Create account data in CSV via Data Contract CLI metadata\"\ntasks:\n  - name: \"csv_account_file\"\n    dataSourceName: \"customer_accounts\"\n</code></pre></p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#schema","title":"Schema","text":"<p>We can point the schema of a data source to our Data Contract CLI file.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"my_csv\", \"/opt/app/data/account-datacontract-cli\", Map.of(\"header\", \"true\"))\n        .fields(metadataSource().dataContractCli(\"/opt/app/mount/datacontract-cli/datacontract.yaml\"))\n        .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account-datacontract-cli\", Map(\"header\" -&gt; \"true\"))\n  .fields(metadataSource.dataContractCli(\"/opt/app/mount/datacontract-cli/datacontract.yaml\"))\n  .count(count.records(100))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-datacontract-cli-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/data/csv/account-datacontract-cli\"\n      metadataSourceType: \"dataContractCli\"\n      dataContractFile: \"/opt/app/mount/datacontract-cli/datacontract.yaml\"\n    count:\n      records: 100\n</code></pre></p> <ol> <li>Click on <code>Connection</code> tab at the top</li> <li>Select <code>Data Contract CLI</code> as the data source and enter <code>example-datacontract-cli</code></li> <li>Copy this file into <code>/tmp/datacontract-cli/datacontract.yaml</code></li> <li>Enter <code>/tmp/datacontract-cli/datacontract.yaml</code> as the <code>Contract File</code></li> </ol> <p>The above defines that the schema will come from Data Contract CLI, which is a type of metadata source that contains information about schemas. Specifically, it points to the schema provided here in the <code>docker/mount/datacontract-cli</code> folder of data-caterer-example repo.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedDataContractCliJavaPlanRun\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <pre><code>./run.sh MyAdvancedDataContractCliPlanRun\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <pre><code>./run.sh my-datacontract-cli.yaml\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>head /tmp/data-caterer/customer/account-datacontract-cli/part-00000*\n</code></pre></li> </ol> <p>It should look something like this.</p> <pre><code>province_state,latitude,confirmed,fips,longitude,country_region,last_update,combined_key,admin2\nfwFaFV F73BAIfFd,69977.84296117249,17533,ln9 CRbGkQ9IEyuW,793.3222856184141,87YVVqgS1podHa S,2024-02-10T10:25:39.176Z,sAnv74T9xOyA6MZI,06iRhvBBy40WBlVf\nW9N6z1 s7CYyc4L3,54580.231575916325,96761,4mxWLbwArVKOhg6E,58977.422371028944,TkCABcFIYJf87okg,2024-09-07T17:45:27.641Z,9GDm6MGk3WfPdorc,TQdRvrCSgCXg ioP\ndp2E6zXwoSKJ5 J2,13368.961196453121,18606,wGJ3iQNg5SdaN4ad,22482.40836235147,r4 Ka6J9ZNKQVEHK,2024-01-25T14:01:09.224Z,RYh6Kl5 46QvOZFR,eEad607OtQX15Vlw\nsfQG0neaO5hS7PlV,17461.556283773938,40155,DeSwWCpYwa4WFx5F,81371.85361585379,F2 tzIJS9JsTlhuE,2024-06-13T08:44:55.555Z,JnnGplRjkjo6SgOX,8B5h7UuV2r965wD4\nrAISjVikM0ScAsRX,65831.49716656232,36392,vKhuncOokeDgia7e,67677.50911541228,zZVJkymK09ef5oFC,2024-01-01T14:32:02.881Z,lLdHa4JExfuN2FXv,ebcPhXgYJMYTAla1\n</code></pre> <p>Looks like we have some data now. But we can do better and add some enhancements to it.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#custom-metadata","title":"Custom metadata","text":"<p>We can see from the data generated, that it isn't quite what we want. Sometimes, the metadata is not sufficient for us to produce production-like data yet, and we want to manually edit it. Let's try to add some enhancements to it.</p> <p>Let's make the <code>latitude</code> and <code>longitude</code> fields make sense. <code>latitude</code> is meant to be between -90 to 90 whilst  <code>longitude</code> is between -180 to 180. <code>country_region</code> should also represent a state name.  For the full guide on data generation options, check the following page.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"my_csv\", \"/opt/app/data/account-datacontract-cli\", Map.of(\"header\", \"true\"))\n            .fields(metadata...)\n            .fields(\n                field().name(\"latitude\").min(-90).max(90),\n                field().name(\"longitude\").min(-180).max(180),\n                field().name(\"country_region\").expression(\"#{Address.state}\")\n            )\n            .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account-datacontract-cli\", Map(\"header\" -&gt; \"true\"))\n  .fields(metadata...)\n  .fields(\n    field.name(\"latitude\").min(-90).max(90),\n    field.name(\"longitude\").min(-180).max(180),\n    field.name(\"country_region\").expression(\"#{Address.state}\")\n  )\n  .count(count.records(100))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-odcs-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/data/csv/account-datacontract-cli\"\n      metadataSourceType: \"dataContractCli\"\n      dataContractFile: \"/opt/app/mount/datacontract-cli/datacontract.yaml\"\n    count:\n      records: 100\n    fields:\n      - name: \"latitude\"\n        options:\n          min: -90\n          max: 90\n      - name: \"longitude\"\n        options:\n          min: -180\n          max: 180\n      - name: \"country_region\"\n        options:\n          expression: \"#{Address.state}\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Go to <code>latitude</code> field</li> <li>Select data type as <code>double</code></li> <li>Click on <code>+</code> dropdown next to <code>double</code> data type</li> <li>Click <code>Min</code> and enter <code>-90</code></li> <li>Click <code>Max</code> and enter <code>90</code></li> </ol> </li> <li>Click on <code>+ Field</code><ol> <li>Go to <code>longitude</code> field</li> <li>Select data type as <code>double</code></li> <li>Click on <code>+</code> dropdown next to <code>double</code> data type</li> <li>Click <code>Min</code> and enter <code>-180</code></li> <li>Click <code>Max</code> and enter <code>180</code></li> </ol> </li> <li>Click on <code>+ Field</code><ol> <li>Go to <code>country_region</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>Faker Expression</code> and enter <code>#{Address.state}</code></li> </ol> </li> </ol> <p>Let's test it out by running it again</p> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedDataContractCliJavaPlanRun\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <pre><code>./run.sh MyAdvancedDataContractCliPlanRun\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <pre><code>./run.sh my-datacontract-cli.yaml\nhead docker/sample/customer/account-datacontract-cli/part-00000-*\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>head /tmp/data-caterer/customer/account-datacontract-cli/part-00000*\n</code></pre></li> </ol> <pre><code>province_state,latitude,confirmed,fips,longitude,country_region,last_update,combined_key,admin2\nHY5GstfIPnXT0em,35.73941132584518,63652,6YS4JJvZ8N9JsqT,27.037747952451554,Connecticut,2023-12-24T12:42:08.798Z,qIPco7WUo5jXA D,ODADv25VyKsf6Qn\nvnkQrkwgf9oj xR,81.87829759208316,73064,cPgrOuPwBVnxK2b,-146.20644012308924,Illinois,2024-03-14T10:24:52.327Z,7NYzdyaM87VjlfH,KUpbi4msmXWZYS4\njnSwW Pk6zj1LsC,82.87970774482852,72341,rL5XqKZtM5unS9x,-153.1279291007243,Mississippi,2024-08-29T15:30:56.338Z,NouXv6EXlWY1Ihe,mirpEgTno0OEDH8\nZmNNb9C5g t8CgJ,43.58312642271184,73116,NFlRmB8p0egkFqG,179.56650534615852,Indiana,2024-01-22T17:05:51.968Z,Fkxf0l3CC a42o5,JznmesYH8ReGhg3\nUf5QH6luS4u5SnO,-75.64320251178277,6232,yRQLBU2OQvm5uqC,-31.025626492871083,New Jersey,2024-09-25T02:35:03.477Z,7IXVfeL6BEpkRbf,f7wUqnigV8WU4B\n</code></pre> <p>Great! Now we have the ability to get schema information from an external source, add our own metadata and generate data.</p>"},{"location":"docs/guide/data-source/metadata/data-contract-cli/#data-validation","title":"Data validation","text":"<p>To find out what data validation options are available, check this link.</p> <p>Another aspect of Data Contract CLI that can be leveraged is the definition of data quality rules. In a later version of Data Caterer, the data quality rules could be later imported and all run within Data Caterer.  Once available, it will be as easy as enabling data validations via <code>enableGenerateValidations</code> in <code>configuration</code>.</p> JavaScalaYAMLUI <pre><code>var conf = configuration().enableGeneratePlanAndTasks(true)\n    .enableGenerateValidations(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\");\n\nexecute(conf, accountTask);\n</code></pre> <pre><code>val conf = configuration.enableGeneratePlanAndTasks(true)\n  .enableGenerateValidations(true)\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(conf, accountTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGenerateValidations = true\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Generate Validations</code></li> </ol> <p>Check out the full example under <code>DataContractCliSourcePlanRun</code> in the example repo.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/","title":"Great Expectations Source","text":"<p>Creating a data generator for a JSON file and validating the data based on expectations  in Great Expectations.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/great-expectations/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/#great-expectations-setup","title":"Great Expectations Setup","text":"<p>A sample expectations file that will be used for this guide can be found  here.</p> <p>If you want to use your own expectations file, simply add it into the <code>docker/mount/ge</code> folder path and follow the below steps.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedGreatExpectationsJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedGreatExpectationsPlanRun.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-great-expectations.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedGreatExpectationsJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGenerateValidations(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedGreatExpectationsPlanRun extends PlanRun {\n  val conf = configuration.enableGenerateValidations(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-great-expectations.yaml</code>: <pre><code>name: \"my_great_expectations_plan\"\ndescription: \"Create account data in JSON format and validate via Great Expectations metadata\"\ntasks:\n  - name: \"json_task\"\n    dataSourceName: \"my_json\"\n</code></pre></p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGenerateValidations = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Generate Validations</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol> <p>We will enable generate validations so that we can read from external sources for validations and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/#great-expectations","title":"Great Expectations","text":"<p>To point to a specific expectations file, we create a metadata source as seen below.</p> JavaScalaYAMLUI <pre><code>var greatExpectationsSource = metadataSource().greatExpectations(\"/opt/app/mount/ge/taxi-expectations.json\");\n</code></pre> <pre><code>val greatExpectationsSource = metadataSource.greatExpectations(\"/opt/app/mount/ge/taxi-expectations.json\")\n</code></pre> <p>In <code>docker/data/custom/task/file/json/json-great-expectations-task.yaml</code>: <pre><code>name: \"json_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"json\"\n    options:\n      path: \"/opt/app/data/json\"\n      metadataSourceType: \"greatExpectations\"\n      expectationsFile: \"/opt/app/mount/ge/taxi-expectations.json\"\n</code></pre></p> <ol> <li>Click on <code>Connection</code> tab at the top</li> <li>Select <code>Great Expectations</code> as the data source and enter <code>my-great-expectations</code></li> <li>Copy this file into <code>/tmp/ge/taxi-expectations.json</code></li> <li>Enter <code>/tmp/ge/taxi-expectations.json</code> as the <code>Expectations File</code></li> </ol>"},{"location":"docs/guide/data-source/metadata/great-expectations/#schema-validation","title":"Schema &amp; Validation","text":"<p>To simulate a scenario where we have an existing data source, we will manually create a sample dataset.</p> <p>At the end, we point to our expectations metadata source to use those validations to validate the data.</p> JavaScalaYAMLUI <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map.of(\"saveMode\", \"overwrite\"))\n        .fields(\n                field().name(\"vendor_id\"),\n                field().name(\"pickup_datetime\").type(TimestampType.instance()),\n                field().name(\"dropoff_datetime\").type(TimestampType.instance()),\n                field().name(\"passenger_count\").type(IntegerType.instance()),\n                field().name(\"trip_distance\").type(DoubleType.instance()),\n                field().name(\"rate_code_id\"),\n                field().name(\"store_and_fwd_flag\"),\n                field().name(\"pickup_location_id\"),\n                field().name(\"dropoff_location_id\"),\n                field().name(\"payment_type\"),\n                field().name(\"fare_amount\").type(DoubleType.instance()),\n                field().name(\"extra\"),\n                field().name(\"mta_tax\").type(DoubleType.instance()),\n                field().name(\"tip_amount\").type(DoubleType.instance()),\n                field().name(\"tolls_amount\").type(DoubleType.instance()),\n                field().name(\"improvement_surcharge\").type(DoubleType.instance()),\n                field().name(\"total_amount\").type(DoubleType.instance()),\n                field().name(\"congestion_surcharge\").type(DoubleType.instance())\n        )\n        .validations(greatExpectations);\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/taxi_json\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(\n    field.name(\"vendor_id\"),\n    field.name(\"pickup_datetime\").`type`(TimestampType),\n    field.name(\"dropoff_datetime\").`type`(TimestampType),\n    field.name(\"passenger_count\").`type`(IntegerType),\n    field.name(\"trip_distance\").`type`(DoubleType),\n    field.name(\"rate_code_id\"),\n    field.name(\"store_and_fwd_flag\"),\n    field.name(\"pickup_location_id\"),\n    field.name(\"dropoff_location_id\"),\n    field.name(\"payment_type\"),\n    field.name(\"fare_amount\").`type`(DoubleType),\n    field.name(\"extra\"),\n    field.name(\"mta_tax\").`type`(DoubleType),\n    field.name(\"tip_amount\").`type`(DoubleType),\n    field.name(\"tolls_amount\").`type`(DoubleType),\n    field.name(\"improvement_surcharge\").`type`(DoubleType),\n    field.name(\"total_amount\").`type`(DoubleType),\n    field.name(\"congestion_surcharge\").`type`(DoubleType),\n  )\n  .validations(greatExpectationsSource)\n</code></pre> <p>In <code>docker/data/custom/task/json/json-great-expectations-task.yaml</code>: <pre><code>name: \"json_task\"\nsteps:\n  - name: \"accounts\"\n    type: \"json\"\n    options:\n      path: \"/opt/app/data/json\"\n      metadataSourceType: \"greatExpectations\"\n      expectationsFile: \"/opt/app/mount/ge/taxi-expectations.json\"\n    fields:\n      - name: \"vendor_id\"\n      - name: \"pickup_datetime\"\n        type: \"timestamp\"\n      - name: \"dropoff_datetime\"\n        type: \"timestamp\"\n      - name: \"passenger_count\"\n        type: \"integer\"\n      - name: \"trip_distance\"\n        type: \"double\"\n      - name: \"rate_code_id\"\n      - name: \"store_and_fwd_flag\"\n      - name: \"pickup_location_id\"\n      - name: \"dropoff_location_id\"\n      - name: \"payment_type\"\n      - name: \"fare_amount\"\n        type: \"double\"\n      - name: \"extra\"\n      - name: \"mta_tax\"\n        type: \"double\"\n      - name: \"tip_amount\"\n        type: \"double\"\n      - name: \"tolls_amount\"\n        type: \"double\"\n      - name: \"improvement_surcharge\"\n        type: \"double\"\n      - name: \"total_amount\"\n        type: \"double\"\n      - name: \"congestion_surcharge\"\n        type: \"double\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code></li> <li>Add name as <code>vendor_id</code></li> <li>Click on <code>Select data type</code> and select <code>string</code></li> <li>Continue with other fields and data types</li> </ol>"},{"location":"docs/guide/data-source/metadata/great-expectations/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedGreatExpectationsJavaPlanRun or MyAdvancedGreatExpectationsPlanRun\n#after completing\n#open docker/sample/report/index.html\n</code></pre> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedGreatExpectationsJavaPlanRun\n#open docker/sample/report/index.html\n</code></pre> <pre><code>./run.sh MyAdvancedGreatExpectationsPlanRun\n#open docker/sample/report/index.html\n</code></pre> <pre><code>./run.sh my-great-expectations.yaml\n#open docker/sample/report/index.html\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>It should look something like this.</p> <p></p> <p>So we were just able to validate our data source from reading a Great Expectations file. Simple! This gives us an easy way to integrate with existing data validations, but now we can relate that to generated data in test environments.</p> <p>But we still may want to add on our own validations outside what is in Great Expectations.</p>"},{"location":"docs/guide/data-source/metadata/great-expectations/#custom-validation","title":"Custom validation","text":"<p>We found that we should also check that the <code>trip_distance</code> has to be less than <code>500</code> but was not included in Great Expectations. No worries, we can simply add it in here alongside the existing expectations.</p> JavaScalaYAMLUI <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map.of(\"saveMode\", \"overwrite\"))\n        .fields(\n            ...\n        ))\n        .validations(greatExpectations)\n        .validations(validation().field(\"trip_distance\").lessThan(500));\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(\n    ...\n  ))\n  .validations(greatExpectationsSource)\n  .validations(validation.field(\"trip_distance\").lessThan(500))\n</code></pre> <p>In <code>docker/data/custom/validation/great-expectations-validation.yaml</code>: <pre><code>---\nname: \"ge_checks\"\ndataSources:\n  my_json:\n    - validations:\n        - expr: \"trip_distance &lt; 500\"\n        - field: \"trip_distance\"  #OR\n          validation:\n            - type: \"lessThan\"\n              value: 500\n</code></pre></p> <ol> <li>Under <code>Validation</code>, click on <code>Manual</code></li> <li>Click on <code>+ Validation</code> and go to <code>Select validation type...</code> as <code>Field</code></li> <li>Set <code>Field</code> to <code>trip_distance</code></li> <li>Click on <code>+</code> next to the field name and select <code>Less Than</code></li> <li>Enter <code>500</code> as the value</li> </ol> <p>Let's test it out by running it again.</p> <p></p> <p>Check out the full example under <code>GreatExpectationsPlanRun</code> in the example repo.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/","title":"JSON Schema Source","text":"<p>Creating a data generator for JSON files based on metadata stored in JSON Schema format. JSON Schema provides a powerful way to describe and validate the structure of JSON data, making it an excellent metadata source for generating realistic test data.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#json-schema-setup","title":"JSON Schema Setup","text":"<p>We will be using a JSON Schema file that defines the structure for a financial payment system. You can use your own JSON Schema file by placing it in the appropriate mount folder and following the steps below.</p> <p>Example JSON Schema structure:</p> <pre><code>{\n  \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n  \"type\": \"object\",\n  \"properties\": {\n    \"customer_direct_debit_initiation_v11\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"group_header\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"message_identification\": { \"type\": \"string\" },\n            \"creation_date_time\": { \"type\": \"string\", \"format\": \"date-time\" },\n            \"number_of_transactions\": { \"type\": \"integer\" },\n            \"control_sum\": { \"type\": \"number\" },\n            \"initiating_party\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"name\": { \"type\": \"string\" }\n              }\n            }\n          }\n        },\n        \"payment_information\": {\n          \"type\": \"object\",\n          \"properties\": {\n            \"payment_information_identification\": { \"type\": \"string\" },\n            \"payment_method\": { \"type\": \"string\" },\n            \"batch_booking\": { \"type\": \"boolean\" },\n            \"direct_debit_transaction_information\": {\n              \"type\": \"object\",\n              \"properties\": {\n                \"payment_identification\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"end_to_end_identification\": { \"type\": \"string\" }\n                  }\n                },\n                \"instructed_amount\": {\n                  \"type\": \"object\",\n                  \"properties\": {\n                    \"value\": { \"type\": \"number\" },\n                    \"currency\": { \"type\": \"string\" }\n                  }\n                }\n              }\n            }\n          }\n        }\n      }\n    }\n  }\n}\n</code></pre>"},{"location":"docs/guide/data-source/metadata/json-schema/#plan-setup","title":"Plan Setup","text":"<p>Create a file depending on which interface you want to use.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyJSONSchemaJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyJSONSchemaPlan.scala</code></li> <li>YAML: <code>docker/data/custom/plan/my-json-schema.yaml</code></li> </ul> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyJSONSchemaJavaPlan extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyJSONSchemaPlan extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-json-schema.yaml</code>: <pre><code>name: \"my_json_schema_plan\"\ndescription: \"Create JSON data via JSON Schema metadata\"\ntasks:\n  - name: \"json_schema_task\"\n    dataSourceName: \"my_json_schema\"\n</code></pre></p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGeneratePlanAndTasks = true\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Generate Plan And Tasks</code></li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#connection-configuration","title":"Connection Configuration","text":"<p>Within our class, we can start by defining the connection properties to read/write from/to JSON and specify the JSON Schema metadata source.</p> JavaScalaYAMLUI <pre><code>var jsonSchemaTask = json(\n    \"my_json_schema\",                           //name\n    \"/opt/app/data/json-schema-output\",         //path\n    Map.of(\"saveMode\", \"overwrite\")             //additional options\n);\n</code></pre> <pre><code>val jsonSchemaTask = json(\n  \"my_json_schema\",                           //name\n  \"/opt/app/data/json-schema-output\",         //path\n  Map(\"saveMode\" -&gt; \"overwrite\")              //additional options\n)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>json {\n    my_json_schema {\n        \"saveMode\": \"overwrite\"\n    }\n}\n</code></pre></p> <ol> <li>Click on <code>Connection</code> towards the top of the screen</li> <li>For connection name, set to <code>my_json_schema</code></li> <li>Click on <code>Select data source type..</code> and select <code>JSON</code></li> <li>Set <code>Path</code> as <code>/tmp/custom/json-schema/output</code></li> <li>Click on <code>Create</code></li> </ol>"},{"location":"docs/guide/data-source/metadata/json-schema/#schema","title":"Schema","text":"<p>We can point the schema of a data source to our JSON Schema file. The metadata source will automatically parse the JSON Schema and generate appropriate field definitions.</p> JavaScalaYAMLUI <pre><code>var jsonSchemaTask = json(\"my_json_schema\", \"/opt/app/data/json-schema-output\", Map.of(\"saveMode\", \"overwrite\"))\n        .fields(metadataSource().jsonSchema(\"/opt/app/mount/json-schema/payment-schema.json\"))\n        .count(count().records(10));\n</code></pre> <pre><code>val jsonSchemaTask = json(\"my_json_schema\", \"/opt/app/data/json-schema-output\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(metadataSource.jsonSchema(\"/opt/app/mount/json-schema/payment-schema.json\"))\n  .count(count.records(10))\n</code></pre> <p>In <code>docker/data/custom/task/file/json/json-schema-task.yaml</code>: <pre><code>name: \"json_schema_task\"\nsteps:\n  - name: \"json_data\"\n    type: \"json\"\n    options:\n      path: \"/opt/app/data/json-schema-output\"\n      saveMode: \"overwrite\"\n      metadataSourceType: \"jsonSchema\"\n      jsonSchemaFile: \"/opt/app/mount/json-schema/payment-schema.json\"\n    count:\n      records: 10\n</code></pre></p> <ol> <li>Click on <code>Connection</code> tab at the top</li> <li>Select <code>JSON Schema</code> as the data source and enter <code>my-json-schema-metadata</code></li> <li>Create your JSON Schema file at <code>/tmp/json-schema/payment-schema.json</code></li> <li>Enter <code>/tmp/json-schema/payment-schema.json</code> as the <code>Schema File</code></li> <li>Click on <code>Generation</code> and select the metadata source connection</li> </ol>"},{"location":"docs/guide/data-source/metadata/json-schema/#field-filtering-options","title":"Field Filtering Options","text":"<p>JSON Schema metadata source supports powerful field filtering capabilities to control which fields are included or excluded from data generation:</p> JavaScalaYAMLUI <pre><code>var jsonSchemaTask = json(\"my_json_schema\", \"/opt/app/data/json-schema-output\", Map.of(\"saveMode\", \"overwrite\"))\n        .fields(metadataSource().jsonSchema(\"/opt/app/mount/json-schema/payment-schema.json\"))\n        // Include specific fields only\n        .includeFields(List.of(\n            \"customer_direct_debit_initiation_v11.group_header.message_identification\",\n            \"customer_direct_debit_initiation_v11.group_header.creation_date_time\",\n            \"customer_direct_debit_initiation_v11.payment_information.payment_information_identification\",\n            \"customer_direct_debit_initiation_v11.payment_information.direct_debit_transaction_information.instructed_amount.value\"\n        ))\n        // Or exclude specific fields\n        // .excludeFields(List.of(\n        //     \"customer_direct_debit_initiation_v11.group_header.control_sum\",\n        //     \"customer_direct_debit_initiation_v11.payment_information.batch_booking\"\n        // ))\n        // Or include fields matching patterns\n        // .includeFieldPatterns(List.of(\".*amount.*\", \".*identification.*\"))\n        // Or exclude fields matching patterns\n        // .excludeFieldPatterns(List.of(\".*internal.*\", \".*debug.*\"))\n        .count(count().records(10));\n</code></pre> <pre><code>val jsonSchemaTask = json(\"my_json_schema\", \"/opt/app/data/json-schema-output\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(metadataSource.jsonSchema(\"/opt/app/mount/json-schema/payment-schema.json\"))\n  // Include specific fields only\n  .includeFields(\n    \"customer_direct_debit_initiation_v11.group_header.message_identification\",\n    \"customer_direct_debit_initiation_v11.group_header.creation_date_time\",\n    \"customer_direct_debit_initiation_v11.payment_information.payment_information_identification\",\n    \"customer_direct_debit_initiation_v11.payment_information.direct_debit_transaction_information.instructed_amount.value\"\n  )\n  // Or exclude specific fields\n  // .excludeFields(\n  //   \"customer_direct_debit_initiation_v11.group_header.control_sum\",\n  //   \"customer_direct_debit_initiation_v11.payment_information.batch_booking\"\n  // )\n  // Or include fields matching patterns\n  // .includeFieldPatterns(\".*amount.*\", \".*identification.*\")\n  // Or exclude fields matching patterns\n  // .excludeFieldPatterns(\".*internal.*\", \".*debug.*\")\n  .count(count.records(10))\n</code></pre> <p>In <code>docker/data/custom/task/file/json/json-schema-task.yaml</code>: <pre><code>name: \"json_schema_task\"\nsteps:\n  - name: \"json_data\"\n    type: \"json\"\n    options:\n      path: \"/opt/app/data/json-schema-output\"\n      saveMode: \"overwrite\"\n      metadataSourceType: \"jsonSchema\"\n      jsonSchemaFile: \"/opt/app/mount/json-schema/payment-schema.json\"\n      # Include specific fields only\n      includeFields:\n        - \"customer_direct_debit_initiation_v11.group_header.message_identification\"\n        - \"customer_direct_debit_initiation_v11.group_header.creation_date_time\"\n        - \"customer_direct_debit_initiation_v11.payment_information.payment_information_identification\"\n      # Or exclude specific fields\n      # excludeFields:\n      #   - \"customer_direct_debit_initiation_v11.group_header.control_sum\"\n      #   - \"customer_direct_debit_initiation_v11.payment_information.batch_booking\"\n      # Or include fields matching patterns\n      # includeFieldPatterns:\n      #   - \".*amount.*\"\n      #   - \".*identification.*\"\n      # Or exclude fields matching patterns\n      # excludeFieldPatterns:\n      #   - \".*internal.*\"\n      #   - \".*debug.*\"\n    count:\n      records: 10\n</code></pre></p> <ol> <li>In the connection configuration, expand <code>Advanced Options</code></li> <li>Add <code>Include Fields</code> and enter comma-separated field paths</li> <li>Or add <code>Exclude Fields</code> for fields to exclude</li> <li>Or use <code>Include Field Patterns</code> with regex patterns</li> <li>Or use <code>Exclude Field Patterns</code> with regex patterns</li> </ol> <p>The field filtering options provide flexibility to:</p> <ul> <li>includeFields: Only generate data for the specified field paths</li> <li>excludeFields: Generate data for all fields except the specified ones</li> <li>includeFieldPatterns: Include fields matching the regex patterns</li> <li>excludeFieldPatterns: Exclude fields matching the regex patterns</li> </ul> <p>Field paths use dot notation to navigate nested structures (e.g., <code>parent.child.grandchild</code>).</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableGeneratePlanAndTasks(true)\n        .enableUniqueCheck(true);\n\nexecute(config, jsonSchemaTask);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableGeneratePlanAndTasks(true)\n  .enableUniqueCheck(true)\n\nexecute(config, jsonSchemaTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGeneratePlanAndTasks = true\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Generate Plan And Tasks</code></li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/data-source/metadata/json-schema/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyJSONSchemaJavaPlan\nhead docker/sample/json-schema-output/part-00000-*\n</code></pre> <pre><code>./run.sh MyJSONSchemaPlan\nhead docker/sample/json-schema-output/part-00000-*\n</code></pre> <pre><code>./run.sh my-json-schema.yaml\nhead docker/sample/json-schema-output/part-00000-*\n</code></pre> <ol> <li>Click the button <code>Execute</code> at the top</li> <li>Progress updates will show in the bottom right corner</li> <li>Click on <code>History</code> at the top</li> <li>Check for your plan name and see the result summary</li> <li>Click on <code>Report</code> on the right side to see more details of what was executed</li> </ol> <p>It should look something like this.</p> <pre><code>{\n  \"customer_direct_debit_initiation_v11\": {\n    \"group_header\": {\n      \"message_identification\": \"MSG001\",\n      \"creation_date_time\": \"2024-03-15T10:30:45Z\",\n      \"number_of_transactions\": 1,\n      \"control_sum\": 100.5,\n      \"initiating_party\": {\n        \"name\": \"ACME Corp\"\n      }\n    },\n    \"payment_information\": {\n      \"payment_information_identification\": \"PMT001\",\n      \"payment_method\": \"DD\",\n      \"batch_booking\": true,\n      \"direct_debit_transaction_information\": {\n        \"payment_identification\": {\n          \"end_to_end_identification\": \"TXN001\"\n        },\n        \"instructed_amount\": {\n          \"value\": 100.5,\n          \"currency\": \"EUR\"\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Congratulations! You have now made a data generator that uses JSON Schema as a metadata source to generate realistic test data following your schema specifications.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#json-schema-support","title":"JSON Schema Support","text":"<p>This section provides comprehensive documentation about Data Caterer's JSON Schema metadata support, including which features are supported, how they map to data generation, and current limitations.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#supported-json-schema-versions","title":"Supported JSON Schema Versions","text":"<p>Data Caterer supports the following JSON Schema versions:</p> <ul> <li>Draft 4 (<code>http://json-schema.org/draft-04/schema#</code>)</li> <li>Draft 6 (<code>http://json-schema.org/draft-06/schema#</code>)</li> <li>Draft 7 (<code>http://json-schema.org/draft-07/schema#</code>)</li> <li>Draft 2019-09 (<code>https://json-schema.org/draft/2019-09/schema</code>)</li> <li>Draft 2020-12 (<code>https://json-schema.org/draft/2020-12/schema</code>) - Default</li> </ul> <p>If no <code>$schema</code> is specified, Data Caterer defaults to Draft 2020-12.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#data-type-mapping","title":"Data Type Mapping","text":"<p>JSON Schema types are mapped to Data Caterer data types as follows:</p> JSON Schema Type Data Caterer Type Generated Data Examples <code>string</code> <code>StringType</code> Random strings, format-specific values <code>integer</code> <code>IntegerType</code> Random integers within constraints <code>number</code> <code>DoubleType</code> Random decimal numbers <code>boolean</code> <code>BooleanType</code> <code>true</code> or <code>false</code> <code>array</code> <code>ArrayType</code> Arrays of the specified item type <code>object</code> <code>StructType</code> Nested objects with defined properties <code>null</code> <code>StringType</code> (nullable) Treated as nullable string"},{"location":"docs/guide/data-source/metadata/json-schema/#core-schema-features","title":"Core Schema Features","text":""},{"location":"docs/guide/data-source/metadata/json-schema/#basic-properties","title":"Basic Properties","text":"<pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"name\": { \"type\": \"string\" },\n    \"age\": { \"type\": \"integer\" },\n    \"active\": { \"type\": \"boolean\" }\n  },\n  \"required\": [\"name\"]\n}\n</code></pre> <ul> <li>Supported: All basic types, nested objects, required fields</li> <li>Data Generation: Required fields are non-nullable, optional fields are nullable</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#arrays","title":"Arrays","text":"<pre><code>{\n  \"type\": \"array\",\n  \"items\": {\n    \"type\": \"string\"\n  },\n  \"minItems\": 1,\n  \"maxItems\": 10,\n  \"uniqueItems\": true\n}\n</code></pre> <ul> <li>Supported: Arrays of primitives and objects, nested arrays</li> <li>Array Constraints: <code>minItems</code>, <code>maxItems</code>, <code>uniqueItems</code></li> <li>Data Generation: Generates arrays within specified size bounds</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#nested-objects","title":"Nested Objects","text":"<pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"address\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"street\": { \"type\": \"string\" },\n        \"city\": { \"type\": \"string\" }\n      },\n      \"required\": [\"street\"]\n    }\n  }\n}\n</code></pre> <ul> <li>Supported: Multi-level nesting, required fields at any level</li> <li>Data Generation: Preserves nested structure and constraints</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#validation-constraints","title":"Validation Constraints","text":""},{"location":"docs/guide/data-source/metadata/json-schema/#string-constraints","title":"String Constraints","text":"Constraint Support Data Generation Effect <code>pattern</code> \u2705 Full Generates strings matching regex pattern <code>minLength</code> \u2705 Full Minimum string length <code>maxLength</code> \u2705 Full Maximum string length <code>enum</code> \u2705 Full Selects from predefined values <code>const</code> \u2705 Full Always generates the constant value <code>format</code> \u2705 Partial Format-specific generators (see below) <p>Format Support:</p> <ul> <li><code>email</code> - Generates realistic email addresses</li> <li><code>uri</code>/<code>url</code> - Generates valid URLs</li> <li><code>uuid</code> - Generates UUID strings</li> <li><code>date</code> - Generates date strings (YYYY-MM-DD)</li> <li><code>date-time</code> - Generates timestamp strings</li> <li><code>time</code> - Generates time strings (HH:MM:SS)</li> <li><code>ipv4</code> - Generates IPv4 addresses</li> <li><code>ipv6</code> - Generates IPv6 addresses</li> <li><code>hostname</code> - Generates hostname strings</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#numeric-constraints","title":"Numeric Constraints","text":"Constraint Support Data Generation Effect <code>minimum</code> \u2705 Full Minimum value (inclusive) <code>maximum</code> \u2705 Full Maximum value (inclusive) <code>exclusiveMinimum</code> \u274c Not supported Treated as minimum <code>exclusiveMaximum</code> \u274c Not supported Treated as maximum <code>multipleOf</code> \u274c Not supported Ignored"},{"location":"docs/guide/data-source/metadata/json-schema/#array-constraints","title":"Array Constraints","text":"Constraint Support Data Generation Effect <code>minItems</code> \u2705 Full Minimum array length <code>maxItems</code> \u2705 Full Maximum array length <code>uniqueItems</code> \u2705 Full Ensures array elements are unique <code>contains</code> \u274c Not supported Ignored <code>minContains</code> \u274c Not supported Ignored <code>maxContains</code> \u274c Not supported Ignored"},{"location":"docs/guide/data-source/metadata/json-schema/#object-constraints","title":"Object Constraints","text":"Constraint Support Data Generation Effect <code>required</code> \u2705 Full Makes fields non-nullable <code>minProperties</code> \u274c Not supported Ignored <code>maxProperties</code> \u274c Not supported Ignored <code>additionalProperties</code> \u26a0\ufe0f Recognized Does not generate additional properties"},{"location":"docs/guide/data-source/metadata/json-schema/#advanced-features","title":"Advanced Features","text":""},{"location":"docs/guide/data-source/metadata/json-schema/#references-ref","title":"References ($ref)","text":"<p>Data Caterer supports JSON Schema references for reusable schema components:</p> <pre><code>{\n  \"type\": \"object\",\n  \"properties\": {\n    \"user\": { \"$ref\": \"#/definitions/User\" }\n  },\n  \"definitions\": {\n    \"User\": {\n      \"type\": \"object\",\n      \"properties\": {\n        \"id\": { \"type\": \"integer\" },\n        \"name\": { \"type\": \"string\" }\n      },\n      \"required\": [\"id\"]\n    }\n  }\n}\n</code></pre> <p>Reference Support:</p> <ul> <li>\u2705 <code>#/definitions/DefinitionName</code> - Internal definitions</li> <li>\u274c External references (different files/URLs)</li> <li>\u274c JSON Pointer references other than <code>#/definitions/</code></li> </ul> <p>Data Generation: References are resolved and constraints from referenced schemas are preserved.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#schema-composition","title":"Schema Composition","text":""},{"location":"docs/guide/data-source/metadata/json-schema/#allof-full-support","title":"allOf (Full Support)","text":"<pre><code>{\n  \"allOf\": [\n    { \"$ref\": \"#/definitions/BaseEntity\" },\n    { \"$ref\": \"#/definitions/TimestampFields\" },\n    {\n      \"properties\": {\n        \"description\": { \"type\": \"string\" }\n      }\n    }\n  ]\n}\n</code></pre> <p>Data Generation: Merges all schemas and generates fields from all combined properties.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#oneofanyof-limited-support","title":"oneOf/anyOf (Limited Support)","text":"<pre><code>{\n  \"oneOf\": [\n    { \"$ref\": \"#/definitions/CreditCardPayment\" },\n    { \"$ref\": \"#/definitions/BankTransferPayment\" }\n  ]\n}\n</code></pre> <p>Data Generation: Uses the first schema from the oneOf/anyOf array. Other schemas are ignored.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#not-not-supported","title":"not (Not Supported)","text":"<p><code>not</code> schemas are not supported and will be ignored.</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#field-filtering","title":"Field Filtering","text":"<p>Data Caterer provides powerful field filtering options for JSON Schema metadata sources:</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#includeexclude-specific-fields","title":"Include/Exclude Specific Fields","text":"<pre><code>// Include only specific fields\n.includeFields(\"profile.name\", \"profile.email\", \"addresses.street\")\n\n// Exclude specific fields\n.excludeFields(\"profile.createdDate\", \"sessionId\")\n</code></pre>"},{"location":"docs/guide/data-source/metadata/json-schema/#pattern-based-filtering","title":"Pattern-Based Filtering","text":"<pre><code>// Include fields matching patterns\n.includeFieldPatterns(\".*email.*\", \".*name.*\")\n\n// Exclude fields matching patterns\n.excludeFieldPatterns(\".*internal.*\", \".*debug.*\")\n</code></pre> <p>Field Path Format: Use dot notation for nested fields (e.g., <code>profile.address.street</code>).</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#constraints-preservation","title":"Constraints Preservation","text":"<p>Data Caterer preserves constraints through:</p> <ol> <li>Direct mapping: Simple constraints map directly to Data Caterer options</li> <li>Reference resolution: Constraints from referenced schemas are maintained</li> <li>Composition merging: allOf compositions merge constraints from all schemas</li> <li>Nested structures: Multi-level constraints are preserved at appropriate levels</li> </ol>"},{"location":"docs/guide/data-source/metadata/json-schema/#example-complex-constraint-preservation","title":"Example: Complex Constraint Preservation","text":"<pre><code>{\n  \"properties\": {\n    \"users\": {\n      \"type\": \"array\",\n      \"items\": {\n        \"$ref\": \"#/definitions/User\"\n      },\n      \"minItems\": 1,\n      \"maxItems\": 100\n    }\n  },\n  \"definitions\": {\n    \"User\": {\n      \"properties\": {\n        \"email\": {\n          \"type\": \"string\",\n          \"format\": \"email\"\n        },\n        \"age\": {\n          \"type\": \"integer\",\n          \"minimum\": 18,\n          \"maximum\": 65\n        }\n      }\n    }\n  }\n}\n</code></pre> <p>Generated Data:</p> <ul> <li>Array will have 1-100 items</li> <li>Each user will have a valid email format</li> <li>Ages will be between 18-65</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#unsupported-features","title":"Unsupported Features","text":"<p>The following JSON Schema features are not currently supported:</p>"},{"location":"docs/guide/data-source/metadata/json-schema/#schema-composition_1","title":"Schema Composition","text":"<ul> <li><code>not</code> schemas</li> <li>Complex <code>anyOf</code>/<code>oneOf</code> resolution (only first option used)</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#advanced-constraints","title":"Advanced Constraints","text":"<ul> <li><code>exclusiveMinimum</code>/<code>exclusiveMaximum</code></li> <li><code>multipleOf</code></li> <li><code>contains</code>, <code>minContains</code>, <code>maxContains</code></li> <li><code>minProperties</code>/<code>maxProperties</code></li> <li><code>unevaluatedProperties</code>/<code>unevaluatedItems</code></li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#advanced-features_1","title":"Advanced Features","text":"<ul> <li><code>if</code>/<code>then</code>/<code>else</code> conditional schemas</li> <li><code>dependentRequired</code>/<code>dependentSchemas</code></li> <li><code>prefixItems</code> (Draft 2020-12)</li> <li>External references</li> <li>Dynamic references</li> <li>Schema recursion detection</li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#format-extensions","title":"Format Extensions","text":"<ul> <li>Custom formats beyond the built-in list</li> <li><code>idn-email</code>, <code>idn-hostname</code></li> <li><code>iri</code>, <code>iri-reference</code></li> <li><code>duration</code></li> </ul>"},{"location":"docs/guide/data-source/metadata/json-schema/#validation","title":"Validation","text":"<p>If you want to validate data against a JSON Schema, you can use the generated data with validation frameworks or tools that support JSON Schema validation.</p> <p>The JSON Schema metadata source in Data Caterer focuses on data generation based on the schema structure, ensuring that the generated data conforms to the defined schema constraints and types.</p>"},{"location":"docs/guide/data-source/metadata/marquez/","title":"Metadata Source","text":"<p>Creating a data generator for Postgres tables and CSV file based on metadata stored in Marquez ( follows OpenLineage API).</p>"},{"location":"docs/guide/data-source/metadata/marquez/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/marquez/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#marquez-setup","title":"Marquez Setup","text":"<p>You can follow the README found here to help with setting up Marquez in your local environment. This comes with an instance of Postgres which we will also be using as a data store for generated data.</p> <p>The command that was run for this example to help with setup of dummy data was <code>./docker/up.sh -a 5001 -m 5002 --seed</code>.</p> <p>Check that the following url shows some data like below once you click on <code>food_delivery</code> from the <code>ns</code> drop down in the top right corner.</p> <p></p>"},{"location":"docs/guide/data-source/metadata/marquez/#postgres-setup","title":"Postgres Setup","text":"<p>Since we will also be using the Marquez Postgres instance as a data source, we will set up a separate database to store the generated data in via:</p> <pre><code>docker exec marquez-db psql -Upostgres -c 'CREATE DATABASE food_delivery'\n</code></pre>"},{"location":"docs/guide/data-source/metadata/marquez/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedMetadataSourceJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedMetadataSourcePlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedMetadataSourceJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedMetadataSourcePlanRun extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#schema","title":"Schema","text":"<p>We can point the schema of a data source to our Marquez instance. For the Postgres data source, we will point to a <code>namespace</code>, which in Marquez or OpenLineage, represents a set of datasets. For the CSV data source, we will point to a specific <code>namespace</code> and <code>dataset</code>.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#single-schema","title":"Single Schema","text":"JavaScala <pre><code>var csvTask = csv(\"my_csv\", \"/tmp/data/csv\", Map.of(\"saveMode\", \"overwrite\", \"header\", \"true\"))\n        .fields(metadataSource().marquez(\"http://localhost:5001\", \"food_delivery\", \"public.delivery_7_days\"))\n        .count(count().records(10));\n</code></pre> <pre><code>val csvTask = csv(\"my_csv\", \"/tmp/data/csv\", Map(\"saveMode\" -&gt; \"overwrite\", \"header\" -&gt; \"true\"))\n  .fields(metadataSource.marquez(\"http://localhost:5001\", \"food_delivery\", \"public.delivery_7_days\"))\n  .count(count.records(10))\n</code></pre> <p>The above defines that the schema will come from Marquez, which is a type of metadata source that contains information about schemas. Specifically, it points to the <code>food_delivery</code> namespace and <code>public.categories</code> dataset to retrieve the schema information from.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#multiple-schemas","title":"Multiple Schemas","text":"JavaScala <pre><code>var postgresTask = postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/food_delivery\", \"postgres\", \"password\", Map.of())\n    .fields(metadataSource().marquez(\"http://host.docker.internal:5001\", \"food_delivery\"))\n    .count(count().records(10));\n</code></pre> <pre><code>val postgresTask = postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/food_delivery\", \"postgres\", \"password\")\n  .fields(metadataSource.marquez(\"http://host.docker.internal:5001\", \"food_delivery\"))\n  .count(count.records(10))\n</code></pre> <p>We now have pointed this Postgres instance to produce multiple schemas that are defined under the <code>food_delivery</code> namespace. Also note that we are using database <code>food_delivery</code> in Postgres to push our generated data to, and we have set the number of records per sub data source (in this case, per table) to be 10.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedMetadataSourceJavaPlanRun or MyAdvancedMetadataSourcePlanRun\n#after completing\ndocker exec marquez-db psql -Upostgres -d food_delivery -c 'SELECT * FROM public.delivery_7_days'\n</code></pre> <p>It should look something like this.</p> <pre><code> order_id |     order_placed_on     |   order_dispatched_on   |   order_delivered_on    |         customer_email         |                     customer_address                     | menu_id | restaurant_id |                        restaurant_address\n   | menu_item_id | category_id | discount_id | city_id | driver_id\n----------+-------------------------+-------------------------+-------------------------+--------------------------------+----------------------------------------------------------+---------+---------------+---------------------------------------------------------------\n---+--------------+-------------+-------------+---------+-----------\n    38736 | 2023-02-05 06:05:23.755 | 2023-09-08 04:29:10.878 | 2023-09-03 23:58:34.285 | april.skiles@hotmail.com       | 5018 Lang Dam, Gaylordfurt, MO 35172                     |   59841 |         30971 | Suite 439 51366 Bartoletti Plains, West Lashawndamouth, CA 242\n42 |        55697 |       36370 |       21574 |   88022 |     16569\n     4376 | 2022-12-19 14:39:53.442 | 2023-08-30 07:40:06.948 | 2023-03-15 20:38:26.11  | adelina.balistreri@hotmail.com | Apt. 340 9146 Novella Motorway, East Troyhaven, UT 34773 |   66195 |         42765 | Suite 670 8956 Rob Fork, Rennershire, CA 04524\n   |        26516 |       81335 |       87615 |   27433 |     45649\n    11083 | 2022-10-30 12:46:38.692 | 2023-06-02 13:05:52.493 | 2022-11-27 18:38:07.873 | johnny.gleason@gmail.com       | Apt. 385 99701 Lemke Place, New Irvin, RI 73305          |   66427 |         44438 | 1309 Danny Cape, Weimanntown, AL 15865\n   |        41686 |       36508 |       34498 |   24191 |     92405\n    58759 | 2023-07-26 14:32:30.883 | 2022-12-25 11:04:08.561 | 2023-04-21 17:43:05.86  | isabelle.ohara@hotmail.com     | 2225 Evie Lane, South Ardella, SD 90805                  |   27106 |         25287 | Suite 678 3731 Dovie Park, Port Luigi, ID 08250\n   |        94205 |       66207 |       81051 |   52553 |     27483\n</code></pre> <p>You can also try query some other tables. Let's also check what is in the CSV file.</p> <pre><code>$ head docker/sample/csv/part-0000*\nmenu_item_id,category_id,discount_id,city_id,driver_id,order_id,order_placed_on,order_dispatched_on,order_delivered_on,customer_email,customer_address,menu_id,restaurant_id,restaurant_address\n72248,37098,80135,45888,5036,11090,2023-09-20T05:33:08.036+08:00,2023-05-16T23:10:57.119+08:00,2023-05-01T22:02:23.272+08:00,demetrice.rohan@hotmail.com,\"406 Harmony Rue, Wisozkburgh, MD 12282\",33762,9042,\"Apt. 751 0796 Ellan Flats, Lake Chetville, WI 81957\"\n41644,40029,48565,83373,89919,58359,2023-04-18T06:28:26.194+08:00,2022-10-15T18:17:48.998+08:00,2023-02-06T17:02:04.104+08:00,joannie.okuneva@yahoo.com,\"Suite 889 022 Susan Lane, Zemlakport, OR 56996\",27467,6216,\"Suite 016 286 Derick Grove, Dooleytown, NY 14664\"\n49299,53699,79675,40821,61764,72234,2023-07-16T21:33:48.739+08:00,2023-02-14T21:23:10.265+08:00,2023-09-18T02:08:51.433+08:00,ina.heller@yahoo.com,\"Suite 600 86844 Heller Island, New Celestinestad, DE 42622\",48002,12462,\"5418 Okuneva Mountain, East Blairchester, MN 04060\"\n83197,86141,11085,29944,81164,65382,2023-01-20T06:08:25.981+08:00,2023-01-11T13:24:32.968+08:00,2023-09-09T02:30:16.890+08:00,lakisha.bashirian@yahoo.com,\"Suite 938 534 Theodore Lock, Port Caitlynland, LA 67308\",69109,47727,\"4464 Stewart Tunnel, Marguritemouth, AR 56791\"\n</code></pre> <p>Looks like we have some data now. But we can do better and add some enhancements to it.</p> <p>What if we wanted the same records in Postgres <code>public.delivery_7_days</code> to also show up in the CSV file? That's where we can use a foreign key definition.</p>"},{"location":"docs/guide/data-source/metadata/marquez/#foreign-key","title":"Foreign Key","text":"<p>We can take a look at the report (under <code>docker/sample/report/index.html</code>) to see what we need to do to create the  foreign key. From the overview, you should see under <code>Tasks</code> there is a <code>my_postgres</code> task which has  <code>food_delivery_public.delivery_7_days</code> as a step. Click on the link for <code>food_delivery_public.delivery_7_days</code> and it  will take us to a page where we can find out about the fields used in this table. Click on the <code>Fields</code> button on the  far right to see.</p> <p>We can copy all of a subset of fields that we want matched across the CSV file and Postgres. For this example, we will  take all the fields.</p> JavaScala <pre><code>var myPlan = plan().addForeignKeyRelationship(\n        postgresTask, List.of(\"key\", \"tmp_year\", \"tmp_name\", \"value\"),\n        List.of(Map.entry(csvTask, List.of(\"account_number\", \"year\", \"name\", \"payload\")))\n);\n\nvar conf = ...\n\nexecute(myPlan, conf, postgresTask, csvTask);\n</code></pre> <pre><code>val foreignCols = List(\"order_id\", \"order_placed_on\", \"order_dispatched_on\", \"order_delivered_on\", \"customer_email\",\n  \"customer_address\", \"menu_id\", \"restaurant_id\", \"restaurant_address\", \"menu_item_id\", \"category_id\", \"discount_id\",\n  \"city_id\", \"driver_id\")\n\nval myPlan = plan.addForeignKeyRelationships(\n  csvTask, foreignCols,\n  List(foreignField(postgresTask, \"food_delivery_public.delivery_7_days\", foreignCols))\n)\n\nval conf = ...\n\nexecute(myPlan, conf, postgresTask, csvTask)\n</code></pre> <p>Notice how we have defined the <code>csvTask</code> and <code>foreignCols</code> as the main foreign key but for <code>postgresTask</code>, we had to  define it as a <code>foreignField</code>. This is because <code>postgresTask</code> has multiple tables within it, and we only want to define our foreign key with respect to the <code>public.delivery_7_days</code> table. We use the step name (can be seen from the report)  to specify the table to target. </p> <p>To test this out, we will truncate the <code>public.delivery_7_days</code> table in Postgres first, and then try run again.</p> <pre><code>docker exec marquez-db psql -Upostgres -d food_delivery -c 'TRUNCATE public.delivery_7_days'\n./run.sh\n#input class MyAdvancedMetadataSourceJavaPlanRun or MyAdvancedMetadataSourcePlanRun\ndocker exec marquez-db psql -Upostgres -d food_delivery -c 'SELECT * FROM public.delivery_7_days'\n</code></pre> <pre><code> order_id |     order_placed_on     |   order_dispatched_on   |   order_delivered_on    |        customer_email        |\n       customer_address                     | menu_id | restaurant_id |                   restaurant_address                   | menu\n_item_id | category_id | discount_id | city_id | driver_id\n----------+-------------------------+-------------------------+-------------------------+------------------------------+-------------\n--------------------------------------------+---------+---------------+--------------------------------------------------------+-----\n---------+-------------+-------------+---------+-----------\n    53333 | 2022-10-15 08:40:23.394 | 2023-01-23 09:42:48.397 | 2023-08-12 08:50:52.397 | normand.aufderhar@gmail.com  | Apt. 036 449\n27 Wilderman Forge, Marvinchester, CT 15952 |   40412 |         70130 | Suite 146 98176 Schaden Village, Grahammouth, SD 12354 |\n   90141 |       44210 |       83966 |   78614 |     77449\n</code></pre> <p>Let's grab the first email from the Postgres table and check whether the same record exists in the CSV file.</p> <pre><code>$ cat docker/sample/csv/part-0000* | grep normand.aufderhar\n90141,44210,83966,78614,77449,53333,2022-10-15T08:40:23.394+08:00,2023-01-23T09:42:48.397+08:00,2023-08-12T08:50:52.397+08:00,normand.aufderhar@gmail.com,\"Apt. 036 44927 Wilderman Forge, Marvinchester, CT 15952\",40412,70130,\"Suite 146 98176 Schaden Village, Grahammouth, SD 12354\"\n</code></pre> <p>Great! Now we have the ability to get schema information from an external source, add our own foreign keys and generate  data.</p> <p>Check out the full example under <code>MetadataSourcePlanRun</code> in the example repo.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/","title":"Open Data Contract Standard (ODCS) Source","text":"<p>Creating a data generator for a CSV file based on metadata stored in Open Data Contract Standard (ODCS).</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#open-data-contract-standard-odcs-setup","title":"Open Data Contract Standard (ODCS) Setup","text":"<p>We will be using the following ODCS file for this example.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java/Scala class or YAML file.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedODCSJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedODCSPlanRun.scala</code></li> <li>YAML: <code>docker/data/customer/plan/my-odcs.yaml</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedODCSJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedODCSPlanRun extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-odcs.yaml</code>: <pre><code>name: \"my_odcs_plan\"\ndescription: \"Create account data in CSV via ODCS metadata\"\ntasks:\n  - name: \"csv_account_file\"\n    dataSourceName: \"customer_accounts\"\n</code></pre></p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#schema","title":"Schema","text":"<p>We can point the schema of a data source to our Open Data Contract Standard (ODCS) file.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"my_csv\", \"/opt/app/data/account-odcs\", Map.of(\"header\", \"true\"))\n        .fields(metadataSource().openDataContractStandard(\"/opt/app/mount/odcs/full-example.yaml\"))\n        .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account-odcs\", Map(\"header\" -&gt; \"true\"))\n  .fields(metadataSource.openDataContractStandard(\"/opt/app/mount/odcs/full-example.yaml\"))\n  .count(count.records(100))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-odcs-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/data/csv/account-odcs\"\n      metadataSourceType: \"openDataContractStandard\"\n      dataContractFile: \"/opt/app/mount/odcs/full-example.yaml\"\n    count:\n      records: 100\n</code></pre></p> <ol> <li>Click on <code>Connection</code> tab at the top</li> <li>Select <code>ODCS</code> as the data source and enter <code>example-odcs</code></li> <li>Copy this file into <code>/tmp/odcs/full-example.yaml</code></li> <li>Enter <code>/tmp/odcs/full-example.yaml</code> as the <code>Contract File</code></li> </ol> <p>The above defines that the schema will come from Open Data Contract Standard (ODCS), which is a type of metadata source that contains information about schemas. Specifically, it points to the schema provided here in the <code>docker/mount/odcs</code> folder of data-caterer-example repo.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedODCSJavaPlanRun\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <pre><code>./run.sh MyAdvancedODCSPlanRun\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <pre><code>./run.sh my-odcs.yaml\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>head /tmp/data-caterer/customer/account-odcs/part-00000*\n</code></pre></li> </ol> <p>It should look something like this.</p> <pre><code>txn_ref_dt,rcvr_id,rcvr_cntry_code\n2023-07-11,PB0Wo dMx,nWlbRGIinpJfP\n2024-05-01,5GtkNkHfwuxLKdM,1a\n2024-05-01,OxuATCLAUIhHzr,gSxn2ct\n2024-05-22,P4qe,y9htWZhyjW\n</code></pre> <p>Looks like we have some data now. But we can do better and add some enhancements to it.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#custom-metadata","title":"Custom metadata","text":"<p>We can see from the data generated, that it isn't quite what we want. Sometimes, the metadata is not sufficient for us to produce production-like data yet, and we want to manually edit it. Let's try to add some enhancements to it.</p> <p>Let's make the <code>rcvr_id</code> field follow the regex <code>RC[0-9]{8}</code> and the field <code>rcvr_cntry_code</code> should only be one of either <code>AU, US or TW</code>. For the full guide on data generation options, check the following page.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"my_csv\", \"/opt/app/data/account-odcs\", Map.of(\"header\", \"true\"))\n            .fields(metadata...)\n            .fields(\n                field().name(\"rcvr_id\").regex(\"RC[0-9]{8}\"),\n                field().name(\"rcvr_cntry_code\").oneOf(\"AU\", \"US\", \"TW\")\n            )\n            .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account-odcs\", Map(\"header\" -&gt; \"true\"))\n  .fields(metadata...)\n  .fields(\n    field.name(\"rcvr_id\").regex(\"RC[0-9]{8}\"),\n    field.name(\"rcvr_cntry_code\").oneOf(\"AU\", \"US\", \"TW\")\n  )\n  .count(count.records(100))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-odcs-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/data/csv/account-odcs\"\n      metadataSourceType: \"openDataContractStandard\"\n      dataContractFile: \"/opt/app/mount/odcs/full-example.yaml\"\n    count:\n      records: 100\n    fields:\n      - name: \"rcvr_id\"\n        options:\n          regex: \"RC[0-9]{8}\"\n      - name: \"rcvr_cntry_code\"\n        options:\n          oneOf:\n            - \"AU\"\n            - \"US\"\n            - \"TW\"\n</code></pre></p> <ol> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Go to <code>rcvr_id</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>Regex</code> and enter <code>RC[0-9]{8}</code></li> </ol> </li> <li>Click on <code>+ Field</code><ol> <li>Go to <code>rcvr_cntry_code</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>One Of</code> and enter <code>AU,US,TW</code></li> </ol> </li> </ol> <p>Let's test it out by running it again</p> JavaScalaYAMLUI <pre><code>./run.sh MyAdvancedODCSJavaPlanRun\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <pre><code>./run.sh MyAdvancedODCSPlanRun\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <pre><code>./run.sh my-odcs.yaml\nhead docker/sample/customer/account-odcs/part-00000-*\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>head /tmp/data-caterer/customer/account-odcs/part-00000*\n</code></pre></li> </ol> <pre><code>txn_ref_dt,rcvr_id,rcvr_cntry_code\n2024-02-15,RC02579393,US\n2023-08-18,RC14320425,AU\n2023-07-07,RC17915355,TW\n2024-06-07,RC47347046,TW\n</code></pre> <p>Great! Now we have the ability to get schema information from an external source, add our own metadata and generate data.</p>"},{"location":"docs/guide/data-source/metadata/open-data-contract-standard/#data-validation","title":"Data validation","text":"<p>To find out what data validation options are available, check this link.</p> <p>Another aspect of Open Data Contract Standard (ODCS) that can be leveraged is the definition of data quality rules. Once the latest version of ODCS is released (version 3.x), there should be a vendor neutral definition of data quality rules that Data Caterer can use. Once available, it will be as easy as enabling data validations via <code>enableGenerateValidations</code> in <code>configuration</code>.</p> JavaScalaYAMLUI <pre><code>var conf = configuration().enableGeneratePlanAndTasks(true)\n    .enableGenerateValidations(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\");\n\nexecute(conf, accountTask);\n</code></pre> <pre><code>val conf = configuration.enableGeneratePlanAndTasks(true)\n  .enableGenerateValidations(true)\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(conf, accountTask)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableGenerateValidations = true\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Generate Validations</code></li> </ol> <p>Check out the full example under <code>ODCSSourcePlanRun</code> in the example repo.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/","title":"OpenMetadata Source","text":"<p>Creating a data generator for a JSON file based on metadata stored in OpenMetadata.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#requirements","title":"Requirements","text":"<ul> <li>10 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/data-source/metadata/open-metadata/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#openmetadata-setup","title":"OpenMetadata Setup","text":"<p>You can follow the local docker setup found here to help with setting up OpenMetadata in your local environment.</p> <p>If that page becomes outdated or the link doesn't work, below are the commands I used to run it:</p> <pre><code>mkdir openmetadata-docker &amp;&amp; cd openmetadata-docker\ncurl -sL https://github.com/open-metadata/OpenMetadata/releases/download/1.2.0-release/docker-compose.yml &gt; docker-compose.yml\ndocker compose -f docker-compose.yml up --detach\n</code></pre> <p>Check that the following url works and login with <code>admin:admin</code>. Then you should see some data  like below:</p> <p></p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedOpenMetadataSourceJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedOpenMetadataSourcePlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedOpenMetadataSourceJavaPlanRun extends PlanRun {\n    {\n        var conf = configuration().enableGeneratePlanAndTasks(true)\n            .generatedReportsFolderPath(\"/opt/app/data/report\");\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedOpenMetadataSourcePlanRun extends PlanRun {\n  val conf = configuration.enableGeneratePlanAndTasks(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n}\n</code></pre> <p>We will enable generate plan and tasks so that we can read from external sources for metadata and save the reports under a folder we can easily access.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#schema","title":"Schema","text":"<p>We can point the schema of a data source to our OpenMetadata instance. We will use a JSON data source so that we can show how nested data types are handled and how we could customise it.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#single-schema","title":"Single Schema","text":"JavaScala <pre><code>import io.github.datacatering.datacaterer.api.model.Constants;\n...\n\nvar jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map.of(\"saveMode\", \"overwrite\"))\n        .fields(metadataSource().openMetadataJava(\n            \"http://localhost:8585/api\",                                                              //url\n            Constants.OPEN_METADATA_AUTH_TYPE_OPEN_METADATA(),                                        //auth type\n            Map.of(                                                                                   //additional options (including auth options)\n                Constants.OPEN_METADATA_JWT_TOKEN(), \"abc123\",                                        //get from settings/bots/ingestion-bot\n                Constants.OPEN_METADATA_TABLE_FQN(), \"sample_data.ecommerce_db.shopify.raw_customer\"  //table fully qualified name\n            )\n        ))\n        .count(count().records(10));\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.model.Constants.{OPEN_METADATA_AUTH_TYPE_OPEN_METADATA, OPEN_METADATA_JWT_TOKEN, OPEN_METADATA_TABLE_FQN, SAVE_MODE}\n...\n\nval jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(metadataSource.openMetadata(\n    \"http://localhost:8585/api\",                                                  //url\n    OPEN_METADATA_AUTH_TYPE_OPEN_METADATA,                                        //auth type\n    Map(                                                                          //additional options (including auth options)\n      OPEN_METADATA_JWT_TOKEN -&gt; \"abc123\",                                        //get from settings/bots/ingestion-bot\n      OPEN_METADATA_TABLE_FQN -&gt; \"sample_data.ecommerce_db.shopify.raw_customer\"  //table fully qualified name\n    )\n  ))\n  .count(count.records(10))\n</code></pre> <p>The above defines that the schema will come from OpenMetadata, which is a type of metadata source that contains information about schemas. Specifically, it points to the <code>sample_data.ecommerce_db.shopify.raw_customer</code> table. You can check out the schema here to see what it looks like.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#run","title":"Run","text":"<p>Let's try run and see what happens.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedOpenMetadataSourceJavaPlanRun or MyAdvancedOpenMetadataSourcePlanRun\n#after completing\ncat docker/sample/json/part-00000-*\n</code></pre> <p>It should look something like this.</p> <pre><code>{\n  \"comments\": \"Mh6jqpD5e4M\",\n  \"creditcard\": \"6771839575926717\",\n  \"membership\": \"Za3wCQUl9E  EJj712\",\n  \"orders\": [\n    {\n      \"product_id\": \"Aa6NG0hxfHVq\",\n      \"price\": 16139,\n      \"onsale\": false,\n      \"tax\": 58134,\n      \"weight\": 40734,\n      \"others\": 45813,\n      \"vendor\": \"Kh\"\n    },\n    {\n      \"product_id\": \"zbHBY \",\n      \"price\": 17903,\n      \"onsale\": false,\n      \"tax\": 39526,\n      \"weight\": 9346,\n      \"others\": 52035,\n      \"vendor\": \"jbkbnXAa\"\n    },\n    {\n      \"product_id\": \"5qs3gakppd7Nw5\",\n      \"price\": 48731,\n      \"onsale\": true,\n      \"tax\": 81105,\n      \"weight\": 2004,\n      \"others\": 20465,\n      \"vendor\": \"nozCDMSXRPH Ev\"\n    },\n    {\n      \"product_id\": \"CA6h17ANRwvb\",\n      \"price\": 62102,\n      \"onsale\": true,\n      \"tax\": 96601,\n      \"weight\": 78849,\n      \"others\": 79453,\n      \"vendor\": \" ihVXEJz7E2EFS\"\n    }\n  ],\n  \"platform\": \"GLt9\",\n  \"preference\": {\n    \"key\": \"nmPmsPjg C\",\n    \"value\": true\n  },\n  \"shipping_address\": [\n    {\n      \"name\": \"Loren Bechtelar\",\n      \"street_address\": \"Suite 526 293 Rohan Road, Wunschshire, NE 25532\",\n      \"city\": \"South Norrisland\",\n      \"postcode\": \"56863\"\n    }\n  ],\n  \"shipping_date\": \"2022-11-03\",\n  \"transaction_date\": \"2023-02-01\",\n  \"customer\": {\n    \"username\": \"lance.murphy\",\n    \"name\": \"Zane Brakus DVM\",\n    \"sex\": \"7HcAaPiO\",\n    \"address\": \"594 Loida Haven, Gilland, MA 26071\",\n    \"mail\": \"Un3fhbvK2rEbenIYdnq\",\n    \"birthdate\": \"2023-01-31\"\n  }\n}\n</code></pre> <p>Looks like we have some data now. But we can do better and add some enhancements to it.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#custom-metadata","title":"Custom metadata","text":"<p>We can see from the data generated, that it isn't quite what we want. The metadata is not sufficient for us to produce production-like data yet. Let's try to add some enhancements to it.</p> <p>Let's make the <code>platform</code> field a choice field that can only be a set of certain values and the nested field <code>customer.sex</code> is also from a predefined set of values.</p> JavaScala <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map.of(\"saveMode\", \"overwrite\"))\n            .fields(\n                metadata...\n            ))\n            .fields(\n                field().name(\"platform\").oneOf(\"website\", \"mobile\"),\n                field().name(\"customer\").fields(field().name(\"sex\").oneOf(\"M\", \"F\", \"O\"))\n            )\n            .count(count().records(10));\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/json\", Map(\"saveMode\" -&gt; \"overwrite\"))\n  .fields(\n    metadata...\n  ))\n  .fields(\n    field.name(\"platform\").oneOf(\"website\", \"mobile\"),\n    field.name(\"customer\").fields(field.name(\"sex\").oneOf(\"M\", \"F\", \"O\"))\n  )\n  .count(count.records(10))\n</code></pre> <p>Let's test it out by running it again</p> <pre><code>./run.sh\n#input class MyAdvancedMetadataSourceJavaPlanRun or MyAdvancedMetadataSourcePlanRun\ncat docker/sample/json/part-00000-*\n</code></pre> <pre><code>{\n  \"comments\": \"vqbPUm\",\n  \"creditcard\": \"6304867705548636\",\n  \"membership\": \"GZ1xOnpZSUOKN\",\n  \"orders\": [\n    {\n      \"product_id\": \"rgOokDAv\",\n      \"price\": 77367,\n      \"onsale\": false,\n      \"tax\": 61742,\n      \"weight\": 87855,\n      \"others\": 26857,\n      \"vendor\": \"04XHR64ImMr9T\"\n    }\n  ],\n  \"platform\": \"mobile\",\n  \"preference\": {\n    \"key\": \"IB5vNdWka\",\n    \"value\": true\n  },\n  \"shipping_address\": [\n    {\n      \"name\": \"Isiah Bins\",\n      \"street_address\": \"36512 Ross Spurs, Hillhaven, IA 18760\",\n      \"city\": \"Averymouth\",\n      \"postcode\": \"75818\"\n    },\n    {\n      \"name\": \"Scott Prohaska\",\n      \"street_address\": \"26573 Haley Ports, Dariusland, MS 90642\",\n      \"city\": \"Ashantimouth\",\n      \"postcode\": \"31792\"\n    },\n    {\n      \"name\": \"Rudolf Stamm\",\n      \"street_address\": \"Suite 878 0516 Danica Path, New Christiaport, ID 10525\",\n      \"city\": \"Doreathaport\",\n      \"postcode\": \"62497\"\n    }\n  ],\n  \"shipping_date\": \"2023-08-24\",\n  \"transaction_date\": \"2023-02-01\",\n  \"customer\": {\n    \"username\": \"jolie.cremin\",\n    \"name\": \"Fay Klein\",\n    \"sex\": \"O\",\n    \"address\": \"Apt. 174 5084 Volkman Creek, Hillborough, PA 61959\",\n    \"mail\": \"BiTmzb7\",\n    \"birthdate\": \"2023-04-07\"\n  }\n}\n</code></pre> <p>Great! Now we have the ability to get schema information from an external source, add our own metadata and generate  data.</p>"},{"location":"docs/guide/data-source/metadata/open-metadata/#data-validation","title":"Data validation","text":"<p>Another aspect of OpenMetadata that can be leveraged is the definition of data quality rules. These rules can be  incorporated into your Data Caterer job as well by enabling data validations via <code>enableGenerateValidations</code> in  <code>configuration</code>.</p> JavaScala <pre><code>var conf = configuration().enableGeneratePlanAndTasks(true)\n    .enableGenerateValidations(true)\n    .generatedReportsFolderPath(\"/opt/app/data/report\");\n\nexecute(conf, jsonTask);\n</code></pre> <pre><code>val conf = configuration.enableGeneratePlanAndTasks(true)\n  .enableGenerateValidations(true)\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(conf, jsonTask)\n</code></pre> <p>Check out the full example under <code>OpenMetadataSourcePlanRun</code> in the example repo.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/","title":"Auto Generate From Data Connection","text":"<p>Creating a data generator based on only a data connection to Postgres.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/auto-generate-connection/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedAutomatedJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedAutomatedPlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedAutomatedJavaPlanRun extends PlanRun {\n    {\n        var autoRun = configuration()\n                .postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n                .enableGeneratePlanAndTasks(true)                                                 (2)\n                .generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (3)\n                .enableUniqueCheck(true)                                                          (4)\n                .generatedReportsFolderPath(\"/opt/app/data/report\");\n\n        execute(autoRun);\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedAutomatedPlanRun extends PlanRun {\n\n  val autoRun = configuration\n    .postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n    .enableGeneratePlanAndTasks(true)                                                 (2)\n    .generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (3)\n    .enableUniqueCheck(true)                                                          (4)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n\n  execute(configuration = autoRun)\n}\n</code></pre> <p>In the above code, we note the following:</p> <ol> <li>Data source configuration to a Postgres data source called <code>my_postgres</code></li> <li>We have enabled the flag <code>enableGeneratePlanAndTasks</code> which tells Data Caterer to go to <code>my_postgres</code> and generate    data for all the tables found under the database <code>customer</code> (which is defined in the connection string).</li> <li>The config <code>generatedPlanAndTaskFolderPath</code> defines where the metadata that is gathered from <code>my_postgres</code> should be    saved at so that we could re-use it later.</li> <li><code>enableUniqueCheck</code> is set to true to ensure that generated data is unique based on primary key or foreign key    definitions.</li> </ol> <p>Note</p> <p>Unique check will only ensure generated data is unique. Any existing data in your data source is not taken into  account, so generated data may fail to insert depending on the data source restrictions</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#postgres-setup","title":"Postgres Setup","text":"<p>If you don't have your own Postgres up and running, you can set up and run an instance configured in the <code>docker</code> folder via.</p> <pre><code>cd docker\ndocker-compose up -d postgres\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c '\\dt+ account.*'\n</code></pre> <p>This will create the tables found under <code>docker/data/sql/postgres/customer.sql</code>. You can change this file to contain your own tables. We can see there are 4 tables created for us, <code>accounts, balances, transactions and mapping</code>.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedAutomatedJavaPlanRun or MyAdvancedAutomatedPlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1;'\n</code></pre> <p>It should look something like this.</p> <pre><code>   id   | account_number  | account_status | created_by | created_by_fixed_length | customer_id_int | customer_id_smallint | customer_id_bigint |   customer_id_decimal    | customer_id_real | customer_id_double | open_date  |     open_timestamp      | last_opened_time |                                                           payload_bytes\n--------+-----------------+----------------+------------+-------------------------+-----------------+----------------------+--------------------+--------------------------+------------------+--------------------+------------+-------------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------\n 100414 | 5uROOVOUyQUbubN | h3H            | SfA0eZJcTm | CuRw                    |              13 |                   42 |               6041 | 76987.745612542900000000 |         91866.78 |  66400.37433202339 | 2023-03-05 | 2023-08-14 11:33:11.343 | 23:58:01.736     | \\x604d315d4547616e6a233050415373317274736f5e682d516132524f3d23233c37463463322f342d34376d597e665d6b3d395b4238284028622b7d6d2b4f5042\n(1 row)\n</code></pre> <p>The data that gets inserted will follow the foreign keys that are defined within Postgres and also ensure the insertion order is correct.</p> <p>Also check the HTML report that gets generated under <code>docker/sample/report/index.html</code>. You can see a summary of what was generated along with other metadata.</p> <p>You can now look to play around with other tables or data sources and auto generate for them.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#additional-topics","title":"Additional Topics","text":""},{"location":"docs/guide/scenario/auto-generate-connection/#learn-from-existing-data","title":"Learn From Existing Data","text":"<p>If you have any existing data within your data source, Data Caterer will gather metadata about the existing data to help guide it when generating new data. There are configurations that can help tune the metadata analysis found here.</p>"},{"location":"docs/guide/scenario/auto-generate-connection/#filter-out-schematables","title":"Filter Out Schema/Tables","text":"<p>As part of your connection definition, you can define any schemas and/or tables your don't want to generate data for. In the example below, it will not generate any data for any tables under the <code>history</code> and <code>audit</code> schemas. Also, any table with the name <code>balances</code> or <code>transactions</code> in any schema will also not have data generated.</p> JavaScala <pre><code>var autoRun = configuration()\n        .postgres(\n              \"my_postgres\", \n              \"jdbc:postgresql://host.docker.internal:5432/customer\",\n              Map.of(\n                  \"filterOutSchema\", \"history, audit\",\n                  \"filterOutTable\", \"balances, transactions\")\n              )\n        )\n</code></pre> <pre><code>val autoRun = configuration\n  .postgres(\n    \"my_postgres\",\n    \"jdbc:postgresql://host.docker.internal:5432/customer\",\n    Map(\n      \"filterOutSchema\" -&gt; \"history, audit\",\n      \"filterOutTable\" -&gt; \"balances, transactions\")\n    )\n  )\n</code></pre>"},{"location":"docs/guide/scenario/auto-generate-connection/#define-record-count","title":"Define record count","text":"<p>You can control the record count per sub data source via <code>numRecordsPerStep</code>.</p> JavaScala <pre><code>var autoRun = configuration()\n      ...\n      .numRecordsPerStep(100)\n\nexecute(autoRun)\n</code></pre> <pre><code>val autoRun = configuration\n  ...\n  .numRecordsPerStep(100)\n\nexecute(configuration = autoRun)\n</code></pre>"},{"location":"docs/guide/scenario/batch-and-event/","title":"Generate Batch and Event Data","text":"<p>Creating a data generator for Kafka topic with matching records in a CSV file.</p>"},{"location":"docs/guide/scenario/batch-and-event/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/batch-and-event/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/batch-and-event/#kafka-setup","title":"Kafka Setup","text":"<p>If you don't have your own Kafka up and running, you can set up and run an instance configured in the <code>docker</code> folder via.</p> <pre><code>cd docker\ndocker-compose up -d kafka\ndocker exec docker-kafkaserver-1 kafka-topics --bootstrap-server localhost:9092 --list\n</code></pre> <p>Let's create a task for inserting data into the <code>account-topic</code> that is already defined under<code>docker/data/kafka/setup_kafka.sh</code>.</p>"},{"location":"docs/guide/scenario/batch-and-event/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedBatchEventJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedBatchEventPlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedBatchEventJavaPlanRun extends PlanRun {\n    {\n        var kafkaTask = new AdvancedKafkaJavaPlanRun().getKafkaTask();\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedBatchEventPlanRun extends PlanRun {\n  val kafkaTask = new AdvancedKafkaPlanRun().kafkaTask\n}\n</code></pre> <p>We will borrow the Kafka task that is already defined under the class <code>KafkaPlanRun</code> or <code>KafkaJavaPlanRun</code>. You can go through the Kafka guide here for more details.</p>"},{"location":"docs/guide/scenario/batch-and-event/#schema","title":"Schema","text":"<p>Let us set up the corresponding schema for the CSV file where we want to match the values that are generated for the Kafka messages.</p> JavaScala <pre><code>var kafkaTask = new AdvancedKafkaJavaPlanRun().getKafkaTask();\n\nvar csvTask = csv(\"my_csv\", \"/opt/app/data/csv/account\")\n        .fields(\n                field().name(\"account_number\"),\n                field().name(\"year\"),\n                field().name(\"name\"),\n                field().name(\"payload\")\n        );\n</code></pre> <pre><code>val kafkaTask = new AdvancedKafkaPlanRun().kafkaTask\n\nval csvTask = csv(\"my_csv\", \"/opt/app/data/csv/account\")\n  .fields(\n    field.name(\"account_number\"),\n    field.name(\"year\"),\n    field.name(\"name\"),\n    field.name(\"payload\")\n)\n</code></pre> <p>This is a simple schema where we want to use the values and metadata that is already defined in the <code>kafkaTask</code> to determine what the data will look like for the CSV file. Even if we defined some metadata here, it would be overridden when we define our foreign key relationships.</p>"},{"location":"docs/guide/scenario/batch-and-event/#foreign-keys","title":"Foreign Keys","text":"<p>From the above CSV schema, we see note the following against the Kafka schema:</p> <ul> <li><code>account_number</code> in CSV needs to match with the <code>account_id</code> in Kafka<ul> <li>We see that <code>account_id</code> is referred to in the <code>key</code> field as <code>field.name(\"key\").sql(\"content.account_id\")</code></li> </ul> </li> <li><code>year</code> needs to match with <code>content.year</code> in Kafka, which is a nested field<ul> <li>We can only do foreign key relationships with top level fields, not nested fields. So we define a new field   called <code>tmp_year</code> which will not appear in the final output for the Kafka messages but is used as an intermediate   step <code>field.name(\"tmp_year\").sql(\"content.year\").omit(true)</code></li> </ul> </li> <li><code>name</code> needs to match with <code>content.details.name</code> in Kafka, also a nested field<ul> <li>Using the same logic as above, we define a temporary field called <code>tmp_name</code> which will take the value of the   nested field but will be omitted <code>field.name(\"tmp_name\").sql(\"content.details.name\").omit(true)</code></li> </ul> </li> <li><code>payload</code> represents the whole JSON message sent to Kafka, which matches to <code>value</code> field</li> </ul> <p>Our foreign keys are therefore defined like below. Order is important when defining the list of fields. The index needs to match with the corresponding field in the other data source.</p> JavaScala <pre><code>var myPlan = plan().addForeignKeyRelationship(\n        kafkaTask, List.of(\"key\", \"tmp_year\", \"tmp_name\", \"value\"),\n        List.of(Map.entry(csvTask, List.of(\"account_number\", \"year\", \"name\", \"payload\")))\n);\n\nvar conf = configuration()\n      .generatedReportsFolderPath(\"/opt/app/data/report\");\n\nexecute(myPlan, conf, kafkaTask, csvTask);\n</code></pre> <pre><code>val myPlan = plan.addForeignKeyRelationship(\n    kafkaTask, List(\"key\", \"tmp_year\", \"tmp_name\", \"value\"),\n    List(csvTask -&gt; List(\"account_number\", \"year\", \"name\", \"payload\"))\n)\n\nval conf = configuration.generatedReportsFolderPath(\"/opt/app/data/report\")\n\nexecute(myPlan, conf, kafkaTask, csvTask)\n</code></pre>"},{"location":"docs/guide/scenario/batch-and-event/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedBatchEventJavaPlanRun or MyAdvancedBatchEventPlanRun\n#after completing\ndocker exec docker-kafkaserver-1 kafka-console-consumer --bootstrap-server localhost:9092 --topic account-topic --from-beginning\n</code></pre> <p>It should look something like this.</p> <pre><code>{\"account_id\":\"ACC03093143\",\"year\":2023,\"amount\":87990.37196728592,\"details\":{\"name\":\"Nadine Heidenreich Jr.\",\"first_txn_date\":\"2021-11-09\",\"updated_by\":{\"user\":\"YfEyJCe8ohrl0j IfyT\",\"time\":\"2022-09-26T20:47:53.404Z\"}},\"transactions\":[{\"txn_date\":\"2021-11-09\",\"amount\":97073.7914706189}]}\n{\"account_id\":\"ACC08764544\",\"year\":2021,\"amount\":28675.58758765888,\"details\":{\"name\":\"Delila Beer\",\"first_txn_date\":\"2021-05-19\",\"updated_by\":{\"user\":\"IzB5ksXu\",\"time\":\"2023-01-26T20:47:26.389Z\"}},\"transactions\":[{\"txn_date\":\"2021-10-01\",\"amount\":80995.23818711648},{\"txn_date\":\"2021-05-19\",\"amount\":92572.40049217848},{\"txn_date\":\"2021-12-11\",\"amount\":99398.79832225188}]}\n{\"account_id\":\"ACC62505420\",\"year\":2023,\"amount\":96125.3125884202,\"details\":{\"name\":\"Shawn Goodwin\",\"updated_by\":{\"user\":\"F3dqIvYp2pFtena4\",\"time\":\"2023-02-11T04:38:29.832Z\"}},\"transactions\":[]}\n</code></pre> <p>Let's also check if there is a corresponding record in the CSV file.</p> <pre><code>$ cat docker/sample/csv/account/part-0000* | grep ACC03093143\nACC03093143,2023,Nadine Heidenreich Jr.,\"{\\\"account_id\\\":\\\"ACC03093143\\\",\\\"year\\\":2023,\\\"amount\\\":87990.37196728592,\\\"details\\\":{\\\"name\\\":\\\"Nadine Heidenreich Jr.\\\",\\\"first_txn_date\\\":\\\"2021-11-09\\\",\\\"updated_by\\\":{\\\"user\\\":\\\"YfEyJCe8ohrl0j IfyT\\\",\\\"time\\\":\\\"2022-09-26T20:47:53.404Z\\\"}},\\\"transactions\\\":[{\\\"txn_date\\\":\\\"2021-11-09\\\",\\\"amount\\\":97073.7914706189}]}\"\n</code></pre> <p>Great! The account, year, name and payload look to all match up.</p>"},{"location":"docs/guide/scenario/batch-and-event/#additional-topics","title":"Additional Topics","text":""},{"location":"docs/guide/scenario/batch-and-event/#order-of-execution","title":"Order of execution","text":"<p>You may notice that the events are generated first, then the CSV file. This is because as part of the <code>execute</code> function, we passed in the <code>kafkaTask</code> first, before the <code>csvTask</code>. You can change the order of execution by passing in <code>csvTask</code> before <code>kafkaTask</code> into the <code>execute</code> function.</p>"},{"location":"docs/guide/scenario/data-generation/","title":"Data Generation","text":"<p>Go through the available options for data generation. Creating a data generator for a CSV file.</p> <p></p>"},{"location":"docs/guide/scenario/data-generation/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/data-generation/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/data-generation/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class or plan YAML.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyCsvPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyCsvPlan.scala</code></li> <li>YAML: <code>docker/data/customer/plan/my-csv.yaml</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScalaYAMLUI <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyCsvJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyCsvPlan extends PlanRun {\n}\n</code></pre> <p>In <code>docker/data/custom/plan/my-csv.yaml</code>: <pre><code>name: \"my_csv_plan\"\ndescription: \"Create account data in CSV file\"\ntasks:\n  - name: \"csv_account_file\"\n    dataSourceName: \"customer_accounts\"\n</code></pre></p> <p>Go to next section.</p> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/scenario/data-generation/#connection-configuration","title":"Connection Configuration","text":"<p>When dealing with CSV files, we need to define a path for our generated CSV files to be saved at, along with any other high level configurations.</p> JavaScalaYAMLUI <pre><code>csv(\n  \"customer_accounts\",              //name\n  \"/opt/app/data/customer/account\", //path\n  Map.of(\"header\", \"true\")          //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p> <pre><code>csv(\n  \"customer_accounts\",              //name\n  \"/opt/app/data/customer/account\", //path\n  Map(\"header\" -&gt; \"true\")           //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>csv {\n  customer_accounts {\n    path = \"/opt/app/data/customer/account\"\n    path = ${?CSV_PATH}\n    header = \"true\"\n  }\n}\n</code></pre></p> <ol> <li>Go to <code>Connection</code> tab in the top bar</li> <li>Select data source as <code>CSV</code><ol> <li>Enter in data source name <code>customer_accounts</code></li> <li>Enter path as <code>/tmp/data-caterer/customer/account</code></li> </ol> </li> </ol>"},{"location":"docs/guide/scenario/data-generation/#schema","title":"Schema","text":"<p>Our CSV file that we generate should adhere to a defined schema where we can also define data types.</p> <p>Let's define each field along with their corresponding data type. You will notice that the <code>string</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\"),\n                field().name(\"balance\").type(DoubleType.instance()),\n                field().name(\"created_by\"),\n                field().name(\"name\"),\n                field().name(\"open_time\").type(TimestampType.instance()),\n                field().name(\"status\")\n        );\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"balance\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"name\"),\n    field.name(\"open_time\").`type`(TimestampType),\n    field.name(\"status\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"balance\"\n        type: \"double\"\n      - name: \"created_by\"\n      - name: \"name\"\n      - name: \"open_time\"\n        type: \"timestamp\"\n      - name: \"status\"\n</code></pre></p> <ol> <li>Go to <code>Home</code> tab in the top bar</li> <li>Enter <code>my-csv</code> as the <code>Plan name</code></li> <li>Under <code>Tasks</code>, enter <code>csv-account-task</code> as <code>Task name</code> and select data source as <code>customer_accounts</code></li> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add field <code>account_id</code> with type <code>string</code></li> <li>Add field <code>balance</code> with type <code>double</code></li> <li>Add field <code>created_by</code> with type <code>string</code></li> <li>Add field <code>name</code> with type <code>string</code></li> <li>Add field <code>open_time</code> with type <code>timestamp</code></li> <li>Add field <code>status</code> with type <code>string</code></li> </ol> </li> </ol>"},{"location":"docs/guide/scenario/data-generation/#field-metadata","title":"Field Metadata","text":"<p>We could stop here and generate random data for the accounts table. But wouldn't it be more useful if we produced data that is closer to the structure of the data that would come in production? We can do this by defining various metadata attributes that add guidelines that the data generator will understand when generating data.</p>"},{"location":"docs/guide/scenario/data-generation/#account_id","title":"account_id","text":"<ul> <li><code>account_id</code> follows a particular pattern that where it starts with <code>ACC</code> and has 8 digits after it.   This can be defined via a regex like below. Alongside, we also mention that values are unique ensure that   unique values are generated.</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre> <pre><code>field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre> <pre><code>fields:\n  - name: \"account_id\"\n    options:\n      regex: \"ACC[0-9]{8}\"\n      unique: true\n</code></pre> <ol> <li>Go to <code>account_id</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>Regex</code> and enter <code>ACC[0-9]{8}</code></li> <li>Click <code>Unique</code> and select <code>true</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#balance","title":"balance","text":"<ul> <li><code>balance</code> let's make the numbers not too large, so we can define a min and max for the generated numbers to be between   <code>1</code> and <code>1000</code>.</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n</code></pre> <pre><code>field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n</code></pre> <pre><code>fields:\n  - name: \"balance\"\n    type: \"double\"\n    options:\n      min: 1\n      max: 1000\n</code></pre> <ol> <li>Go to <code>balance</code> field</li> <li>Click on <code>+</code> dropdown next to <code>double</code> data type</li> <li>Click <code>Min</code> and enter <code>1</code></li> <li>Click <code>Max</code> and enter <code>1000</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#name","title":"name","text":"<ul> <li><code>name</code> is a string that also follows a certain pattern, so we could also define a regex but here we will choose to   leverage the DataFaker library and create an <code>expression</code> to generate real looking name. All possible faker   expressions   can be found here</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"name\").expression(\"#{Name.name}\"),\n</code></pre> <pre><code>field.name(\"name\").expression(\"#{Name.name}\"),\n</code></pre> <pre><code>fields:\n  - name: \"name\"\n    options:\n      expression: \"#{Name.name}\"\n</code></pre> <ol> <li>Go to <code>name</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>Faker Expression</code> and enter <code>#{Name.name}</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#open_time","title":"open_time","text":"<ul> <li><code>open_time</code> is a timestamp that we want to have a value greater than a specific date. We can define a min date by   using   <code>java.sql.Date</code> like below.</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre> <pre><code>field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre> <pre><code>fields:\n  - name: \"open_time\"\n    type: \"timestamp\"\n    options:\n      min: \"2022-01-01\"\n</code></pre> <ol> <li>Go to <code>open_time</code> field</li> <li>Click on <code>+</code> dropdown next to <code>timestamp</code> data type</li> <li>Click <code>Min</code> and enter <code>2022-01-01</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#status","title":"status","text":"<ul> <li><code>status</code> is a field that can only obtain one of four values, <code>open, closed, suspended or pending</code>.</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre> <pre><code>field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre> <pre><code>fields:\n  - name: \"status\"\n    options:\n      oneOf:\n        - \"open\"\n        - \"closed\"\n        - \"suspended\"\n        - \"pending\"\n</code></pre> <ol> <li>Go to <code>status</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>One Of</code> and enter <code>open,closed,suspended,pending</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#created_by","title":"created_by","text":"<ul> <li><code>created_by</code> is a field that is based on the <code>status</code> field where it follows the   logic: <code>if status is open or closed, then   it is created_by eod else created_by event</code>. This can be achieved by defining a SQL expression like below.</li> </ul> JavaScalaYAMLUI <pre><code>field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <pre><code>field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <pre><code>fields:\n  - name: \"created_by\"\n    options:\n      sql: \"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"\n</code></pre> <ol> <li>Go to <code>created_by</code> field</li> <li>Click on <code>+</code> dropdown next to <code>string</code> data type</li> <li>Click <code>SQL</code> and enter <code>CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END</code></li> </ol> <p>Putting it all the fields together, our structure should now look like this.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                field().name(\"name\").expression(\"#{Name.name}\"),\n                field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n        );\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n    field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n    field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n    field.name(\"name\").expression(\"#{Name.name}\"),\n    field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n    field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/account\"\n    count:\n      records: 100\n    fields:\n      - name: \"account_id\"\n        options:\n          regex: \"ACC1[0-9]{9}\"\n          unique: true\n      - name: \"balance\"\n        type: \"double\"\n        options:\n          min: 1\n          max: 1000\n      - name: \"created_by\"\n        options:\n          sql: \"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"\n      - name: \"name\"\n        options:\n          expression: \"#{Name.name}\"\n      - name: \"open_time\"\n        type: \"timestamp\"\n        options:\n          min: \"2022-01-01\"\n      - name: \"status\"\n        options:\n          oneOf:\n            - \"open\"\n            - \"closed\"\n            - \"suspended\"\n            - \"pending\"\n</code></pre></p> <p>Open <code>Task</code> and <code>Generation</code> to see all the fields.</p>"},{"location":"docs/guide/scenario/data-generation/#record-count","title":"Record Count","text":"<p>We only want to generate 100 records, so that we can see what the output looks like. This is controlled at the <code>accountTask</code> level like below. If you want to generate more records, set it to the value you want.</p> JavaScalaYAMLUI <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.records(100))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/transactions\"\n    count:\n      records: 100\n    fields:\n      ...\n</code></pre></p> <ol> <li>Under task <code>customer_accounts</code>, click on <code>Generation</code></li> <li>Under title <code>Record Count</code>, set <code>Records</code> to <code>100</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScalaYAMLUI <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre> <p>In <code>docker/data/custom/application.conf</code>: <pre><code>flags {\n  enableUniqueCheck = true\n}\nfolders {\n  generatedReportsFolderPath = \"/opt/app/data/report\"\n}\n</code></pre></p> <ol> <li>Click on <code>Advanced Configuration</code> towards the bottom of the screen</li> <li>Click on <code>Flag</code> and click on <code>Unique Check</code></li> <li>Click on <code>Folder</code> and enter <code>/tmp/data-caterer/report</code> for <code>Generated Reports Folder Path</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScalaYAMLUI <pre><code>public class MyCsvJavaPlan extends PlanRun {\n    {\n        var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                        field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"name\").expression(\"#{Name.name}\"),\n                        field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\n  val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n      field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n    val config = configuration\n      .generatedReportsFolderPath(\"/opt/app/data/report\")\n      .enableUniqueCheck(true)\n\n    execute(config, accountTask)\n}\n</code></pre> <p>Plan and task file should be ready.</p> <ol> <li>Click <code>Save</code> at the top</li> </ol>"},{"location":"docs/guide/scenario/data-generation/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> JavaScalaYAMLUI <pre><code>./run.sh MyCsvJavaPlan\nhead docker/sample/customer/account/part-00000*\n</code></pre> <pre><code>./run.sh MyCsvPlan\nhead docker/sample/customer/account/part-00000*\n</code></pre> <pre><code>./run.sh my-csv.yaml\nhead docker/sample/customer/account/part-00000*\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>head /tmp/data-caterer/customer/account/part-00000*\n</code></pre></li> </ol> <p>Your output should look like this.</p> <pre><code>account_id,balance,created_by,name,open_time,status\nACC06192462,853.9843359645766,eod,Hoyt Kertzmann MD,2023-07-22T11:17:01.713Z,closed\nACC15350419,632.5969895326234,eod,Dr. Claude White,2022-12-13T21:57:56.840Z,open\nACC25134369,592.0958847218986,eod,Fabian Rolfson,2023-04-26T04:54:41.068Z,open\nACC48021786,656.6413439322964,eod,Dewayne Stroman,2023-05-17T06:31:27.603Z,open\nACC26705211,447.2850352884595,event,Garrett Funk,2023-07-14T03:50:22.746Z,pending\nACC03150585,750.4568929015996,event,Natisha Reichel,2023-04-11T11:13:10.080Z,suspended\nACC29834210,686.4257811608622,event,Gisele Ondricka,2022-11-15T22:09:41.172Z,suspended\nACC39373863,583.5110618128994,event,Thaddeus Ortiz,2022-09-30T06:33:57.193Z,suspended\nACC39405798,989.2623959059525,eod,Shelby Reinger,2022-10-23T17:29:17.564Z,open\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/scenario/data-generation/#join-with-another-csv","title":"Join With Another CSV","text":"<p>Now that we have generated some accounts, let's also try to generate a set of transactions for those accounts in CSV format as well. The transactions could be in any other format, but to keep this simple, we will continue using CSV.</p> <p>We can define our schema the same way along with any additional metadata.</p> JavaScalaYAMLUI <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\"),\n                field().name(\"name\"),\n                field().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\n                field().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                field().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n        );\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"full_name\"),\n    field.name(\"amount\").`type`(DoubleType).min(1).max(100),\n    field.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n    field.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n  )\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/account\"\n    ...\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/transactions\"\n    fields:\n      - name: \"account_id\"\n      - name: \"full_name\"\n      - name: \"amount\"\n        type: \"double\"\n        options:\n          min: 1\n          max: 100\n      - name: \"time\"\n        type: \"timestamp\"\n        options:\n          min: \"2022-01-01\"\n      - name: \"date\"\n        type: \"date\"\n        options:\n          sql: \"DATE(time)\"\n</code></pre></p> <ol> <li>Go to <code>Connection</code> tab and add new <code>CSV</code> data source with path <code>/tmp/data-caterer/customer/transactions</code></li> <li>Go to <code>Plan</code> tab and click on <code>Edit</code> for <code>my-csv</code></li> <li>Click on <code>+ Task</code> towards the top</li> <li>Under the new task, enter <code>csv-transaction-task</code> as <code>Task name</code> and select data source as <code>customer_accounts</code></li> <li>Click on <code>Generation</code> and tick the <code>Manual</code> checkbox</li> <li>Click on <code>+ Field</code><ol> <li>Add field <code>account_id</code> with type <code>string</code></li> <li>Add field <code>balance</code> with type <code>double</code></li> <li>Add field <code>created_by</code> with type <code>string</code></li> <li>Add field <code>name</code> with type <code>string</code></li> <li>Add field <code>open_time</code> with type <code>timestamp</code></li> <li>Add field <code>status</code> with type <code>string</code></li> </ol> </li> </ol>"},{"location":"docs/guide/scenario/data-generation/#records-per-field","title":"Records Per Field","text":"<p>Usually, for a given <code>account_id, full_name</code>, there should be multiple records for it as we want to simulate a customer having multiple transactions. We can achieve this through defining the number of records to generate in the <code>count</code> function.</p> JavaScalaYAMLUI <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(...)\n        .count(count().recordsPerField(5, \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(...)\n  .count(count.recordsPerField(5, \"account_id\", \"full_name\"))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    ...\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/transactions\"\n    count:\n      records: 100\n      perField:\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n        count: 5\n</code></pre></p> <ol> <li>Under title <code>Record count</code>, click on <code>Advanced</code></li> <li>Enter <code>account_id,name</code> in <code>Field(s)</code></li> <li>Click on <code>Per unique set of values</code> checkbox</li> <li>Set <code>Records</code> to <code>5</code></li> </ol>"},{"location":"docs/guide/scenario/data-generation/#random-records-per-field","title":"Random Records Per Field","text":"<p>Above, you will notice that we are generating 5 records per <code>account_id, full_name</code>. This is okay but still not quite reflective of the real world. Sometimes, people have accounts with no transactions in them, or they could have many. We can accommodate for this via defining a random number of records per field.</p> JavaScalaYAMLUI <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().recordsPerFieldGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.recordsPerFieldGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n</code></pre> <p>In <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>: <pre><code>name: \"csv_account_file\"\nsteps:\n  - name: \"accounts\"\n    ...\n  - name: \"transactions\"\n    type: \"csv\"\n    options:\n      path: \"/opt/app/custom/csv/transactions\"\n    count:\n      records: 100\n      perField:\n        fieldNames:\n          - \"account_id\"\n          - \"name\"\n        options:\n          min: 0\n          max: 5\n</code></pre></p> <ol> <li>Under title <code>Record count</code>, click on <code>Advanced</code></li> <li>Enter <code>account_id,name</code> in <code>Field(s)</code></li> <li>Click on <code>Per unique set of values between</code> checkbox</li> <li>Set <code>Min</code> to <code>0</code> and <code>Max to</code>5`</li> </ol> <p>Here we set the minimum number of records per field to be 0 and the maximum to 5.</p>"},{"location":"docs/guide/scenario/data-generation/#foreign-key","title":"Foreign Key","text":"<p>In this scenario, we want to match the <code>account_id</code> in <code>account</code> to match the same field values in <code>transaction</code>. We also want to match <code>name</code> in <code>account</code> to <code>full_name</code> in <code>transaction</code>. This can be done via plan configuration like below.</p> JavaScalaYAMLUI <pre><code>var myPlan = plan().addForeignKeyRelationship(\n        accountTask, List.of(\"account_id\", \"name\"), //the task and fields we want linked\n        List.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\"))) //list of other tasks and their respective field names we want matched\n);\n</code></pre> <pre><code>val myPlan = plan.addForeignKeyRelationship(\n  accountTask, List(\"account_id\", \"name\"),  //the task and fields we want linked\n  List(transactionTask -&gt; List(\"account_id\", \"full_name\"))  //list of other tasks and their respective field names we want matched\n)\n</code></pre> <p>In <code>docker/data/custom/plan/my-csv.yaml</code>: ```yaml name: \"my_csv_plan\" description: \"Create account data in CSV file\" tasks:   - name: \"csv_account_file\"     dataSourceName: \"customer_accounts\"</p> <p>sinkOptions:   foreignKeys:     - source:         dataSource: \"customer_accounts\"         step: \"accounts\"         fields: [\"account_id\", \"name\"]       generate:         - dataSource: \"customer_accounts\"           step: \"transactions\"           fields: [\"account_id\", \"full_name\"]   ```</p> <ol> <li>Click <code>Relationships</code> and then click <code>+ Relationship</code></li> <li>Select <code>csv-account-task</code> and enter <code>account_id,name</code> in <code>Field(s)</code></li> <li>Open <code>Generation</code> and click <code>+ Link</code></li> <li>Select <code>csv-transaction-task</code> and enter <code>account_id,full_name</code> in <code>Field(s)</code></li> </ol> <p>Now, stitching it all together for the <code>execute</code> function, our final plan should look like this.</p> JavaScalaYAMLUI <pre><code>public class MyCsvJavaPlan extends PlanRun {\n    {\n        var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                        field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"name\").expression(\"#{Name.name}\"),\n                        field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                )\n                .count(count().records(100));\n\n        var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\"),\n                        field().name(\"name\"),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\n                        field().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n                )\n                .count(count().recordsPerFieldGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        var myPlan = plan().addForeignKeyRelationship(\n                accountTask, List.of(\"account_id\", \"name\"),\n                List.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\")))\n        );\n\n        execute(myPlan, config, accountTask, transactionTask);\n    }\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\n  val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n      field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n    .count(count.records(100))\n\n  val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\"),\n      field.name(\"name\"),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(100),\n      field.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n    )\n    .count(count.recordsPerFieldGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  val myPlan = plan.addForeignKeyRelationship(\n    accountTask, List(\"account_id\", \"name\"),\n    List(transactionTask -&gt; List(\"account_id\", \"full_name\"))\n  )\n\n  execute(myPlan, config, accountTask, transactionTask)\n}\n</code></pre> <p>Check content of <code>docker/data/custom/plan/my-csv.yaml</code> and <code>docker/data/custom/task/file/csv/csv-account-task.yaml</code>.</p> <p>Open UI dropdowns to see all details.</p> <p>Let's clean up the old data and try run again.</p> <pre><code>#clean up old data\nrm -rf docker/sample/customer/account\n</code></pre> JavaScalaYAMLUI <pre><code>./run.sh MyCsvJavaPlan\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <pre><code>./run.sh MyCsvPlan\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <pre><code>./run.sh my-csv.yaml\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <ol> <li>Click on <code>Execute</code> at the top <pre><code>account=$(tail -1 /tmp/data-caterer/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat /tmp/data-caterer/customer/transaction/part-00000* | grep $account\n</code></pre></li> </ol> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>DocumentationJavaPlanRun.java</code> or <code>DocumentationPlanRun.scala</code> files as well to check that your plan is the same.</p>"},{"location":"docs/guide/scenario/data-validation/","title":"Data Validations","text":"<p>Creating a data validator for a JSON file.</p> <p></p>"},{"location":"docs/guide/scenario/data-validation/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/data-validation/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/data-validation/#data-setup","title":"Data Setup","text":"<p>To aid in showing the functionality of data validations, we will first generate some data that our validations will run against. Run the below command and it will generate JSON files under <code>docker/sample/json</code> folder.</p> <pre><code>./run.sh JsonPlan\n</code></pre>"},{"location":"docs/guide/scenario/data-validation/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyValidationJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyValidationPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyValidationJavaPlan extends PlanRun {\n    {\n        var jsonTask = json(\"my_json\", \"/opt/app/data/json\");\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableValidation(true)\n                .enableGenerateData(false);\n\n        execute(config, jsonTask);\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyValidationPlan extends PlanRun {\n  val jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableValidation(true)\n    .enableGenerateData(false)\n\n  execute(config, jsonTask)\n}\n</code></pre> <p>As noted above, we create a JSON task that points to where the JSON data has been created at folder <code>/opt/app/data/json</code> . We also note that <code>enableValidation</code> is set to <code>true</code> and <code>enableGenerateData</code> to <code>false</code> to tell Data Catering, we only want to validate data.</p>"},{"location":"docs/guide/scenario/data-validation/#validations","title":"Validations","text":"<p>For reference, the schema in which we will be validating against looks like the below.</p> <pre><code>.fields(\n  field.name(\"account_id\"),\n  field.name(\"year\").`type`(IntegerType),\n  field.name(\"balance\").`type`(DoubleType),\n  field.name(\"date\").`type`(DateType),\n  field.name(\"status\"),\n  field.name(\"update_history\").`type`(ArrayType)\n    .fields(\n      field.name(\"updated_time\").`type`(TimestampType),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"pending\", \"suspended\"),\n    ),\n  field.name(\"customer_details\")\n    .fields(\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"age\").`type`(IntegerType),\n      field.name(\"city\").expression(\"#{Address.city}\")\n    )\n)\n</code></pre>"},{"location":"docs/guide/scenario/data-validation/#basic-validation","title":"Basic Validation","text":"<p>Let's say our goal is to validate the <code>customer_details.name</code> field to ensure it conforms to the regex pattern <code>[A-Z][a-z]+ [A-Z][a-z]+</code>. Given the diversity in naming conventions across cultures and countries, variations such as middle names, suffixes, prefixes, or language-specific differences are tolerated to a certain extent. The validation considers an acceptable error threshold before marking it as failed.</p>"},{"location":"docs/guide/scenario/data-validation/#validation-criteria","title":"Validation Criteria","text":"<ul> <li>Field to Validate: <code>customer_details.name</code></li> <li>Regex Pattern: <code>[A-Z][a-z]+ [A-Z][a-z]+</code></li> <li>Error Tolerance: If more than 10% do not match the regex, then fail.</li> </ul>"},{"location":"docs/guide/scenario/data-validation/#considerations","title":"Considerations","text":"<ul> <li>Customisation<ul> <li>Adjust the regex pattern and error threshold based on your specific data schema and validation requirements.</li> <li>For the full list of types of basic validations that can be   used, check this page.</li> </ul> </li> <li>Understanding Tolerance<ul> <li>Be mindful of the error threshold, as it directly influences what percentage of deviations from the pattern is   acceptable.</li> </ul> </li> </ul> JavaScala <pre><code>validation().field(\"customer_details.name\")\n    .matches(\"[A-Z][a-z]+ [A-Z][a-z]+\")\n    .errorThreshold(0.1)                                      //&lt;=10% failure rate is acceptable\n    .description(\"Names generally follow the same pattern\"),  //description to add context in report or other developers\n</code></pre> <pre><code>validation.field(\"customer_details.name\")\n  .matches(\"[A-Z][a-z]+ [A-Z][a-z]+\")\n  .errorThreshold(0.1)                                      //&lt;=10% failure rate is acceptable\n  .description(\"Names generally follow the same pattern\"),  //description to add context in report or other developers\n</code></pre>"},{"location":"docs/guide/scenario/data-validation/#custom-validation","title":"Custom Validation","text":"<p>There will be situation where you have a complex data setup and require you own custom logic to use for data validation. You can achieve this via setting your own SQL expression that returns a boolean value. An example is seen below where we want to check the array <code>update_history</code>, that each entry has <code>updated_time</code> greater than a certain timestamp.</p> JavaScala <pre><code>validation().expr(\"FORALL(update_history, x -&gt; x.updated_time &gt; TIMESTAMP('2022-01-01 00:00:00'))\"),\n</code></pre> <pre><code>validation.expr(\"FORALL(update_history, x -&gt; x.updated_time &gt; TIMESTAMP('2022-01-01 00:00:00'))\"),\n</code></pre> <p>If you want to know what other SQL function are available for you to use, check this page.</p>"},{"location":"docs/guide/scenario/data-validation/#group-by-validation","title":"Group By Validation","text":"<p>There are scenarios where you want to validate against grouped values or the whole dataset via aggregations. An example would be validating that each customer's transactions sum is greater than 0.</p>"},{"location":"docs/guide/scenario/data-validation/#validation-criteria_1","title":"Validation Criteria","text":"<p>Line 1: <code>validation.groupBy().count().isEqual(100)</code></p> <ul> <li>Method Chaining<ul> <li><code>groupBy()</code>: Group by whole dataset.</li> <li><code>count()</code>: Counts the number of dataset elements.</li> <li><code>isEqual(100)</code>: Checks if the count is equal to 100.</li> </ul> </li> <li>Validation Rule<ul> <li>This line ensures that the count of the total dataset is exactly 100.</li> </ul> </li> </ul> <p>Line 2: <code>validation.groupBy(\"account_id\").max(\"balance\").lessThan(900)</code></p> <ul> <li>Method Chaining<ul> <li><code>groupBy(\"account_id\")</code>: Groups the data based on the <code>account_id</code> field.</li> <li><code>max(\"balance\")</code>: Calculates the maximum value of the <code>balance</code> field within each group.</li> <li><code>lessThan(900)</code>: Checks if the maximum balance in each group is less than 900.</li> </ul> </li> <li>Validation Rule<ul> <li>This line ensures that, for each group identified by <code>account_id</code> the maximum balance is less than 900.</li> </ul> </li> </ul>"},{"location":"docs/guide/scenario/data-validation/#considerations_1","title":"Considerations","text":"<ul> <li>Adjust the <code>errorThreshold</code> or validation to your specification scenario. The full list   of types of validations can be found here.</li> <li>For the full list of types of group by validations that can be   used, check this page.</li> </ul> JavaScala <pre><code>validation().groupBy().count().isEqual(100),\nvalidation().groupBy(\"account_id\").max(\"balance\").lessThan(900)\n</code></pre> <pre><code>validation.groupBy().count().isEqual(100),\nvalidation.groupBy(\"account_id\").max(\"balance\").lessThan(900)\n</code></pre>"},{"location":"docs/guide/scenario/data-validation/#sample-validation","title":"Sample Validation","text":"<p>To try cover the majority of validation cases, the below has been created.</p> JavaScala <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n        .validations(\n                validation().field(\"customer_details.name\").matches(\"[A-Z][a-z]+ [A-Z][a-z]+\").errorThreshold(0.1).description(\"Names generally follow the same pattern\"),\n                validation().field(\"date\").isNull(true).errorThreshold(10),\n                validation().field(\"balance\").greaterThan(500),\n                validation().expr(\"YEAR(date) == year\"),\n                validation().field(\"status\").in(\"open\", \"closed\", \"pending\").errorThreshold(0.2).description(\"Could be new status introduced\"),\n                validation().field(\"customer_details.age\").greaterThan(18),\n                validation().expr(\"FORALL(update_history, x -&gt; x.updated_time &gt; TIMESTAMP('2022-01-01 00:00:00'))\"),\n                validation().field(\"update_history\").greaterThanSize(2),\n                validation().unique(\"account_id\"),\n                validation().groupBy().count().isEqual(1000),\n                validation().groupBy(\"account_id\").max(\"balance\").lessThan(900)\n        );\n\nvar config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableValidation(true)\n        .enableGenerateData(false);\n\nexecute(config, jsonTask);\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n  .validations(\n    validation.field(\"customer_details.name\").matches(\"[A-Z][a-z]+ [A-Z][a-z]+\").errorThreshold(0.1).description(\"Names generally follow the same pattern\"),\n    validation.field(\"date\").isNull(true).errorThreshold(10),\n    validation.field(\"balance\").greaterThan(500),\n    validation.expr(\"YEAR(date) == year\"),\n    validation.field(\"status\").in(\"open\", \"closed\", \"pending\").errorThreshold(0.2).description(\"Could be new status introduced\"),\n    validation.field(\"customer_details.age\").greaterThan(18),\n    validation.expr(\"FORALL(update_history, x -&gt; x.updated_time &gt; TIMESTAMP('2022-01-01 00:00:00'))\"),\n    validation.field(\"update_history\").greaterThanSize(2),\n    validation.unique(\"account_id\"),\n    validation.groupBy().count().isEqual(1000),\n    validation.groupBy(\"account_id\").max(\"balance\").lessThan(900)\n  )\n\nval config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableValidation(true)\n  .enableGenerateData(false)\n\nexecute(config, jsonTask)\n</code></pre>"},{"location":"docs/guide/scenario/data-validation/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>./run.sh\n#input class MyValidationJavaPlan or MyValidationPlan\n#after completing, check report at docker/sample/report/index.html\n</code></pre> <p>It should look something like this.</p> <p>Check the full example at <code>ValidationPlanRun</code> inside the examples repo.</p>"},{"location":"docs/guide/scenario/delete-generated-data/","title":"Delete Generated Data","text":"<p>Creating a data generator for Postgres and delete the generated data after using it.</p>"},{"location":"docs/guide/scenario/delete-generated-data/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/delete-generated-data/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/delete-generated-data/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyAdvancedDeleteJavaPlanRun.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyAdvancedDeletePlanRun.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyAdvancedDeleteJavaPlanRun extends PlanRun {\n    {\n        var autoRun = configuration()\n                .postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n                .enableGeneratePlanAndTasks(true)                                                 (2)\n                .enableRecordTracking(true)                                                       (3)\n                .enableDeleteGeneratedRecords(false)                                              (4)\n                .enableUniqueCheck(true)\n                .generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (5)\n                .recordTrackingFolderPath(\"/opt/app/data/recordTracking\")                         (6)\n                .generatedReportsFolderPath(\"/opt/app/data/report\");\n\n        execute(autoRun);\n   }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyAdvancedDeletePlanRun extends PlanRun {\n\n  val autoRun = configuration\n    .postgres(\"my_postgres\", \"jdbc:postgresql://host.docker.internal:5432/customer\")  (1)\n    .enableGeneratePlanAndTasks(true)                                                 (2)\n    .enableRecordTracking(true)                                                       (3)\n    .enableDeleteGeneratedRecords(false)                                              (4)\n    .enableUniqueCheck(true)\n    .generatedPlanAndTaskFolderPath(\"/opt/app/data/generated\")                        (5)\n    .recordTrackingFolderPath(\"/opt/app/data/recordTracking\")                         (6)\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n\n  execute(configuration = autoRun)\n}\n</code></pre> <p>In the above code we note the following:</p> <ol> <li>We have defined a Postgres connection called <code>my_postgres</code></li> <li><code>enableGeneratePlanAndTasks</code> is enabled to auto generate data for all tables under <code>customer</code> database</li> <li><code>enableRecordTracking</code> is enabled to ensure that all generated records are tracked. This will get used when we want    to delete data afterwards</li> <li><code>enableDeleteGeneratedRecords</code> is disabled for now. We want to see the generated data first and delete sometime after</li> <li><code>generatedPlanAndTaskFolderPath</code> is the folder path where we saved the metadata we have gathered from <code>my_postgres</code></li> <li><code>recordTrackingFolderPath</code> is the folder path where record tracking is maintained. We need to persist this data to    ensure it is still available when we want to delete data</li> </ol>"},{"location":"docs/guide/scenario/delete-generated-data/#postgres-setup","title":"Postgres Setup","text":"<p>If you don't have your own Postgres up and running, you can set up and run an instance configured in the <code>docker</code> folder via.</p> <pre><code>cd docker\ndocker-compose up -d postgres\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c '\\dt+ account.*'\n</code></pre> <p>This will create the tables found under <code>docker/data/sql/postgres/customer.sql</code>. You can change this file to contain your own tables. We can see there are 4 tables created for us, <code>accounts, balances, transactions and mapping</code>.</p>"},{"location":"docs/guide/scenario/delete-generated-data/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>cd ..\n./run.sh\n#input class MyAdvancedDeleteJavaPlanRun or MyAdvancedDeletePlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1'\n</code></pre> <p>It should look something like this.</p> <pre><code>   id   | account_number  | account_status | created_by | created_by_fixed_length | customer_id_int | customer_id_smallint | customer_id_bigint |   customer_id_decimal    | customer_id_real | customer_id_double | open_date  |     open_timestamp      | last_opened_time |                                                           payload_bytes\n--------+-----------------+----------------+------------+-------------------------+-----------------+----------------------+--------------------+--------------------------+------------------+--------------------+------------+-------------------------+------------------+------------------------------------------------------------------------------------------------------------------------------------\n 100414 | 5uROOVOUyQUbubN | h3H            | SfA0eZJcTm | CuRw                    |              13 |                   42 |               6041 | 76987.745612542900000000 |         91866.78 |  66400.37433202339 | 2023-03-05 | 2023-08-14 11:33:11.343 | 23:58:01.736     | \\x604d315d4547616e6a233050415373317274736f5e682d516132524f3d23233c37463463322f342d34376d597e665d6b3d395b4238284028622b7d6d2b4f5042\n(1 row)\n</code></pre> <p>The data that gets inserted will follow the foreign keys that are defined within Postgres and also ensure the insertion order is correct.</p> <p>Check the number of records via:</p> <pre><code>docker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select count(1) from account.accounts'\n#open report under docker/sample/report/index.html\n</code></pre>"},{"location":"docs/guide/scenario/delete-generated-data/#delete","title":"Delete","text":"<p>We are now at a stage where we want to delete the data that was generated. All we need to do is flip two flags.</p> <pre><code>.enableDeleteGeneratedRecords(true)\n.enableGenerateData(false)  //we need to explicitly disable generating data\n</code></pre> <p>Enable delete generated records and disable generating data. </p> <p>Before we run again, let us insert a record manually to see if that data will survive after running the job to delete the generated data.</p> <pre><code>docker exec docker-postgresserver-1 psql -Upostgres -d customer -c \"insert into account.accounts (account_number) values ('my_account_number')\"\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c \"select count(1) from account.accounts\"\n</code></pre> <p>We now should have 1001 records in our <code>account.accounts</code> table. Let's delete the generated data now.</p> <pre><code>./run.sh\n#input class MyAdvancedDeleteJavaPlanRun or MyAdvancedDeletePlanRun\n#after completing\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select * from account.accounts limit 1'\ndocker exec docker-postgresserver-1 psql -Upostgres -d customer -c 'select count(1) from account.accounts'\n</code></pre> <p>You should see that only 1 record is left, the one that we manually inserted. Great, now we can generate data reliably  and also be able to clean it up.</p>"},{"location":"docs/guide/scenario/delete-generated-data/#additional-topics","title":"Additional Topics","text":""},{"location":"docs/guide/scenario/delete-generated-data/#one-class-for-generating-another-for-deleting","title":"One class for generating, another for deleting?","text":"<p>Yes, this is possible. There are two requirements: - the connection names used need to be the same across both classes - <code>recordTrackingFolderPath</code> needs to be set to the same value</p>"},{"location":"docs/guide/scenario/delete-generated-data/#define-record-count","title":"Define record count","text":"<p>You can control the record count per sub data source via <code>numRecordsPerStep</code>.</p> JavaScala <pre><code>var autoRun = configuration()\n      ...\n      .numRecordsPerStep(100)\n\nexecute(autoRun)\n</code></pre> <pre><code>val autoRun = configuration\n  ...\n  .numRecordsPerStep(100)\n\nexecute(configuration = autoRun)\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/","title":"First Data Generation","text":"<p>Creating a data generator for a CSV file.</p> <p></p>"},{"location":"docs/guide/scenario/first-data-generation/#requirements","title":"Requirements","text":"<ul> <li>20 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/first-data-generation/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/first-data-generation/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyCsvPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyCsvPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n\npublic class MyCsvJavaPlan extends PlanRun {\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n\nclass MyCsvPlan extends PlanRun {\n}\n</code></pre> <p>This class defines where we need to define all of our configurations for generating data. There are helper variables and methods defined to make it simple and easy to use.</p>"},{"location":"docs/guide/scenario/first-data-generation/#connection-configuration","title":"Connection Configuration","text":"<p>When dealing with CSV files, we need to define a path for our generated CSV files to be saved at, along with any other high level configurations.</p> JavaScala <pre><code>csv(\n  \"customer_accounts\",              //name\n  \"/opt/app/data/customer/account\", //path\n  Map.of(\"header\", \"true\")          //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p> <pre><code>csv(\n  \"customer_accounts\",              //name\n  \"/opt/app/data/customer/account\", //path\n  Map(\"header\" -&gt; \"true\")           //optional additional options\n)\n</code></pre> <p>Other additional options for CSV can be found here</p>"},{"location":"docs/guide/scenario/first-data-generation/#schema","title":"Schema","text":"<p>Our CSV file that we generate should adhere to a defined schema where we can also define data types.</p> <p>Let's define each field along with their corresponding data type. You will notice that the <code>string</code> fields do not have a data type defined. This is because the default data type is <code>StringType</code>.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\"),\n                field().name(\"balance\").type(DoubleType.instance()),\n                field().name(\"created_by\"),\n                field().name(\"name\"),\n                field().name(\"open_time\").type(TimestampType.instance()),\n                field().name(\"status\")\n        );\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"balance\").`type`(DoubleType),\n    field.name(\"created_by\"),\n    field.name(\"name\"),\n    field.name(\"open_time\").`type`(TimestampType),\n    field.name(\"status\")\n  )\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#field-metadata","title":"Field Metadata","text":"<p>We could stop here and generate random data for the accounts table. But wouldn't it be more useful if we produced data that is closer to the structure of the data that would come in production? We can do this by defining various metadata attributes that add guidelines that the data generator will understand when generating data.</p>"},{"location":"docs/guide/scenario/first-data-generation/#account_id","title":"account_id","text":"<ul> <li><code>account_id</code> follows a particular pattern that where it starts with <code>ACC</code> and has 8 digits after it.   This can be defined via a regex like below. Alongside, we also mention that values are unique ensure that   unique values are generated.</li> </ul> JavaScala <pre><code>field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre> <pre><code>field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#balance","title":"balance","text":"<ul> <li><code>balance</code> let's make the numbers not too large, so we can define a min and max for the generated numbers to be between   <code>1</code> and <code>1000</code>.</li> </ul> JavaScala <pre><code>field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n</code></pre> <pre><code>field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#name","title":"name","text":"<ul> <li><code>name</code> is a string that also follows a certain pattern, so we could also define a regex but here we will choose to   leverage the DataFaker library and create an <code>expression</code> to generate real looking name. All possible faker   expressions   can be found here</li> </ul> JavaScala <pre><code>field().name(\"name\").expression(\"#{Name.name}\"),\n</code></pre> <pre><code>field.name(\"name\").expression(\"#{Name.name}\"),\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#open_time","title":"open_time","text":"<ul> <li><code>open_time</code> is a timestamp that we want to have a value greater than a specific date. We can define a min date by   using   <code>java.sql.Date</code> like below.</li> </ul> JavaScala <pre><code>field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre> <pre><code>field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#status","title":"status","text":"<ul> <li><code>status</code> is a field that can only obtain one of four values, <code>open, closed, suspended or pending</code>.</li> </ul> JavaScala <pre><code>field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre> <pre><code>field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#created_by","title":"created_by","text":"<ul> <li><code>created_by</code> is a field that is based on the <code>status</code> field where it follows the   logic: <code>if status is open or closed, then   it is created_by eod else created_by event</code>. This can be achieved by defining a SQL expression like below.</li> </ul> JavaScala <pre><code>field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <pre><code>field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n</code></pre> <p>Putting it all the fields together, our class should now look like this.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                field().name(\"name\").expression(\"#{Name.name}\"),\n                field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n        );\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n    field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n    field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n    field.name(\"name\").expression(\"#{Name.name}\"),\n    field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n    field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n  )\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#record-count","title":"Record Count","text":"<p>We only want to generate 100 records, so that we can see what the output looks like. This is controlled at the <code>accountTask</code> level like below. If you want to generate more records, set it to the value you want.</p> JavaScala <pre><code>var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().records(100));\n</code></pre> <pre><code>val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.records(100))\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#additional-configurations","title":"Additional Configurations","text":"<p>At the end of data generation, a report gets generated that summarises the actions it performed. We can control the output folder of that report via configurations. We will also enable the unique check to ensure any unique fields will have unique values generated.</p> JavaScala <pre><code>var config = configuration()\n        .generatedReportsFolderPath(\"/opt/app/data/report\")\n        .enableUniqueCheck(true);\n</code></pre> <pre><code>val config = configuration\n  .generatedReportsFolderPath(\"/opt/app/data/report\")\n  .enableUniqueCheck(true)\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#execute","title":"Execute","text":"<p>To tell Data Caterer that we want to run with the configurations along with the <code>accountTask</code>, we have to call <code>execute</code> . So our full plan run will look like this.</p> JavaScala <pre><code>public class MyCsvJavaPlan extends PlanRun {\n    {\n        var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                        field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"name\").expression(\"#{Name.name}\"),\n                        field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, accountTask);\n    }\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\n  val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n      field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n    val config = configuration\n      .generatedReportsFolderPath(\"/opt/app/data/report\")\n      .enableUniqueCheck(true)\n\n    execute(config, accountTask)\n}\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#run","title":"Run","text":"<p>Now we can run via the script <code>./run.sh</code> that is in the top level directory of the <code>data-caterer-example</code> to run the class we just created.</p> <pre><code>./run.sh\n#input class MyCsvJavaPlan or MyCsvPlan\n#after completing\nhead docker/sample/customer/account/part-00000*\n</code></pre> <p>Your output should look like this.</p> <pre><code>account_id,balance,created_by,name,open_time,status\nACC06192462,853.9843359645766,eod,Hoyt Kertzmann MD,2023-07-22T11:17:01.713Z,closed\nACC15350419,632.5969895326234,eod,Dr. Claude White,2022-12-13T21:57:56.840Z,open\nACC25134369,592.0958847218986,eod,Fabian Rolfson,2023-04-26T04:54:41.068Z,open\nACC48021786,656.6413439322964,eod,Dewayne Stroman,2023-05-17T06:31:27.603Z,open\nACC26705211,447.2850352884595,event,Garrett Funk,2023-07-14T03:50:22.746Z,pending\nACC03150585,750.4568929015996,event,Natisha Reichel,2023-04-11T11:13:10.080Z,suspended\nACC29834210,686.4257811608622,event,Gisele Ondricka,2022-11-15T22:09:41.172Z,suspended\nACC39373863,583.5110618128994,event,Thaddeus Ortiz,2022-09-30T06:33:57.193Z,suspended\nACC39405798,989.2623959059525,eod,Shelby Reinger,2022-10-23T17:29:17.564Z,open\n</code></pre> <p>Also check the HTML report, found at <code>docker/sample/report/index.html</code>, that gets generated to get an overview of what was executed.</p> <p></p>"},{"location":"docs/guide/scenario/first-data-generation/#join-with-another-csv","title":"Join With Another CSV","text":"<p>Now that we have generated some accounts, let's also try to generate a set of transactions for those accounts in CSV format as well. The transactions could be in any other format, but to keep this simple, we will continue using CSV.</p> <p>We can define our schema the same way along with any additional metadata.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                field().name(\"account_id\"),\n                field().name(\"name\"),\n                field().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\n                field().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                field().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n        );\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"full_name\"),\n    field.name(\"amount\").`type`(DoubleType).min(1).max(100),\n    field.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n    field.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n  )\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#records-per-field","title":"Records Per Field","text":"<p>Usually, for a given <code>account_id, full_name</code>, there should be multiple records for it as we want to simulate a customer having multiple transactions. We can achieve this through defining the number of records to generate in the <code>count</code> function.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().recordsPerField(5, \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.recordsPerField(5, \"account_id\", \"full_name\"))\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#random-records-per-field","title":"Random Records Per Field","text":"<p>Above, you will notice that we are generating 5 records per <code>account_id, full_name</code>. This is okay but still not quite reflective of the real world. Sometimes, people have accounts with no transactions in them, or they could have many. We can accommodate for this via defining a random number of records per field.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().recordsPerFieldGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.recordsPerFieldGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n</code></pre> <p>Here we set the minimum number of records per field to be 0 and the maximum to 5.</p>"},{"location":"docs/guide/scenario/first-data-generation/#foreign-key","title":"Foreign Key","text":"<p>In this scenario, we want to match the <code>account_id</code> in <code>account</code> to match the same field values in <code>transaction</code>. We also want to match <code>name</code> in <code>account</code> to <code>full_name</code> in <code>transaction</code>. This can be done via plan configuration like below.</p> JavaScala <pre><code>var myPlan = plan().addForeignKeyRelationship(\n        accountTask, List.of(\"account_id\", \"name\"), //the task and fields we want linked\n        List.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\"))) //list of other tasks and their respective field names we want matched\n);\n</code></pre> <pre><code>val myPlan = plan.addForeignKeyRelationship(\n  accountTask, List(\"account_id\", \"name\"),  //the task and fields we want linked\n  List(transactionTask -&gt; List(\"account_id\", \"full_name\"))  //list of other tasks and their respective field names we want matched\n)\n</code></pre> <p>Now, stitching it all together for the <code>execute</code> function, our final plan should look like this.</p> JavaScala <pre><code>public class MyCsvJavaPlan extends PlanRun {\n    {\n        var accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n                        field().name(\"balance\").type(DoubleType.instance()).min(1).max(1000),\n                        field().name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n                        field().name(\"name\").expression(\"#{Name.name}\"),\n                        field().name(\"open_time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n                )\n                .count(count().records(100));\n\n        var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\"),\n                        field().name(\"name\"),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\n                        field().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n                )\n                .count(count().recordsPerFieldGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        var myPlan = plan().addForeignKeyRelationship(\n                accountTask, List.of(\"account_id\", \"name\"),\n                List.of(Map.entry(transactionTask, List.of(\"account_id\", \"full_name\")))\n        );\n\n        execute(myPlan, config, accountTask, transactionTask);\n    }\n}\n</code></pre> <pre><code>class MyCsvPlan extends PlanRun {\n\n  val accountTask = csv(\"customer_accounts\", \"/opt/app/data/customer/account\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\").unique(true),\n      field.name(\"balance\").`type`(DoubleType).min(1).max(1000),\n      field.name(\"created_by\").sql(\"CASE WHEN status IN ('open', 'closed') THEN 'eod' ELSE 'event' END\"),\n      field.name(\"name\").expression(\"#{Name.name}\"),\n      field.name(\"open_time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"status\").oneOf(\"open\", \"closed\", \"suspended\", \"pending\")\n    )\n    .count(count.records(100))\n\n  val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\"),\n      field.name(\"name\"),\n      field.name(\"amount\").`type`(DoubleType).min(1).max(100),\n      field.name(\"time\").`type`(TimestampType).min(java.sql.Date.valueOf(\"2022-01-01\")),\n      field.name(\"date\").`type`(DateType).sql(\"DATE(time)\")\n    )\n    .count(count.recordsPerFieldGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n    .enableUniqueCheck(true)\n\n  val myPlan = plan.addForeignKeyRelationship(\n    accountTask, List(\"account_id\", \"name\"),\n    List(transactionTask -&gt; List(\"account_id\", \"full_name\"))\n  )\n\n  execute(myPlan, config, accountTask, transactionTask)\n}\n</code></pre> <p>Let's try run again.</p> <pre><code>#clean up old data\nrm -rf docker/sample/customer/account\n./run.sh\n#input class MyCsvJavaPlan or MyCsvPlan\n#after completing, let's pick an account and check the transactions for that account\naccount=$(tail -1 docker/sample/customer/account/part-00000* | awk -F \",\" '{print $1 \",\" $4}')\necho $account\ncat docker/sample/customer/transaction/part-00000* | grep $account\n</code></pre> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>Congratulations! You have now made a data generator that has simulated a real world data scenario. You can check the <code>DocumentationJavaPlanRun.java</code> or <code>DocumentationPlanRun.scala</code> files as well to check that your plan is the same.</p> <p>We can now look to consume this CSV data from a job or service. Usually, once we have consumed the data, we would also want to check and validate that our consumer has correctly ingested the data.</p>"},{"location":"docs/guide/scenario/first-data-generation/#validate","title":"Validate","text":"<p>In this scenario, our consumer will read in the CSV file, do some transformations, and then save the data to Postgres. Let's try to configure data validations for the data that gets pushed into Postgres.</p>"},{"location":"docs/guide/scenario/first-data-generation/#postgres-setup","title":"Postgres setup","text":"<p>First, we define our connection properties for Postgres. You can check out the full options available here.</p> JavaScala <pre><code>var postgresValidateTask = postgres(\n    \"my_postgres\",                                          //connection name\n    \"jdbc:postgresql://host.docker.internal:5432/customer\", //url\n    \"postgres\",                                             //username\n    \"password\"                                              //password\n).table(\"account\", \"transactions\");\n</code></pre> <pre><code>val postgresValidateTask = postgres(\n  \"my_postgres\",                                          //connection name\n  \"jdbc:postgresql://host.docker.internal:5432/customer\", //url\n  \"postgres\",                                             //username\n  \"password\"                                              //password\n).table(\"account\", \"transactions\")\n</code></pre> <p>We can connect and access the data inside the table <code>account.transactions</code>. Now to define our data validations.</p>"},{"location":"docs/guide/scenario/first-data-generation/#validations","title":"Validations","text":"<p>For full information about validation options and configurations, check here. Below, we have an example that should give you a good understanding of what validations are possible.</p> JavaScala <pre><code>var postgresValidateTask = postgres(...)\n        .table(\"account\", \"transactions\")\n        .validations(\n                validation().field(\"account_id\").isNull(true),\n                validation().field(\"name\").matches(\"[A-Z][a-z]+ [A-Z][a-z]+\").errorThreshold(0.2).description(\"Some names have different formats\"),\n                validation().field(\"balance\").greaterThanOrEqual(0).errorThreshold(10).description(\"Account can have negative balance if overdraft\"),\n                validation().expr(\"CASE WHEN status == 'closed' THEN isNotNull(close_date) ELSE isNull(close_date) END\"),\n                validation().unique(\"account_id\", \"name\"),\n                validation().groupBy(\"account_id\", \"name\").max(\"login_retry\").lessThan(10)\n        );\n</code></pre> <pre><code>val postgresValidateTask = postgres(...)\n  .table(\"account\", \"transactions\")\n  .validations(\n    validation.field(\"account_id\").isNull(true),\n    validation.field(\"name\").matches(\"[A-Z][a-z]+ [A-Z][a-z]+\").errorThreshold(0.2).description(\"Some names have different formats\"),\n    validation.field(\"balance\").greaterThanOrEqual(0).errorThreshold(10).description(\"Account can have negative balance if overdraft\"),\n    validation.expr(\"CASE WHEN status == 'closed' THEN isNotNull(close_date) ELSE isNull(close_date) END\"),\n    validation.unique(\"account_id\", \"name\"),\n    validation.groupBy(\"account_id\", \"name\").max(\"login_retry\").lessThan(10)\n  )\n</code></pre>"},{"location":"docs/guide/scenario/first-data-generation/#name_1","title":"name","text":"<p>For all values in the <code>name</code> field, we check if they match the regex <code>[A-Z][a-z]+ [A-Z][a-z]+</code>. As we know in the real world, names do not always follow the same pattern, so we allow for an <code>errorThreshold</code> before marking the validation as failed. Here, we define the <code>errorThreshold</code> to be <code>0.2</code>, which means, if the error percentage is greater than 20%, then fail the validation. We also append on a helpful description so other developers/users can understand the context of the validation.</p>"},{"location":"docs/guide/scenario/first-data-generation/#balance_1","title":"balance","text":"<p>We check that all <code>balance</code> values are greater than or equal to 0. This time, we have a slightly different <code>errorThreshold</code> as it is set to <code>10</code>, which means, if the number of errors is greater than 10, then fail the validation.</p>"},{"location":"docs/guide/scenario/first-data-generation/#expr","title":"expr","text":"<p>Sometimes, we may need to include the values of multiple fields to validate a certain condition. This is where we can use <code>expr</code> to define a SQL expression that returns a boolean. In this scenario, we are checking if the <code>status</code> field has value <code>closed</code>, then the <code>close_date</code> should be not null, otherwise, <code>close_date</code> is null.</p>"},{"location":"docs/guide/scenario/first-data-generation/#unique","title":"unique","text":"<p>We check whether the combination of <code>account_id</code> and <code>name</code> are unique within the dataset. You can define one or more fields for <code>unique</code> validations.</p>"},{"location":"docs/guide/scenario/first-data-generation/#groupby","title":"groupBy","text":"<p>There may be some business rule that states the number of <code>login_retry</code> should be less than 10 for each account. We can check this via a group by validation where we group by the <code>account_id, name</code>, take the maximum value for <code>login_retry</code> per <code>account_id,name</code> combination, then check if it is less than 10.</p> <p>You can now look to play around with other configurations or data sources to meet your needs. Also, make sure to explore the docs further as it can guide you on what can be configured.</p>"},{"location":"docs/guide/scenario/records-per-field/","title":"Multiple Records Per Field","text":"<p>Creating a data generator for a CSV file where there are multiple records per field values.</p>"},{"location":"docs/guide/scenario/records-per-field/#requirements","title":"Requirements","text":"<ul> <li>5 minutes</li> <li>Git</li> <li>Gradle</li> <li>Docker</li> </ul>"},{"location":"docs/guide/scenario/records-per-field/#get-started","title":"Get Started","text":"<p>First, we will clone the data-caterer-example repo which will already have the base project setup required.</p> JavaScalaYAMLUI <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <pre><code>git clone git@github.com:data-catering/data-caterer-example.git\n</code></pre> <p>Run Data Caterer UI via the 'Quick Start' found here.</p>"},{"location":"docs/guide/scenario/records-per-field/#plan-setup","title":"Plan Setup","text":"<p>Create a new Java or Scala class.</p> <ul> <li>Java: <code>src/main/java/io/github/datacatering/plan/MyMultipleRecordsPerColJavaPlan.java</code></li> <li>Scala: <code>src/main/scala/io/github/datacatering/plan/MyMultipleRecordsPerColPlan.scala</code></li> </ul> <p>Make sure your class extends <code>PlanRun</code>.</p> JavaScala <pre><code>import io.github.datacatering.datacaterer.java.api.PlanRun;\n...\n\npublic class MyMultipleRecordsPerColJavaPlan extends PlanRun {\n    {\n        var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n                .fields(\n                        field().name(\"account_id\"),\n                        field().name(\"full_name\"),\n                        field().name(\"amount\").type(DoubleType.instance()).min(1).max(100),\n                        field().name(\"time\").type(TimestampType.instance()).min(java.sql.Date.valueOf(\"2022-01-01\")),\n                        field().name(\"date\").type(DateType.instance()).sql(\"DATE(time)\")\n                );\n\n        var config = configuration()\n                .generatedReportsFolderPath(\"/opt/app/data/report\")\n                .enableUniqueCheck(true);\n\n        execute(config, transactionTask);\n    }\n}\n</code></pre> <pre><code>import io.github.datacatering.datacaterer.api.PlanRun\n...\n\nclass MyMultipleRecordsPerColPlan extends PlanRun {\n\n  val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n    .fields(\n      field.name(\"account_id\").regex(\"ACC[0-9]{8}\"), \n      field.name(\"full_name\").expression(\"#{Name.name}\"), \n      field.name(\"amount\").`type`(DoubleType.instance).min(1).max(100),\n      field.name(\"time\").`type`(TimestampType.instance).min(java.sql.Date.valueOf(\"2022-01-01\")), \n      field.name(\"date\").`type`(DateType.instance).sql(\"DATE(time)\")\n    )\n\n  val config = configuration\n    .generatedReportsFolderPath(\"/opt/app/data/report\")\n\n  execute(config, transactionTask)\n}\n</code></pre>"},{"location":"docs/guide/scenario/records-per-field/#record-count","title":"Record Count","text":"<p>By default, tasks will generate 1000 records. You can alter this value via the <code>count</code> configuration which can be applied to individual tasks. For example, in Scala, <code>csv(...).count(count.records(100))</code> to generate only 100 records.</p>"},{"location":"docs/guide/scenario/records-per-field/#records-per-field","title":"Records Per Field","text":"<p>In this scenario, for a given <code>account_id, full_name</code>, there should be multiple records for it as we want to simulate a customer having multiple transactions. We can achieve this through defining the number of records to generate in the <code>count</code> function.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().recordsPerField(5, \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.recordsPerField(5, \"account_id\", \"full_name\"))\n</code></pre> <p>This will generate <code>1000 * 5 = 5000</code> records as the default number of records is set (1000) and per <code>account_id, full_name</code> from the initial 1000 records, 5 records will be generated.</p>"},{"location":"docs/guide/scenario/records-per-field/#random-records-per-field","title":"Random Records Per Field","text":"<p>Generating 5 records per field is okay but still not quite reflective of the real world. Sometimes, people have accounts with no transactions in them, or they could have many. We can accommodate for this via defining a random number of records per field.</p> JavaScala <pre><code>var transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map.of(\"header\", \"true\"))\n        .fields(\n                ...\n        )\n        .count(count().recordsPerFieldGenerator(generator().min(0).max(5), \"account_id\", \"full_name\"));\n</code></pre> <pre><code>val transactionTask = csv(\"customer_transactions\", \"/opt/app/data/customer/transaction\", Map(\"header\" -&gt; \"true\"))\n  .fields(\n    ...\n  )\n  .count(count.recordsPerFieldGenerator(generator.min(0).max(5), \"account_id\", \"full_name\"))\n</code></pre> <p>Here we set the minimum number of records per field to be 0 and the maximum to 5. This will follow a uniform distribution so the average number of records per account is 2.5. We could also define other metadata, just like we did with fields, when defining the generator. For example, we could set <code>standardDeviation</code> and <code>mean</code> for the number of records generated per field to follow a normal distribution.</p>"},{"location":"docs/guide/scenario/records-per-field/#run","title":"Run","text":"<p>Let's try run.</p> <pre><code>#clean up old data\nrm -rf docker/sample/customer/account\n./run.sh\n#input class MyMultipleRecordsPerColJavaPlan or MyMultipleRecordsPerColPlan\n#after completing\nhead docker/sample/customer/transaction/part-00000*\n</code></pre> <p>It should look something like this.</p> <pre><code>ACC29117767,Willodean Sauer\nACC29117767,Willodean Sauer,84.99145871948083,2023-05-14T09:55:51.439Z,2023-05-14\nACC29117767,Willodean Sauer,58.89345733567232,2022-11-22T07:38:20.143Z,2022-11-22\n</code></pre> <p>You can now look to play around with other count configurations found here.</p>"},{"location":"docs/report/alert/","title":"Alert","text":"<p>Alerts can be configured to help users receive feedback from their data testing results. Currently, Data Caterer  supports Slack for alerts.</p>"},{"location":"docs/report/alert/#slack","title":"Slack","text":"<p>Define a Slack token and one or more Slack channels that will  receive an alert like the below.</p> <p></p> JavaScala <pre><code>var conf = configuration()\n    .slackAlertToken(\"abc123\")                                //use appropriate Slack token (usually bot token)\n    .slackAlertChannels(\"#test-alerts\", \"#pre-prod-testing\"); //define Slack channel(s) to receive alerts on\n\nexecute(conf, ...);\n</code></pre> <pre><code>val conf = configuration\n  .slackAlertToken(\"abc123\")                                //use appropriate Slack token (usually bot token)\n  .slackAlertChannels(\"#test-alerts\", \"#pre-prod-testing\")  //define Slack channel(s) to receive alerts on\n\nexecute(conf, ...)\n</code></pre>"},{"location":"docs/report/html-report/","title":"Report","text":"<p>Data Caterer can be configured to produce a report of the data generated to help users understand what was run, how much  data was generated, where it was generated, validation results and any associated metadata. </p>"},{"location":"docs/report/html-report/#sample","title":"Sample","text":"<p>Once run, it will produce a report like this.</p>"},{"location":"docs/validation/basic-validation/","title":"Basic Validations","text":"<p>Run validations on a field to ensure the values adhere to your requirement. Can be set to complex validation logic via SQL expression as well if needed (see here).</p>"},{"location":"docs/validation/basic-validation/#pre-filter","title":"Pre-filter","text":"<p>If you want to only run the validation on a specific subset of data, you can define pre-filter conditions. Find more  details here.</p>"},{"location":"docs/validation/basic-validation/#equal","title":"Equal","text":"<p>Ensure all data in field is equal/not equal to certain value. Value can be of any data type. Can use <code>isEqualField</code> to  define SQL expression that can reference other fields.</p> JavaScalaYAML <pre><code>validation().field(\"year\").isEqual(2021),\nvalidation().field(\"year\").isEqualField(\"YEAR(date)\"),\n\nvalidation().field(\"year\").isEqual(2021, true), //check not equal to\nvalidation().field(\"year\").isEqualField(\"YEAR(date)\", true),\n</code></pre> <pre><code>validation.field(\"year\").isEqual(2021),\nvalidation.field(\"year\").isEqualField(\"YEAR(date)\"),\n\nvalidation.field(\"year\").isEqual(2021, true),  //check not equal to\nvalidation.field(\"year\").isEqualField(\"YEAR(date)\", true),\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"year\"\n        validation:\n          - type: \"equal\"\n            value: 2021\n          - type: \"equal\"\n            value: 2021\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#null","title":"Null","text":"<p>Ensure all data in field is null or not null.</p> JavaScalaYAML <pre><code>validation().field(\"year\").isNull()\n\nvalidation().field(\"year\").isNull(true) //check not null\n</code></pre> <pre><code>validation.field(\"year\").isNull()\n\nvalidation().field(\"year\").isNull(true) //check not null\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"year\"\n        validation:\n          - type: \"null\"\n          - type: \"null\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#contains","title":"Contains","text":"<p>Ensure all data in field is contains/not contains a certain string. Field has to have type string.</p> JavaScalaYAML <pre><code>validation().field(\"name\").contains(\"peter\")\n\nvalidation().field(\"name\").contains(\"peter\", true)  //check not contains\n</code></pre> <pre><code>validation.field(\"name\").contains(\"peter\")\n\nvalidation.field(\"name\").contains(\"peter\", true)  //check not contains\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"contains\"\n            value: \"peter\"\n          - type: \"contains\"\n            value: \"peter\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#unique","title":"Unique","text":"<p>Ensure all data in field is unique.</p> JavaScalaYAML <pre><code>validation().unique(\"account_id\", \"name\")\n</code></pre> <pre><code>validation.unique(\"account_id\", \"name\")\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - unique: [\"account_id\", \"name\"]\n</code></pre>"},{"location":"docs/validation/basic-validation/#less-than","title":"Less Than","text":"<p>Ensure all data in field is less than certain value. Can use <code>lessThanField</code> to define SQL expression that can reference  other fields.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").lessThan(100),\nvalidation().field(\"amount\").lessThanField(\"balance + 1\"),\n\nvalidation().field(\"amount\").lessThan(100, false), //check less than or equal to\nvalidation().field(\"amount\").lessThanField(\"balance + 1\", false),\n</code></pre> <pre><code>validation.field(\"amount\").lessThan(100),\nvalidation.field(\"amount\").lessThanField(\"balance + 1\"),\n\nvalidation.field(\"amount\").lessThan(100, false), //check less than or equal to\nvalidation.field(\"amount\").lessThanField(\"balance + 1\", false),\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"amount &lt; 100\"\n      - expr: \"amount &lt; balance + 1\"\n      - field: \"amount\"\n        validation:\n          - type: \"lessThan\"\n            value: 100\n          - type: \"lessThan\"\n            value: 100\n            strictly: false\n</code></pre>"},{"location":"docs/validation/basic-validation/#greater-than","title":"Greater Than","text":"<p>Ensure all data in field is greater than certain value. Can use <code>greaterThanField</code> to define SQL expression that can reference other fields.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").greaterThan(100),\nvalidation().field(\"amount\").greaterThanField(\"balance\"),\n\nvalidation().field(\"amount\").greaterThan(100, false), //check greater than or equal to\nvalidation().field(\"amount\").greaterThanField(\"balance\", false),\n</code></pre> <pre><code>validation.field(\"amount\").greaterThan(100),\nvalidation.field(\"amount\").greaterThanField(\"balance\"),\n\nvalidation.field(\"amount\").greaterThan(100), //check greater than or equal to\nvalidation.field(\"amount\").greaterThanField(\"balance\"),\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"amount &gt; 100\"\n      - expr: \"amount &gt; balance\"\n      - field: \"amount\"\n        validation:\n          - type: \"greaterThan\"\n            value: 100\n          - type: \"greaterThan\"\n            value: 100\n            strictly: false\n</code></pre>"},{"location":"docs/validation/basic-validation/#between","title":"Between","text":"<p>Ensure all data in field is between two values. Can use <code>betweenFields</code> to define SQL expression that references other  fields.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").between(100, 200),\nvalidation().field(\"amount\").betweenFields(\"balance * 0.9\", \"balance * 1.1\"),\n\nvalidation().field(\"amount\").between(100, 200, true), //check not between\nvalidation().field(\"amount\").betweenFields(\"balance * 0.9\", \"balance * 1.1\", true),\n</code></pre> <pre><code>validation.field(\"amount\").between(100, 200),\nvalidation.field(\"amount\").betweenFields(\"balance * 0.9\", \"balance * 1.1\"),\n\nvalidation.field(\"amount\").between(100, 200, true), //check not between\nvalidation.field(\"amount\").betweenFields(\"balance * 0.9\", \"balance * 1.1\", true),\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"amount BETWEEN 100 AND 200\"\n      - expr: \"amount BETWEEN balance * 0.9 AND balance * 1.1\"\n      - field: \"amount\"\n        validation:\n          - type: \"between\"\n            min: 100\n            max: 200\n          - type: \"between\"\n            min: 100\n            max: 200\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#in","title":"In","text":"<p>Ensure all data in field is in set of defined values.</p> JavaScalaYAML <pre><code>validation().field(\"status\").in(\"open\", \"closed\")\n\nvalidation().field(\"status\").in(List.of(\"open\", \"closed\"), true)  //check not in\n</code></pre> <pre><code>validation.field(\"status\").in(\"open\", \"closed\")\n\nvalidation.field(\"status\").in(List(\"open\", \"closed\"), true) //check not in\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"status IN ('open', 'closed')\"\n      - field: \"status\"\n        validation:\n          - type: \"in\"\n            values: [\"open\", \"closed\"]\n          - type: \"in\"\n            values: [\"open\", \"closed\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#matches","title":"Matches","text":"<p>Ensure all data in field matches certain regex expression(s).</p> JavaScalaYAML <pre><code>validation().field(\"account_id\").matches(\"ACC[0-9]{8}\")\nvalidation().field(\"account_id\").matchesList(List.of(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"))  //check matches all regexes\n\nvalidation().field(\"account_id\").matches(\"ACC[0-9]{8}\", true) //check not matches\nvalidation().field(\"account_id\").matchesList(List.of(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), true, false) //check does not match all regexes\n\nvalidation().field(\"account_id\").matchesList(List.of(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), false, true)  //check matches at least one regex\n\nvalidation().field(\"account_id\").matchesList(List.of(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), false, false)  //check does not match at least one regex\n</code></pre> <pre><code>validation.field(\"account_id\").matches(\"ACC[0-9]{8}\")\nvalidation.field(\"account_id\").matchesList(List(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"))  //check matches all regexes\n\nvalidation.field(\"account_id\").matches(\"ACC[0-9]{8}\", true) //check not matches\nvalidation.field(\"account_id\").matchesList(List(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), true, false)  //check does not match all regexes\n\nvalidation.field(\"account_id\").matchesList(List(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), false, true)  //check matches at least one regex\n\nvalidation.field(\"account_id\").matchesList(List(\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"), false, false)  //check does not match at least one regex\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"REGEXP(account_id, ACC[0-9]{8})\"\n      - field: \"account_id\"\n        validation:\n          - type: \"matches\"\n            regex: \"ACC[0-9]{8}\"\n          - type: \"matches\"\n            regex: \"ACC[0-9]{8}\"\n            negate: true\n          - type: \"matchesList\"\n            regexes: [\"ACC[0-9]{8}\", \"ACC[0-9]{10}\"]\n            matchAll: true\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#starts-with","title":"Starts With","text":"<p>Ensure all data in field starts with certain string. Field has to have type string.</p> JavaScalaYAML <pre><code>validation().field(\"account_id\").startsWith(\"ACC\")\n\nvalidation().field(\"account_id\").startsWith(\"ACC\", true)  //check does not start with\n</code></pre> <pre><code>validation.field(\"account_id\").startsWith(\"ACC\")\n\nvalidation.field(\"account_id\").startsWith(\"ACC\", true)  //check does not start with\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"STARTSWITH(account_id, 'ACC')\"\n      - field: \"account_id\"\n        validation:\n          - type: \"startsWith\"\n            value: \"ACC\"\n          - type: \"startsWith\"\n            value: \"ACC\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#ends-with","title":"Ends With","text":"<p>Ensure all data in field ends with certain string. Field has to have type string.</p> JavaScalaYAML <pre><code>validation().field(\"account_id\").endsWith(\"ACC\")\n\nvalidation().field(\"account_id\").endsWith(\"ACC\", true)  //check does not end with\n</code></pre> <pre><code>validation.field(\"account_id\").endsWith(\"ACC\")\n\nvalidation.field(\"account_id\").endsWith(\"ACC\", true)  //check does not end with\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"ENDWITH(account_id, 'ACC')\"\n      - field: \"account_id\"\n        validation:\n          - type: \"endsWith\"\n            value: \"ACC\"\n          - type: \"endsWith\"\n            value: \"ACC\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#size","title":"Size","text":"<p>Ensure all data in field has certain size. Field has to have type array or map.</p> JavaScalaYAML <pre><code>validation().field(\"transactions\").size(5)\n\nvalidation().field(\"transactions\").size(5, true)  //check does not have size\n</code></pre> <pre><code>validation.field(\"transactions\").size(5)\n\nvalidation.field(\"transactions\").size(5, true)  //check does not have size\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"SIZE(transactions, 5)\"\n      - field: \"transactions\"\n        validation:\n          - type: \"size\"\n            value: 5\n          - type: \"size\"\n            value: 5\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#less-than-size","title":"Less Than Size","text":"<p>Ensure all data in field has size less than certain value. Field has to have type array or map.</p> JavaScalaYAML <pre><code>validation().field(\"transactions\").lessThanSize(5)\n\nvalidation().field(\"transactions\").lessThanSize(5, false) //check for less than or equal to size\n</code></pre> <pre><code>validation.field(\"transactions\").lessThanSize(5)\n\nvalidation.field(\"transactions\").lessThanSize(5, false) //check for less than or equal to size\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"SIZE(transactions) &lt; 5\"\n      - field: \"transactions\"\n        validation:\n          - type: \"lessThanSize\"\n            value: 5\n          - type: \"lessThanSize\"\n            value: 5\n            strictly: false\n</code></pre>"},{"location":"docs/validation/basic-validation/#greater-than-size","title":"Greater Than Size","text":"<p>Ensure all data in field has size greater than certain value. Field has to have type array or map.</p> JavaScalaYAML <pre><code>validation().field(\"transactions\").greaterThanSize(5)\n\nvalidation().field(\"transactions\").greaterThanSize(5, false)  //check for less than or equal to size\n</code></pre> <pre><code>validation.field(\"transactions\").greaterThanSize(5)\n\nvalidation.field(\"transactions\").greaterThanSize(5, false)  //check for less than or equal to size\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"SIZE(transactions) &gt; 5\"\n      - field: \"transactions\"\n        validation:\n          - type: \"greaterThanSize\"\n            value: 5\n          - type: \"greaterThanSize\"\n            value: 5\n            strictly: false\n</code></pre>"},{"location":"docs/validation/basic-validation/#luhn-check","title":"Luhn Check","text":"<p>Ensure all data in field passes Luhn check. Luhn check is used to validate credit card numbers and certain identification numbers (see here for more details).</p> JavaScalaYAML <pre><code>validation().field(\"credit_card\").luhnCheck()\n\nvalidation().field(\"credit_card\").luhnCheck(true) //check does not pass Luhn check\n</code></pre> <pre><code>validation.field(\"credit_card\").luhnCheck()\n\nvalidation.field(\"credit_card\").luhnCheck(true) //check does not pass Luhn check\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"LUHN_CHECK(credit_card)\"\n      - field: \"credit_card\"\n        validation:\n          - type: \"luhnCheck\"\n          - type: \"luhnCheck\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#has-type","title":"Has Type","text":"<p>Ensure all data in field has certain data type.</p> JavaScalaYAML <pre><code>validation().field(\"id\").hasType(\"string\")\nvalidation().field(\"id\").hasTypes(List.of(\"string\", \"double\"))\n\nvalidation().field(\"id\").hasType(\"string\", true)  //check does not have type\nvalidation().field(\"id\").hasTypes(List(\"string\", \"double\"), true)\n</code></pre> <pre><code>validation.field(\"id\").hasType(\"string\")\nvalidation.field(\"id\").hasTypes(List.of(\"string\", \"double\"))\n\nvalidation.field(\"id\").hasType(\"string\", true)  //check does not have type\nvalidation.field(\"id\").hasTypes(List(\"string\", \"double\"), true)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - expr: \"TYPEOF(id) == 'string'\"\n      - field: \"id\"\n        validation:\n          - type: \"hasType\"\n            value: \"string\"\n          - type: \"hasType\"\n            value: \"string\"\n            negate: true\n          - type: \"hasTypes\"\n            values: [\"string\", \"double\"]\n          - type: \"hasType\"\n            values: [\"string\", \"double\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#distinct-values-in-set","title":"Distinct Values In Set","text":"<p>Check if distinct values of field exist in set.</p> JavaScalaYAML <pre><code>validation().field(\"name\").distinctInSet(\"peter\", \"john\")\n\nvalidation().field(\"name\").distinctInSet(List.of(\"peter\", \"john\"), false) //check for distinct values not in set\n</code></pre> <pre><code>validation.field(\"name\").distinctInSet(\"peter\", \"john\")\n\nvalidation.field(\"name\").distinctInSet(List(\"peter\", \"john\"), true) //check for distinct values not in set\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"distinctInSet\"\n            values: [\"peter\", \"john\"]\n          - type: \"distinctInSet\"\n            values: [\"peter\", \"john\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#distinct-values-contains-set","title":"Distinct Values Contains Set","text":"<p>Check if distinct values of field contains set of values.</p> JavaScalaYAML <pre><code>validation().field(\"name\").distinctContainsSet(\"peter\", \"john\")\n\nvalidation().field(\"name\").distinctContainsSet(List.of(\"peter\", \"john\"), false) //check for distinct values not contains set\n</code></pre> <pre><code>validation.field(\"name\").distinctContainsSet(\"peter\", \"john\")\n\nvalidation.field(\"name\").distinctContainsSet(List(\"peter\", \"john\"), true) //check for distinct values not contains set\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"distinctContainsSet\"\n            values: [\"peter\", \"john\"]\n          - type: \"distinctContainsSet\"\n            values: [\"peter\", \"john\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#distinct-values-equal","title":"Distinct Values Equal","text":"<p>Check if distinct values of field equals set of values.</p> JavaScalaYAML <pre><code>validation().field(\"name\").distinctEqual(\"peter\", \"john\")\n\nvalidation().field(\"name\").distinctEqual(List.of(\"peter\", \"john\"), false) //check for distinct values not equals set\n</code></pre> <pre><code>validation.field(\"name\").distinctEqual(\"peter\", \"john\")\n\nvalidation.field(\"name\").distinctEqual(List(\"peter\", \"john\"), true) //check for distinct values not equals set\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"distinctEqual\"\n            values: [\"peter\", \"john\"]\n          - type: \"distinctEqual\"\n            values: [\"peter\", \"john\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#max-mean-median-min-standard-deviation-sum-between","title":"Max, Mean, Median, Min, Standard Deviation, Sum Between","text":"<p>Check if aggregation of values for field is between set of values.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").maxBetween(1, 100)\nvalidation().field(\"amount\").meanBetween(1, 100)\nvalidation().field(\"amount\").medianBetween(1, 100)\nvalidation().field(\"amount\").minBetween(1, 100)\nvalidation().field(\"amount\").stdDevBetween(1, 100)\nvalidation().field(\"amount\").sumBetween(1, 100)\n\nvalidation().field(\"amount\").maxBetween(1, 100, true) //check max amount is not between 1 and 100\nvalidation().field(\"amount\").meanBetween(1, 100, true)\nvalidation().field(\"amount\").medianBetween(1, 100, true)\nvalidation().field(\"amount\").minBetween(1, 100, true)\nvalidation().field(\"amount\").stdDevBetween(1, 100, true)\nvalidation().field(\"amount\").sumBetween(1, 100, true)\n</code></pre> <pre><code>validation.field(\"amount\").maxBetween(1, 100)\nvalidation.field(\"amount\").meanBetween(1, 100)\nvalidation.field(\"amount\").medianBetween(1, 100)\nvalidation.field(\"amount\").minBetween(1, 100)\nvalidation.field(\"amount\").stdDevBetween(1, 100)\nvalidation.field(\"amount\").sumBetween(1, 100)\n\nvalidation.field(\"amount\").maxBetween(1, 100, true) //check max amount is not between 1 and 100\nvalidation.field(\"amount\").meanBetween(1, 100, true)\nvalidation.field(\"amount\").medianBetween(1, 100, true)\nvalidation.field(\"amount\").minBetween(1, 100, true)\nvalidation.field(\"amount\").stdDevBetween(1, 100, true)\nvalidation.field(\"amount\").sumBetween(1, 100, true)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"amount\"\n        validation:\n          - type: \"maxBetween\"\n            min: 1\n            max: 100\n          - type: \"meanBetween\"\n            min: 1\n            max: 100\n          - type: \"medianBetween\"\n            min: 1\n            max: 100\n          - type: \"minBetween\"\n            min: 1\n            max: 100\n          - type: \"stdDevBetween\"\n            min: 1\n            max: 100\n          - type: \"sumBetween\"\n            min: 1\n            max: 100\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#length-equalbetween","title":"Length Equal/Between","text":"<p>Check if length of field values is between or equal to value(s).</p> JavaScalaYAML <pre><code>validation().field(\"name\").lengthBetween(1, 10)\nvalidation().field(\"name\").lengthEqual(5)\n\nvalidation().field(\"name\").lengthBetween(1, 10, false) //check for length not between 1 and 10\nvalidation().field(\"name\").lengthEqual(5, false)\n</code></pre> <pre><code>validation.field(\"name\").lengthBetween(1, 10)\nvalidation.field(\"name\").lengthEqual(5)\n\nvalidation.field(\"name\").lengthBetween(1, 10, false) //check for length not between 1 and 10\nvalidation.field(\"name\").lengthEqual(5, false)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"lengthBetween\"\n            min: 1\n            max: 10\n          - type: \"lengthEqual\"\n            value: 5\n          - type: \"lengthBetween\"\n            min: 1\n            max: 10\n            negate: true\n          - type: \"lengthEqual\"\n            value: 5\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#is-increasingdecreasing","title":"Is Increasing/Decreasing","text":"<p>Check if values of a field are increasing or decreasing.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").isDecreasing()\nvalidation().field(\"amount\").isIncreasing()\n\nvalidation().field(\"amount\").isDecreasing(false) //check it is not strictly decreasing\nvalidation().field(\"amount\").isIncreasing(false)\n</code></pre> <pre><code>validation.field(\"amount\").isDecreasing()\nvalidation.field(\"amount\").isIncreasing()\n\nvalidation.field(\"amount\").isDecreasing(false) //check it is not strictly decreasing\nvalidation.field(\"amount\").isIncreasing(false)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"amount\"\n        validation:\n          - type: \"isDecreasing\"\n          - type: \"isIncreasing\"\n          - type: \"isDecreasing\"\n            strictly: false\n          - type: \"isIncreasing\"\n            strictly: false\n</code></pre>"},{"location":"docs/validation/basic-validation/#is-json-parsable","title":"Is JSON Parsable","text":"<p>Check if values of a field are JSON parsable.</p> JavaScalaYAML <pre><code>validation().field(\"details\").isJsonParsable()\n\nvalidation().field(\"details\").isJsonParsable(true) //check it is not JSON parsable\n</code></pre> <pre><code>validation.field(\"details\").isJsonParsable()\n\nvalidation.field(\"details\").isJsonParsable(true) //check it is not JSON parsable\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"details\"\n        validation:\n          - type: \"isJsonParsable\"\n          - type: \"isJsonParsable\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#match-json-schema","title":"Match JSON Schema","text":"<p>Check if values of a field match JSON schema.</p> JavaScalaYAML <pre><code>validation().field(\"details\").matchJsonSchema(\"id STRING, amount DOUBLE\")\n\nvalidation().field(\"details\").matchJsonSchema(\"id STRING, amount DOUBLE\", true) //check values do not match JSON schema\n</code></pre> <pre><code>validation.field(\"details\").matchJsonSchema(\"id STRING, amount DOUBLE\")\n\nvalidation.field(\"details\").matchJsonSchema(\"id STRING, amount DOUBLE\", true) //check values do not match JSON schema\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"details\"\n        validation:\n          - type: \"matchJsonSchema\"\n            schema: \"id STRING, amount DOUBLE\"\n          - type: \"matchJsonSchema\"\n            schema: \"id STRING, amount DOUBLE\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#match-date-time-format","title":"Match Date Time Format","text":"<p>Check if values of a field match date time format (defined formats).</p> JavaScalaYAML <pre><code>validation().field(\"date\").matchDateTimeFormat(\"yyyy-MM-dd\")\n\nvalidation().field(\"date\").matchDateTimeFormat(\"yyyy-MM-dd\", true) //check values do not match date time format\n</code></pre> <pre><code>validation.field(\"date\").matchDateTimeFormat(\"yyyy-MM-dd\")\n\nvalidation.field(\"date\").matchDateTimeFormat(\"yyyy-MM-dd\", true) //check values do not match date time format\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"date\"\n        validation:\n          - type: \"matchDateTimeFormat\"\n            format: \"yyyy-MM-dd\"\n          - type: \"matchDateTimeFormat\"\n            format: \"yyyy-MM-dd\"\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#most-common-value-in-set","title":"Most Common Value In Set","text":"<p>Check if the most common field value exists in set of values.</p> JavaScalaYAML <pre><code>validation().field(\"name\").mostCommonValueInSet(\"peter\", \"john\")\n\nvalidation().field(\"name\").mostCommonValueInSet(List.of(\"peter\", \"john\"), true) //check is most common value does not exist in set\n</code></pre> <pre><code>validation.field(\"name\").mostCommonValueInSet(\"peter\", \"john\")\n\nvalidation.field(\"name\").mostCommonValueInSet(List(\"peter\", \"john\"), true) //check is most common value does not exist in set\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"mostCommonValueInSet\"\n            values: [\"peter\", \"john\"]\n          - type: \"mostCommonValueInSet\"\n            values: [\"peter\", \"john\"]\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#unique-values-proportion-between","title":"Unique Values Proportion Between","text":"<p>Check if the fields proportion of unique values is between two values.</p> JavaScalaYAML <pre><code>validation().field(\"name\").uniqueValuesProportionBetween(0.1, 0.3)\n\nvalidation().field(\"name\").uniqueValuesProportionBetween(0.1, 0.3, true) //check if proportion of unique values is not between two values\n</code></pre> <pre><code>validation.field(\"name\").uniqueValuesProportionBetween(0.1, 0.3)\n\nvalidation.field(\"name\").uniqueValuesProportionBetween(0.1, 0.3, true) //check if proportion of unique values is not between two values\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"name\"\n        validation:\n          - type: \"uniqueValuesProportionBetween\"\n            min: 0.1\n            max: 0.3\n          - type: \"uniqueValuesProportionBetween\"\n            min: 0.1\n            max: 0.3\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#quantile-values-between","title":"Quantile Values Between","text":"<p>Check if quantiles of field values is within range.</p> JavaScalaYAML <pre><code>validation().field(\"amount\").quantileValuesBetween(Map.of(0.1, new Tuple2(1.0, 2.0)))\n\nvalidation().field(\"amount\").quantileValuesBetween(Map.of(0.1, new Tuple2(1.0, 2.0)), true) //check if quantile value is not between two values\n</code></pre> <pre><code>validation.field(\"amount\").quantileValuesBetween(Map(0.1 -&gt; (1.0, 2.0)))\n\nvalidation.field(\"amount\").quantileValuesBetween(Map(0.1 -&gt; (1.0, 2.0)), true) //check if quantile value is not between two values\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"amount\"\n        validation:\n          - type: quantileValuesBetween\n            quantileRanges:\n              \"0.1\":\n                - 1.0\n                - 10.0\n          - type: \"quantileValuesBetween\"\n            quantileRanges:\n              \"0.1\":\n                - 1.0\n                - 10.0\n            negate: true\n</code></pre>"},{"location":"docs/validation/basic-validation/#expression","title":"Expression","text":"<p>Ensure all data in field adheres to SQL expression defined that returns back a boolean. You can define complex logic in here that could combine multiple fields.</p> <p>For example, <code>CASE WHEN status == 'open' THEN balance &gt; 0 ELSE balance == 0 END</code> would check all rows with <code>status</code> open to have <code>balance</code> greater than 0, otherwise, check the <code>balance</code> is 0.</p> JavaScalaYAML <pre><code>var csvTxns = csv(\"transactions\", \"/tmp/csv\", Map.of(\"header\", \"true\"))\n  .validations(\n    validation().expr(\"amount &lt; 100\"),\n    validation().expr(\"year == 2021\").errorThreshold(0.1),  //equivalent to if error percentage is &gt; 10%, then fail\n    validation().expr(\"REGEXP_LIKE(name, 'Peter .*')\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n  );\n\nvar conf = configuration().enableValidation(true);\n</code></pre> <pre><code>val csvTxns = csv(\"transactions\", \"/tmp/csv\", Map(\"header\" -&gt; \"true\"))\n  .validations(\n    validation.expr(\"amount &lt; 100\"),\n    validation.expr(\"year == 2021\").errorThreshold(0.1),  //equivalent to if error percentage is &gt; 10%, then fail\n    validation.expr(\"REGEXP_LIKE(name, 'Peter .*')\").errorThreshold(200)  //equivalent to if number of errors is &gt; 200, then fail\n  )\n\nval conf = configuration.enableValidation(true)\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  transactions:\n    options:\n      path: \"/tmp/csv\"\n    validations:\n      - expr: \"amount &lt; 100\"\n      - expr: \"year == 2021\"\n        errorThreshold: 0.1   #equivalent to if error percentage is &gt; 10%, then fail\n      - expr: \"REGEXP_LIKE(name, 'Peter .*')\"\n        errorThreshold: 200   #equivalent to if number of errors is &gt; 200, then fail\n        description: \"Should be lots of Peters\"\n\n#enableValidation inside application.conf\n</code></pre>"},{"location":"docs/validation/external-source-validation/","title":"External Source Validations","text":"<p>Use validations that are defined in external sources such as Great Expectations or OpenMetadata. This allows you to generate data for your upstream data sources and validate your pipelines based on the same rules that would be applied in production.</p> <p></p>"},{"location":"docs/validation/external-source-validation/#supported-sources","title":"Supported Sources","text":"Source Support OpenMetadata Great Expectations DBT Constraints SodaCL MonteCarlo"},{"location":"docs/validation/external-source-validation/#openmetadata","title":"OpenMetadata","text":"<p>Use data quality rules defined from OpenMetadata to execute over dataset.</p> JavaScalaYAML <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n    .validations(metadataSource().openMetadata(\n        \"http://host.docker.internal:8585/api\",\n        Constants.OPEN_METADATA_AUTH_TYPE_OPEN_METADATA(),\n        Map.of(\n                Constants.OPEN_METADATA_JWT_TOKEN(), \"abc123\",\n                Constants.OPEN_METADATA_TABLE_FQN(), \"sample_data.ecommerce_db.shopify.raw_customer\"\n        )\n    ));\n\nvar conf = configuration().enableGenerateValidations(true);\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n  .validations(metadataSource.openMetadata(\n    \"http://host.docker.internal:8585/api\",\n    OPEN_METADATA_AUTH_TYPE_OPEN_METADATA,\n    Map(\n      OPEN_METADATA_JWT_TOKEN -&gt; \"abc123\", //find under settings/bots/ingestion-bot/token\n      OPEN_METADATA_TABLE_FQN -&gt; \"sample_data.ecommerce_db.shopify.raw_customer\"\n    )\n  ))\n\nval conf = configuration.enableGenerateValidations(true)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  my_json:\n    - options:\n        metadataSourceType: \"openMetadata\"\n        authType: \"openMetadataJwtToken\"\n        openMetadataJwtToken: \"abc123\"\n        tableFqn: \"sample_data.ecommerce_db.shopify.raw_customer\"\n</code></pre>"},{"location":"docs/validation/external-source-validation/#great-expectations","title":"Great Expectations","text":"<p>Use data quality rules defined from OpenMetadata to execute over dataset.</p> JavaScalaYAML <pre><code>var jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n    .validations(metadataSource().greatExpectations(\"great-expectations/taxi-expectations.json\");\n\nvar conf = configuration().enableGenerateValidations(true);\n</code></pre> <pre><code>val jsonTask = json(\"my_json\", \"/opt/app/data/json\")\n  .validations(metadataSource.greatExpectations(\"great-expectations/taxi-expectations.json\")\n\nval conf = configuration.enableGenerateValidations(true)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  my_json:\n    - options:\n        metadataSourceType: \"greatExpectations\"\n        expectationsFile: \"great-expectations/taxi-expectations.json\"\n</code></pre>"},{"location":"docs/validation/field-name-validation/","title":"Field Name Validations","text":"<p>Run validations on the field names to check for field name count of existence of field names.</p>"},{"location":"docs/validation/field-name-validation/#count-equal","title":"Count Equal","text":"<p>Ensure field name count is equal to certain number.</p> JavaScalaYAML <pre><code>validation().fieldNames().countEqual(3)\n</code></pre> <pre><code>validation.fieldNames.countEqual(3)\n</code></pre> <pre><code>- fieldNameType: \"fieldCountEqual\"\n  count: \"3\"\n</code></pre>"},{"location":"docs/validation/field-name-validation/#not-equal","title":"Not Equal","text":"<p>Ensure field name count is between two numbers.</p> JavaScalaYAML <pre><code>validation().fieldNames().countBetween(10, 12)\n</code></pre> <pre><code>validation.fieldNames.countBetween(10, 12)\n</code></pre> <pre><code>- fieldNameType: \"fieldCountBetween\"\n  minCount: \"10\"\n  maxCount: \"12\"\n</code></pre>"},{"location":"docs/validation/field-name-validation/#match-order","title":"Match Order","text":"<p>Ensure all field names match particular ordering and is complete.</p> JavaScalaYAML <pre><code>validation().fieldNames().matchOrder(\"account_id\", \"amount\", \"name\")\n</code></pre> <pre><code>validation.fieldNames.matchOrder(\"account_id\", \"amount\", \"name\")\n</code></pre> <pre><code>- fieldNameType: \"fieldNameMatchOrder\"\n  names: [\"account_id\", \"amount\", \"name\"]\n</code></pre>"},{"location":"docs/validation/field-name-validation/#match-set","title":"Match Set","text":"<p>Ensure field names contains set of expected names. Order is not checked.</p> JavaScalaYAML <pre><code>validation().fieldNames().matchSet(\"account_id\", \"first_name\")\n</code></pre> <pre><code>validation.fieldNames.matchSet(\"account_id\", \"first_name\")\n</code></pre> <pre><code>- fieldNameType: \"fieldNameMatchSet\"\n  names: [\"account_id\", \"first_name\"]\n</code></pre>"},{"location":"docs/validation/group-by-validation/","title":"Group By Validation","text":"<p>If you want to run aggregations based on a particular set of fields or just the whole dataset, you can do so via group by validations. An example would be checking that the sum of <code>amount</code> is less than 1000 per <code>account_id, year</code>. The validations applied can be one of the validations from the basic validation set found here.</p>"},{"location":"docs/validation/group-by-validation/#pre-filter","title":"Pre-filter","text":"<p>If you want to only run the validation on a specific subset of data, you can define pre-filter conditions. Find more details here.</p>"},{"location":"docs/validation/group-by-validation/#record-count","title":"Record count","text":"<p>Check the number of records across the whole dataset.</p> JavaScalaYAML <pre><code>validation().groupBy().count().lessThan(1000)\n</code></pre> <pre><code>validation.groupBy().count().lessThan(1000)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - aggType: \"count\"\n        aggExpr: \"count &lt; 1000\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#record-count-per-group","title":"Record count per group","text":"<p>Check the number of records for each group.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").count().lessThan(10)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").count().lessThan(10)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggType: \"count\"\n        aggExpr: \"count &lt; 10\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#sum","title":"Sum","text":"<p>Check the sum of a fields values for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").sum(\"amount\").lessThan(1000)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").sum(\"amount\").lessThan(1000)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggExpr: \"sum(amount) &lt; 1000\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#count","title":"Count","text":"<p>Check the count for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").count(\"amount\").lessThan(10)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").count(\"amount\").lessThan(10)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggType: \"count\"\n        aggExpr: \"count(amount) &lt; 10\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#min","title":"Min","text":"<p>Check the min for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").min(\"amount\").greaterThan(0)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").min(\"amount\").greaterThan(0)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggExpr: \"min(amount) &gt; 0\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#max","title":"Max","text":"<p>Check the max for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").max(\"amount\").lessThanOrEqual(100)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").max(\"amount\").lessThanOrEqual(100)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggExpr: \"max(amount) &lt; 100\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#average","title":"Average","text":"<p>Check the average for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").avg(\"amount\").between(40, 60)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").avg(\"amount\").between(40, 60)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggExpr: \"avg(amount) BETWEEN 40 &amp;&amp; 60\"\n</code></pre>"},{"location":"docs/validation/group-by-validation/#standard-deviation","title":"Standard deviation","text":"<p>Check the standard deviation for each group adheres to validation.</p> JavaScalaYAML <pre><code>validation().groupBy(\"account_id\", \"year\").stddev(\"amount\").between(0.5, 0.6)\n</code></pre> <pre><code>validation.groupBy(\"account_id\", \"year\").stddev(\"amount\").between(0.5, 0.6)\n</code></pre> <pre><code>name: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - groupByCols: [\"account_id\", \"year\"]\n        aggExpr: \"stddev(amount) BETWEEN 0.5 &amp;&amp; 0.6\"\n</code></pre>"},{"location":"docs/validation/upstream-data-source-validation/","title":"Upstream Data Source Validation","text":"<p>If you want to run data validations based on data generated or data from another data source, you can use the upstream data source validations. An example would be generating a Parquet file that gets ingested by a job and inserted into Postgres. The validations can then check for each <code>account_id</code> generated in the Parquet, it exists in <code>account_number</code> field in Postgres. The validations can be chained with basic and group by validations or even other upstream data sources, to cover any complex validations.</p> <p></p> <p></p>"},{"location":"docs/validation/upstream-data-source-validation/#pre-filter","title":"Pre-filter","text":"<p>If you want to only run the validation on a specific subset of data, you can define pre-filter conditions. Find more details here.</p>"},{"location":"docs/validation/upstream-data-source-validation/#basic-join","title":"Basic join","text":"<p>Join across datasets by particular fields. Then run validations on the joined dataset. You will notice that the data source name is appended onto the field names when joined (i.e. <code>my_first_json_customer_details</code>), to ensure field names do not clash and make it obvious which fields are being validated.</p> <p>In the below example, we check that the for the same <code>account_id</code>, then <code>customer_details.name</code> in the <code>my_first_json</code> dataset should equal to the <code>name</code> field in the <code>my_second_json</code>.</p> JavaScalaYAML <pre><code>var firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").expression(\"#{Name.name}\")\n      )\n  );\n\nvar secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation().upstreamData(firstJsonTask)                   //upstream data generation task is `firstJsonTask`\n      .joinFields(\"account_id\")                               //use `account_id` field in both datasets to join corresponding records (outer join by default)\n      .withValidation(\n        validation().field(\"my_first_json_customer_details.name\")\n          .isEqualField(\"name\")                                  //validate the name in `my_second_json` is equal to `customer_details.name` in `my_first_json` when the `account_id` matches\n      )\n  );\n</code></pre> <pre><code>val firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n\nval secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation.upstreamData(firstJsonTask)                   //upstream data generation task is `firstJsonTask`\n      .joinFields(\"account_id\")                             //use `account_id` field in both datasets to join corresponding records (outer join by default)\n      .withValidation(\n        validation.field(\"my_first_json_customer_details.name\")\n          .isEqualField(\"name\")                                //validate the name in `my_second_json` is equal to `customer_details.name` in `my_first_json` when the `account_id` matches\n      )\n  )\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/tmp/data/second_json\"\n      validations:\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"account_id\"]\n          validation:\n            expr: \"my_first_json_customer_details.name == name\"\n</code></pre>"},{"location":"docs/validation/upstream-data-source-validation/#join-expression","title":"Join expression","text":"<p>Define join expression to link two datasets together. This can be any SQL expression that returns a boolean value. Useful in situations where join is based on transformations or complex logic.</p> <p>In the below example, we have to use <code>CONCAT</code> SQL function to combine <code>'ACC'</code> and <code>account_number</code> to join with <code>account_id</code> field in <code>my_first_json</code> dataset.</p> JavaScalaYAML <pre><code>var firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").expression(\"#{Name.name}\")\n      )\n  );\n\nvar secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation().upstreamData(firstJsonTask)\n      .joinExpr(\"my_first_json_account_id == CONCAT('ACC', account_number)\")  //generic SQL expression that returns a boolean\n      .withValidation(\n        validation().field(\"my_first_json_customer_details.name\")\n          .isEqualField(\"name\")\n      )\n  );\n</code></pre> <pre><code>val firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n\nval secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation.upstreamData(firstJsonTask)\n      .joinExpr(\"my_first_json_account_id == CONCAT('ACC', account_number)\")  //generic SQL expression that returns a boolean\n      .withValidation(\n        validation.field(\"my_first_json_customer_details.name\")\n          .isEqualField(\"name\")\n      )\n  )\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/tmp/data/second_json\"\n      validations:\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"expr:my_first_json_account_id == CONCAT('ACC', account_number)\"]\n          validation:\n            expr: \"my_first_json_customer_details.name == name\"\n</code></pre>"},{"location":"docs/validation/upstream-data-source-validation/#different-join-type","title":"Different join type","text":"<p>By default, an outer join is used to gather fields from both datasets together for validation. But there may be scenarios where you want to control the join type.</p> <p>Possible join types include:</p> <ul> <li>inner</li> <li>outer, full, fullouter, full_outer</li> <li>leftouter, left, left_outer</li> <li>rightouter, right, right_outer</li> <li>leftsemi, left_semi, semi</li> <li>leftanti, left_anti, anti</li> <li>cross</li> </ul> <p>In the example below, we do an <code>anti</code> join by field <code>account_id</code> and check if there are no records. This essentially checks that all <code>account_id</code>'s from <code>my_second_json</code> exist in <code>my_first_json</code>. The second validation also does something similar but does an <code>outer</code> join (by default) and checks that the joined dataset has 30 records.</p> JavaScalaYAML <pre><code>var firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").expression(\"#{Name.name}\")\n      )\n  );\n\nvar secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation().upstreamData(firstJsonTask)\n      .joinFields(\"account_id\")\n      .joinType(\"anti\")\n      .withValidation(validation().count().isEqual(0)),\n    validation().upstreamData(firstJsonTask)\n      .joinFields(\"account_id\")\n      .withValidation(validation().count().isEqual(30))\n  );\n</code></pre> <pre><code>val firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n\nval secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation.upstreamData(firstJsonTask)\n      .joinFields(\"account_id\")\n      .joinType(\"anti\")\n      .withValidation(validation.count().isEqual(0)),\n    validation.upstreamData(firstJsonTask)\n      .joinFields(\"account_id\")\n      .withValidation(validation.count().isEqual(30))\n  )\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/tmp/data/second_json\"\n      validations:\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"account_id\"]\n          joinType: \"anti\"\n          validation:\n            aggType: \"count\"\n            aggExpr: \"count == 0\"\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"account_id\"]\n          validation:\n            aggType: \"count\"\n            aggExpr: \"count == 30\"\n</code></pre>"},{"location":"docs/validation/upstream-data-source-validation/#join-then-group-by-validation","title":"Join then group by validation","text":"<p>We can apply aggregate or group by validations to the resulting joined dataset as the <code>withValidation</code> method accepts any type of validation.</p> <p>Here we group by <code>account_id, my_first_json_balance</code> to check that when the <code>amount</code> field is summed up per group, it is between 0.8 and 1.2 times the balance.</p> JavaScalaYAML <pre><code>var firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field().name(\"balance\").type(DoubleType.instance()).min(10).max(1000),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").expression(\"#{Name.name}\")\n      )\n  );\n\nvar secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation().upstreamData(firstJsonTask).joinFields(\"account_id\")\n      .withValidation(\n        validation().groupBy(\"account_id\", \"my_first_json_balance\")\n          .sum(\"amount\")\n          .betweenFields(\"my_first_json_balance * 0.8\", \"my_first_json_balance * 1.2\")\n      )\n  );\n</code></pre> <pre><code>val firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field.name(\"balance\").`type`(DoubleType).min(10).max(1000),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n\nval secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation.upstreamData(firstJsonTask).joinFields(\"account_id\")\n      .withValidation(\n        validation.groupBy(\"account_id\", \"my_first_json_balance\")\n          .sum(\"amount\")\n          .betweenFields(\"my_first_json_balance * 0.8\", \"my_first_json_balance * 1.2\")\n      )\n  )\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/tmp/data/second_json\"\n      validations:\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"account_id\"]\n          validation:\n            groupByCols: [\"account_id\", \"my_first_json_balance\"]\n            aggExpr: \"sum(amount) BETWEEN my_first_json_balance * 0.8 AND my_first_json_balance * 1.2\"\n</code></pre>"},{"location":"docs/validation/upstream-data-source-validation/#chained-validations","title":"Chained validations","text":"<p>Given that the <code>withValidation</code> method accepts any other type of validation, you can chain other upstream data sources with it. Here we will show a third upstream data source being checked to ensure 30 records exists after joining them together by <code>account_id</code>.</p> JavaScalaYAML <pre><code>var firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field().name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field().name(\"balance\").type(DoubleType.instance()).min(10).max(1000),\n    field().name(\"customer_details\")\n      .fields(\n        field().name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n  .count(count().records(10));\n\nvar thirdJsonTask = json(\"my_third_json\", \"/tmp/data/third_json\")\n  .fields(\n    field().name(\"account_id\"),\n    field().name(\"amount\").type(IntegerType.instance()).min(1).max(100),\n    field().name(\"name\").expression(\"#{Name.name}\")\n  )\n  .count(count().records(10));\n\nvar secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation().upstreamData(firstJsonTask)\n      .joinFields(\"account_id\")\n      .withValidation(\n        validation().upstreamData(thirdJsonTask)\n          .joinFields(\"account_id\")\n          .withValidation(validation().count().isEqual(30))\n      )\n  );\n</code></pre> <pre><code>val firstJsonTask = json(\"my_first_json\", \"/tmp/data/first_json\")\n  .fields(\n    field.name(\"account_id\").regex(\"ACC[0-9]{8}\"),\n    field.name(\"balance\").`type`(DoubleType).min(10).max(1000),\n    field.name(\"customer_details\")\n      .fields(\n        field.name(\"name\").expression(\"#{Name.name}\")\n      )\n  )\n  .count(count.records(10))\n\nval thirdJsonTask = json(\"my_third_json\", \"/tmp/data/third_json\")\n  .fields(\n    field.name(\"account_id\"),\n    field.name(\"amount\").`type`(IntegerType).min(1).max(100),\n    field.name(\"name\").expression(\"#{Name.name}\"),\n  )\n  .count(count.records(10))\n\nval secondJsonTask = json(\"my_second_json\", \"/tmp/data/second_json\")\n  .validations(\n    validation.upstreamData(firstJsonTask).joinFields(\"account_id\")\n      .withValidation(\n        validation.groupBy(\"account_id\", \"my_first_json_balance\")\n          .sum(\"amount\")\n          .betweenFields(\"my_first_json_balance * 0.8\", \"my_first_json_balance * 1.2\")\n      ),\n  )\n</code></pre> <pre><code>---\nname: \"account_checks\"\ndataSources:\n  json:\n    - options:\n        path: \"/tmp/data/second_json\"\n      validations:\n        - upstreamDataSource: \"my_first_json\"\n          joinFields: [\"account_id\"]\n          validation:\n            upstreamDataSource: \"my_third_json\"\n            joinFields: [\"account_id\"]\n            validation:\n              aggType: \"count\"\n              aggExpr: \"count == 30\"\n</code></pre> <p>Can check out a full example here for more details.</p>"},{"location":"get-started/quick-start/","title":"Run Data Caterer","text":""},{"location":"get-started/quick-start/#quick-start","title":"Quick start","text":"<ul> <li> <p> Java/Scala</p> <p>Instructions for using Java/Scala API via Docker</p> </li> <li> <p> YAML</p> <p>Instructions for using YAML via Docker</p> </li> <li> <p> UI App - Docker</p> <p>Instructions for Docker download</p> </li> <li> <p> UI App - Mac</p> <p>Instructions for Mac download</p> </li> <li> <p> UI App - Windows</p> <p>Instructions for Windows download</p> </li> <li> <p> UI App - Linux</p> <p>Instructions for Linux download</p> </li> </ul>"},{"location":"get-started/quick-start/#javascala-api","title":"Java/Scala API","text":"<pre><code>git clone git@github.com:data-catering/data-caterer-example.git\ncd data-caterer-example &amp;&amp; ./run.sh\n#check results under docker/sample/report/index.html folder\n#If you want to run any other examples, check the class names under src/scala or src/java\n#And then run with ./run.sh &lt;class_name&gt;\n#i.e. ./run.sh CsvPlan\n</code></pre>"},{"location":"get-started/quick-start/#yaml","title":"YAML","text":"<pre><code>git clone git@github.com:data-catering/data-caterer-example.git\ncd data-caterer-example &amp;&amp; ./run.sh simple-json.yaml\n#check results under docker/sample/report/index.html folder\n#check example YAML files under:\n# - docker/data/custom/plan\n# - docker/data/custom/task\n# - docker/data/custom/validation\n#If you want to run any other examples, check the files under docker/data/custom/plan\n#And then run with ./run.sh &lt;file_name&gt;\n#i.e. ./run.sh parquet.yaml\n</code></pre>"},{"location":"get-started/quick-start/#docker","title":"Docker","text":"<ol> <li>Docker    <pre><code>docker run -d -i -p 9898:9898 -e DEPLOY_MODE=standalone --name datacaterer datacatering/data-caterer:0.16.1\n</code></pre></li> <li>Open localhost:9898</li> </ol>"},{"location":"get-started/quick-start/#mac","title":"Mac","text":"<ol> <li>Mac download</li> <li>Drag Data Caterer to your Applications folder and double-click to run</li> <li>If your browser doesn't open, go to http://localhost:9898 in your preferred browser</li> </ol>"},{"location":"get-started/quick-start/#windows","title":"Windows","text":"<ol> <li>Windows download</li> <li>Click on 'More info' then at the bottom, click 'Run anyway'</li> <li>Go to '/Program Files/DataCaterer' folder and run DataCaterer application</li> <li>If your browser doesn't open, go to http://localhost:9898 in your preferred browser</li> </ol>"},{"location":"get-started/quick-start/#linux","title":"Linux","text":"<ol> <li>Linux download</li> <li>If your browser doesn't open, go to http://localhost:9898 in your preferred browser</li> </ol>"},{"location":"get-started/quick-start/#report","title":"Report","text":"<p>Check the report generated under <code>docker/data/custom/report/index.html</code>.</p> <p>Sample report can also be seen here.</p>"},{"location":"get-started/quick-start/#gradual-start","title":"Gradual start","text":"<p>If you prefer a step-by-step approach to learning the capabilities of Data Caterer, there are a number of guides that take you through the various data sources and approaches that can be taken when using the tool.</p> <ul> <li>Check out the starter guide here that will take your through step by step</li> <li>You can also check the other guides here to see the other possibilities of what Data Caterer can achieve for you.</li> </ul>"},{"location":"sample/demo/demo-index/","title":"Demo index","text":""},{"location":"sample/demo/demo-index/#what-does-it-look-like","title":"What does it look like","text":"<p>Play around with the example below to see how Data Caterer can generate transaction data, validate it and then clean it up.</p> <p>*This is for illustrative purposes</p> <p> Generate Validate Cleanup </p>"},{"location":"use-case/business-value/","title":"Business Value","text":"<p>Below is a list of the business related benefits from using Data Caterer which may be applicable for your use case.</p> Problem Data Caterer Solution Resources Effects Reliable test data creation - Profile existing data- Create scenarios- Generate data Software Engineers, QA, Testers Cost reduction in labor, more time spent on development, more bugs caught before production Faster development cycles - Generate data in local, test, UAT, pre-prod- Run different scenarios Software Engineers, QA, Testers More defects caught in lower environments, features pushed to production faster, common framework used across all environments Data compliance - Profiling existing data- Generate based on metadata- No complex masking- No production data used in lower environments Audit and compliance No chance for production data breaches Storage costs - Delete generated data- Test specific scenarios Infrastructure Lower data storage costs, less time spent on data management and clean up Schema evolution - Create metadata from data sources- Generate data based off fresh metadata Software Engineers, QA, Testers Less time spent altering tests due to schema changes, ease of use between environments and application versions"},{"location":"use-case/comparison/","title":"Comparison to similar tools","text":"<p>I have tried to include all the companies found in the list here from Mostly AI blog post and used information that is publicly available.</p> <p>The companies/products not shown below either have:</p> <ul> <li>a website with insufficient information about the technology side of data generation/validation</li> <li>no/little documentation</li> <li>don't have a free, no sign-up version of their app to use</li> </ul>"},{"location":"use-case/comparison/#data-generation","title":"Data Generation","text":"Tool Description Cost Pros Cons Data Catering Scala based data generation and validation tool via metadata Free (Open Source)  Data generation and validation Batch and event generation Maintain referential integrity Scala/Java SDK Customisable scenarios and validations Open source Metadata driven Report generation Use validation rules from existing tools Data clean up UI Alerting  No load testing metrics No validation of real time data sources Clearbox AI Python based data generation tool via ML Unclear  Python SDK UI interface Detect private data Report generation  Batch data only No data clean up Limited/no documentation Curiosity Software Platform solution for test data management Unclear  Extensive documentation Generate data based off test cases UI interface Web/API/UI/mobile testing  No quick start No SDK Many components that may not be required No event generation support DataCebo Synthetic Data Vault Python based data generation tool via ML Unclear  Python SDK Report generation Data quality checks Business logic constraints  No data connection support No data clean up No foreign key support Datafaker Realistic data generation library Free  SDK for many languages Simple, easy to use Extensible Open source Generate realistic values  No data connection support No data clean up No validation No foreign key support DBLDatagen Python based data generation tool Free  Python SDK Open source Good documentation Customisable scenarios Customisable field generation Generate from existing data/schemas Plugin third-party libraries  Limited support if issues Code required No data clean up No data validation Gatling HTTP API load testing tool Free (Open Source)Gatling Enterprise, usage based, starts from \u20ac89 per month, 1 user, 6.25 hours of testing  Kotlin, Java &amp; Scala SDK Widely used Open source Clear documentation Extensive testing/validation support Customisable scenarios Report generation  Only supports HTTP, JMS and JDBC No data clean up Data feeders not based off metadata Gretel Python based data generation tool via ML Usage based, starts from $295 per month, $2.20 per credit, assumed USD  CLI &amp; Python SDK UI interface Training and re-use of models Detect private data Customisable scenarios  Batch data only No relationships between data sources Only simple foreign key relations defined No data clean up Charge by usage Howso Python based data generation tool via ML Unclear  Python SDK Playground to try Open source library Customisable scenarios  No support for data sources No data validation No data clean up Mostly AI Python based data generation tool via ML Usage based, Enterprise 1 user, 100 columns, 100K rows $3,100 per month, assumed USD  Report generation Non-technical users can use UI Customisable scenarios  Charge by usage Batch data only No data clean up Confusing use of 'smart select' for multiple foreign keys Limited custom field generation logic Multiple deployment components No SDK Octopize Python based data generation tool via ML Unclear  Python &amp; R SDK Report generation API for metadata Customisable scenarios  Input data source is only CSV Multiple manual steps before starting Quickstart is not a quickstart Documentation lacks code examples Synthesized Python based data generation tool via ML Unclear  CLI &amp; Python SDK API for metadata IDE setup Data quality checks  Not sure what is SDK &amp; TDK Charge by usage No report of what was generated No relationships between data sources Tonic Platform solution for generating data Unclear  UI interface Good documentation Detect private data Support for encrypted columns Report generation Alerting  Batch data only Multiple deployment components No relationships between data sources No data validation No data clean up No SDK (only API) Difficult to embed complex business logic YData Python based data generation tool via ML. Platform solution as well Unclear  Python SDK Open source Detect private data Compare datasets Report generation  No data connection support Batch data only No data clean up Separate data generation and data validation No foreign key support"},{"location":"use-case/comparison/#use-of-ml-models","title":"Use of ML models","text":"<p>You may notice that the majority of data generators use machine learning (ML) models to learn from your existing datasets to generate new data. Below are some pros and cons to the approach.</p> <p>Pros</p> <ul> <li>Simple setup</li> <li>Ability to reproduce complex logic</li> <li>Flexible to accept all types of data</li> </ul> <p>Cons</p> <ul> <li>Long time for model learning</li> <li>Black box of logic</li> <li>Maintain, store and update of ML models</li> <li>Required connection to production data</li> <li>Potential to leak PII data from production</li> <li>Restriction on input data lengths</li> <li>May not maintain referential integrity</li> <li>Require deeper understanding of ML models for fine-tuning</li> <li>Accuracy may be worse than non-ML models</li> </ul>"},{"location":"use-case/roadmap/","title":"Roadmap","text":"<p>Items below summarise the roadmap of Data Caterer. As each task gets completed, it will be documented and linked.</p> Feature Description Sub Tasks Data source support Batch or real time data sources that can be added to Data Caterer. Support data sources that users want - AWS, GCP and Azure related data services ( cloud storage)-  Delta Lake-  Iceberg-  RabbitMQ-  BigQuery- ActiveMQ- MongoDB- Elasticsearch- Snowflake- Databricks- Pulsar Metadata discovery Allow for schema and data profiling from external metadata sources -  HTTP (OpenAPI spec)- JMS- Read from samples-  OpenLineage metadata (Marquez)-  OpenMetadata-  Open Data Contract Standard (ODCS)-  Data Contract CLI- Amundsen- Datahub- Confluent Schema Registry- Solace Event Portal- Airflow- DBT- Manually insert create table statement from UI Developer API Scala/Java interface for developers/testers to create data generation and validation tasks -  Scala-  Java- Python- Javascript Report generation Generate a report that summarises the data generation or validation results -  Report for data generated and validation rules UI portal Allow users to access a UI to input data generation or validation tasks. Also be able to view report results -  Base UI with create, edit and delete plan, connections and history-  Run on Mac, Linux and Windows- Metadata stored in database-  Store data generation/validation run information in file/database- Preview of generated data- Additional dialog to confirm delete and execute plan Integration with data validation tools Derive data validation rules from existing data validation tools -  Great Expectation- DBT constraints- SodaCL- MonteCarlo-  OpenMetadata Data validation rule suggestions Based on metadata, generate data validation rules appropriate for the dataset -  Suggest basic data validations (yet to document) Wait conditions before data validation Define certain conditions to be met before starting data validations -  Webhook-  File exists-  Data exists via SQL expression-  Pause Validation types Ability to define simple/complex data validations -  Basic validations-  Aggregates (sum of amount per account is &gt; 500)- Ordering (transactions are ordered by date)-  Relationship (at least one account entry in history table per account in accounts table)- Data profile (how close the generated data profile is compared to the expected data profile)-  Field name (check field count, field names, ordering)-  Pre-conditions before validating data Data generation record count Generate scenarios where there are one to many, many to many situations relating to record count. Also ability to cover all edge cases or scenarios -  Cover all possible cases (i.e. record for each combination of oneOf values, positive/negative values, pairwise etc.)- Ability to override edge cases Alerting When tasks have completed, ability to define alerts based on certain conditions -  Slack- Email Metadata enhancements Based on data profiling or inference, can add to existing metadata - PII detection (can integrate with Presidio)- Relationship detection across data sources- SQL generation- Ordering information Data cleanup Ability to clean up generated data -  Clean up generated data-  Clean up data in consumer data sinks- Clean up data from real time sources (i.e. DELETE HTTP endpoint, delete events in JMS) Trial version Trial version of the full app for users to test out all the features -  Trial app to try out all features Code generation Based on metadata or existing classes, code for data generation and validation could be generated - Code generation- Schema generation from Scala/Java class Real time response data validations Ability to define data validations based on the response from real time data sources (e.g. HTTP response) -  HTTP response data validation"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/","title":"A year of getting paid from Medium articles","text":""},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#creating-articles","title":"Creating Articles","text":"<p>At the end of last year, I decided to put more effort into creating articles both to help boost awareness of my new business Data Catering and consolidate my knowledge and exploration of technology topics. I had been using Medium for a couple of articles before and was enticed by the fact that you could monetize these articles via people viewing, reading and reacting to what you have written. So I signed up to the Medium Partner Program for a year to find out how much my articles are worth, hoping to at least break even at the end of it.</p> <p>A small thing to note, Medium refers to articles as \"stories\". So wherever you see \"story\", just replace with article as I'm not a story writer.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#monetizing-an-article","title":"Monetizing an Article","text":"<p>To earn money from an article, you have to restrict access to the article to only other members of the Medium Partner Program. You have the option as a writer of the article to also share a friend's link which allows others to bypass this restriction, but you don't earn anything based on the interactions from the friend's link.</p> <p>So your only chances of earning from an article are essentially based on people outside your network, who are members, interacting with your article.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#medium-dashboards","title":"Medium Dashboards","text":"<p>You have access to a few dashboards which give you summary statistics on how users are interacting with your articles.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#partner-program-dashboard","title":"Partner Program Dashboard","text":"<p>Shows you a summary of the current month earnings and gives you an overview on a per-story basis. Another thing to note is that you now only get paid once you have reached $10 USD. Previously, you would get paid per month no matter the amount.</p> <p></p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#audience-dashboard","title":"Audience Dashboard","text":"<p>An overview of subscribers to your articles. You can see a jump in October because I wrote an article that month.</p> <p></p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#story-dashboard","title":"Story Dashboard","text":"<p>Details on how many people have viewed and read your articles.</p> <p></p> <p>Basic sorting options for story statistics.</p> <p></p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#per-story-dashboard","title":"Per Story Dashboard","text":"<p>You can drill down into more details at the article level.</p> <p></p> <p>The breakdown of member/non-member reads and views per day.</p> <p></p> <p>The sources of traffic to your article and the interests of your readers.</p> <p></p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#i-want-more-insights","title":"I Want More Insights","text":"<p>I wanted to run some further analysis on my article data. So I went searching for some export options within the Medium website but could only find you can export out your audience statistics. Eventually, I found something more comprehensive in this GitHub repo called medium_stats. Great!</p> <p>When I ran it after installing it via pip, it ran into a JSON decoding error. Most likely Medium has changed its API and the project needs to be updated. Using my internet skills, I opened up \"Inspect\" in my browsers, went to the  \"Network\" tab and tried to find out which API call contains all the juicy information. After a few clicks, I found this GraphQL response.</p> <p></p> <p>I could see the medium_stats project was already making some GraphQL calls. So I quickly cloned the repo, added in the missing GraphQL calls and got it to kinda work. Now I have exported out my stats in JSON format. How can we analyse this data quickly?</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#duckdb","title":"DuckDB","text":"<p>Without putting too much thought into it, I knew I could easily use DuckDB to query this JSON data via SQL. Now I can run queries like this to get a nice compact view of the stats I'm interested in.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#views-reads-and-earnings-per-story","title":"Views, reads and earnings per story","text":"<pre><code>SELECT node.title                                                                 AS title,\n       node.totalStats.views                                                      AS views,\n       node.totalStats.reads                                                      AS reads,\n       node.readingTime,\n       CAST(CONCAT(node.earnings.total.units, '.',\n                   LEFT(CAST(node.earnings.total.nanos AS string), 2)) AS DOUBLE) AS earnings\nFROM\n    read_json('/tmp/stats_exports/*/agg_stats/*.json')\nORDER BY earnings DESC;\n</code></pre>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#earnings-per-day-per-story","title":"Earnings per day per story","text":"<pre><code>SELECT a.node.title AS title, e.date AS date, e.total_earnings AS earn\nFROM (SELECT d.id                                          AS id,\n             STRFTIME(MAKE_TIMESTAMP(CAST(d.daily_earning.periodStartedAt AS BIGINT) * 1000),\n                      '%Y-%m-%d')                          AS date,\n             ROUND(SUM(d.daily_earning.amount / 100.0), 2) AS total_earnings\n      FROM (SELECT p.post.id AS id, UNNEST(p.post.earnings.dailyEarnings) AS daily_earning\n            FROM (SELECT UNNEST(data.post) AS post\n                  FROM\n                      read_json('/tmp/stats_exports/*/post_events/*.json')) p) d\n      GROUP BY id,\n               date) e\n         JOIN read_json('/tmp/stats_exports/*/agg_stats/*.json') a ON a.node.id = e.id\nORDER BY earn DESC;\n</code></pre>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#earnings-per-member-interaction-per-story","title":"Earnings per member interaction per story","text":"<pre><code>SELECT id,\n       STRFTIME(MAKE_TIMESTAMP(CAST(earnings.periodStartedAt AS BIGINT) * 1000),\n                '%Y-%m-%d')                                      AS date,\n       earnings.amount                                           AS amount,\n       stats.readersThatReadCount                                AS reads,\n       stats.readersThatViewedCount                              AS views,\n       stats.readersThatClappedCount                             AS claps,\n       stats.readersThatRepliedCount                             AS replies,\n       stats.readersThatHighlightedCount                         AS highlights,\n       stats.readersThatInitiallyFollowedAuthorFromThisPostCount AS follows\nFROM (SELECT d.id               AS id,\n             d.stats            AS stats,\n             UNNEST(d.earnings) AS earnings\n      FROM (SELECT t.post.id                                   AS id,\n                   t.post.earnings.dailyEarnings               AS earnings,\n                   UNNEST(t.post.postStatsDailyBundle.buckets) AS stats\n            FROM (SELECT UNNEST(data.post) AS post\n                  FROM read_json('/tmp/stats_exports/*/post_earnings_breakdown/*.json')) t) d\n      WHERE earnings NOT NULL AND stats.membershipType = 'MEMBER')\nWHERE earnings.periodStartedAt = stats.dayStartsAt\nORDER BY amount DESC;\n</code></pre>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#linear-regression-of-member-interactions-with-earnings","title":"Linear regression of member interactions with earnings","text":"<p>I don't think I have enough data to get an accurate estimation of the formula used by Medium to calculate earnings but the below query gives a rough indication. Someone else with a larger following could get a better estimate.</p> <pre><code>SELECT REGR_SLOPE(earnings.amount, stats.readersThatReadCount)                                AS slope_read,\n       REGR_SLOPE(earnings.amount, stats.readersThatViewedCount)                              AS slope_view,\n       REGR_SLOPE(earnings.amount, stats.readersThatClappedCount)                             AS slope_clap,\n       REGR_SLOPE(earnings.amount, stats.readersThatRepliedCount)                             AS slope_reply,\n       REGR_SLOPE(earnings.amount, stats.readersThatHighlightedCount)                         AS slope_highlight,\n       REGR_SLOPE(earnings.amount, stats.readersThatInitiallyFollowedAuthorFromThisPostCount) AS slope_follow,\n       REGR_INTERCEPT(earnings.amount, stats.readersThatReadCount)                            AS intercept\nFROM (SELECT d.id               AS id,\n             d.stats            AS stats,\n             UNNEST(d.earnings) AS earnings\n      FROM (SELECT t.post.id                                   AS id,\n                   t.post.earnings.dailyEarnings               AS earnings,\n                   UNNEST(t.post.postStatsDailyBundle.buckets) AS stats\n            FROM (SELECT UNNEST(data.post) AS post\n                  FROM read_json('/tmp/stats_exports/*/post_earnings_breakdown/*.json')) t) d\n      WHERE earnings NOT NULL AND stats.membershipType = 'MEMBER')\nWHERE earnings.periodStartedAt = stats.dayStartsAt;\n</code></pre> <p></p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#did-i-reach-break-even","title":"Did I Reach Break Even?","text":"<p>So I wasn't really expecting much out of it but was at least hoping to break even with my initial $50 USD investment.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#total-earnings-across-all-articles","title":"Total earnings across all articles","text":"<pre><code>SELECT SUM(CAST(CONCAT(node.earnings.total.units, '.',\n                       LEFT(CAST(node.earnings.total.nanos AS string), 2)) AS DOUBLE)) AS total_earnings\nFROM\n    read_json('/tmp/stats_exports/*/agg_stats/*.json');\n</code></pre> <p>Nope! But surely, the knowledge gained, priceless.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#check-your-stats","title":"Check Your Stats","text":"<p>I've created a PR to the main repo here but if you can't wait, try to run the following:</p> <pre><code>git clone git@github.com:pflooky/medium_stats.git\ncd medium_stats\ngit checkout update-graphql\n#check README for details on how to use your Medium credentials\npython3 -m venv .venv\nsource .venv/bin/activate\npip3 install pyproject.toml\nPYTHONPATH=. python -m medium_stats scrape_user -u &lt;username&gt; --all\n</code></pre> <p>Now all your stats should be available in the <code>stats_export</code> directory.</p>"},{"location":"use-case/blog/a-year-of-getting-paid-from-medium-articles/#using-duckdb","title":"Using DuckDB","text":"<p>To query the data, you can use DuckDB. If you have it installed already, use that, else, you can use it via a tool I created called insta-infra. All you need is Docker.</p> <pre><code>git clone git@github.com:data-catering/insta-infra.git\ncd insta-infra &amp;&amp; ./run.sh duckdb\ndocker cp &lt;directory path to stats_export&gt; duckdb:/tmp\n./run.sh -c duckdb\n</code></pre> <p>You can use the above queries to now check your stats. You can also find all the queries here.</p>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/","title":"Rethinking \"Shift-Left\" for Data Pipelines: How to Fix Issues Before They Crash the Party","text":"<p>You know that feeling when you\u2019re cooking dinner, and instead of waiting for your guests to complain about a burnt dish, you taste it along the way to make sure everything\u2019s perfect? That\u2019s what shift-left should be all about\u2014catching problems early rather than waiting for disaster to strike. But in the world of data pipelines, \u201cshift-left\u201d has been thrown around so loosely that it\u2019s lost its real meaning.</p>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#the-misunderstanding-data-observability-shift-left","title":"The Misunderstanding: Data Observability \u2260 Shift-Left","text":"<p>These days, many teams say they\u2019re doing shift-left when they use data observability tools. And don\u2019t get me wrong, watching your data flow in production and getting real-time alerts is super useful. But here\u2019s the thing: relying on these tools is like waiting for a smoke alarm to go off before you start worrying about a fire. Sure, the alarm tells you something\u2019s wrong, but wouldn\u2019t it be better if you had checked your stove before the kitchen went up in flames?</p> <p>In other words, data observability in production is more of a \u201cshift-right\u201d move. It tells you about issues after they\u2019ve already happened, which isn\u2019t really shifting quality assurance to the left of your development cycle.</p> <p></p>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#what-shift-left-should-really-look-like","title":"What Shift-Left Should Really Look Like","text":"<p>True shift-left is all about being proactive. Imagine if, before you even started cooking, you laid out all your ingredients, read through the recipe, and prepped everything. That\u2019s the idea here: catch the errors in your data pipeline before they become a full-blown mess in production. Here\u2019s how you can make that happen:</p>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#1-test-in-your-local-kitchen-aka-localtest-environments","title":"1. Test in Your Local Kitchen (a.k.a. Local/Test Environments)","text":"<p>Before you serve a dish at a dinner party (or push your data pipeline to production), you should test it out in a safe, controlled environment. Think of it like doing a practice run:</p> <ul> <li>Simulate Real Scenarios: Just as you\u2019d taste-test your food, simulate edge cases and error conditions to see how   your pipeline behaves.</li> <li>Practice with Contracts: If you\u2019re working with data from different sources, use contract tests to make sure   everyone\u2019s on the same page \u2014 like having a clear recipe that both the chef and cooks follow.</li> </ul>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#2-contract-driven-data-pipelines-your-recipe-for-success","title":"2. Contract-Driven Data Pipelines: Your Recipe for Success","text":"<p>Imagine if every time you cooked, you had a contract with your ingredients \u2014 \u201cI promise to use only 150g of chicken\u201d or \u201cI promise to preheat the oven to 375\u00b0F.\u201d In data pipelines, a contract-driven approach means agreeing on a specific  format and structure between the team that produces data and the one that consumes it. This way:</p> <ul> <li>Everyone Knows What to Expect: No more surprises when the dish (or data) arrives.</li> <li>Automated Checks Keep Things Honest: Just like checking a recipe before adding salt, automated tests ensure that   changes in data or schema don\u2019t break everything downstream.</li> </ul>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#3-simulate-a-real-dinner-party-production-in-a-mini-version","title":"3. Simulate a Real Dinner Party (Production) in a Mini Version","text":"<p>A good chef will always cook this dish for themselves before serving to their customers. It\u2019s a great way to catch  issues before the stakes are high. In data terms:</p> <ul> <li>Generate Production-like Data: Work with data sets in your test environment to mimic what happens in production.</li> <li>Introduce Controlled \u201cSurprises\u201d: Intentionally throw in some anomalies to see if your pipeline can handle   unexpected situations\u2014kind of like inviting a friend who always brings an odd dish to the party, just to see how   everyone reacts.</li> </ul>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#the-perks-of-doing-shift-left-right","title":"The Perks of Doing Shift-Left Right","text":"<p>When you really commit to shift-left, you\u2019re not just avoiding fires in the kitchen \u2014 you\u2019re making your whole process smoother and more reliable. Here\u2019s what you can expect:</p> <ul> <li>Fewer Production Nightmares: By catching issues early, you can avoid those costly, last-minute fixes.</li> <li>Faster Iteration: With robust pre-production testing, you can make changes confidently and quickly.</li> <li>Less Stress, More Confidence: Knowing that your data pipeline has been thoroughly checked before going live builds   trust among your team and stakeholders.</li> <li>Better Overall Quality: Just like a well-planned meal, a shift-left approach ensures that every step of your   process is aligned for success.</li> </ul>"},{"location":"use-case/blog/rethinking-shift-left-for-data-pipelines/#wrapping-it-up","title":"Wrapping It Up","text":"<p>At its core, the idea of shift-left should be about stopping problems before they start\u2014like checking your ingredients before you begin cooking rather than scrambling to salvage a ruined meal. While data observability tools are essential, they shouldn\u2019t be the only line of defense. By being proactive with testing, using contract-driven pipelines, and simulating production environments early on, you\u2019re setting up your data processes for success.</p> <p>So next time you hear someone talk about shift-left in data pipelines, ask yourself: Are we just watching for smoke, or are we making sure the fire never starts?</p>"},{"location":"use-case/blog/shift-left-data-quality/","title":"Shifting Data Quality Left with Data Catering","text":""},{"location":"use-case/blog/shift-left-data-quality/#empowering-proactive-data-management","title":"Empowering Proactive Data Management","text":"<p>In the ever-evolving landscape of data-driven decision-making, ensuring data quality is non-negotiable. Traditionally, data quality has been a concern addressed late in the development lifecycle, often leading to reactive measures and increased costs. However, a paradigm shift is underway with the adoption of a \"shift left\" approach, placing data quality at the forefront of the development process.</p>"},{"location":"use-case/blog/shift-left-data-quality/#today","title":"Today","text":"<pre><code>graph LR\n  subgraph badQualityData[&lt;b&gt;Manually generated data, limited data scenarios, fragmented testing tools&lt;/b&gt;]\n  local[&lt;b&gt;Local&lt;/b&gt;\\nManual test, unit test]\n  dev[&lt;b&gt;Dev&lt;/b&gt;\\nManual test, integration test]\n  stg[&lt;b&gt;Staging&lt;/b&gt;\\nSanity checks]\n  end\n\n  subgraph qualityData[&lt;b&gt;Reliable data, the true test&lt;/b&gt;]\n  prod[&lt;b&gt;Production&lt;/b&gt;\\nData quality checks, monitoring, observaibility]\n  end\n\n  style badQualityData fill:#d9534f,fill-opacity:0.7\n  style qualityData fill:#5cb85c,fill-opacity:0.7\n\n  local --&gt; dev\n  dev --&gt; stg\n  stg --&gt; prod</code></pre>"},{"location":"use-case/blog/shift-left-data-quality/#with-data-caterer","title":"With Data Caterer","text":"<pre><code>graph LR\n  subgraph qualityData[&lt;b&gt;Reliable data anywhere, common testing tool across all data sources&lt;/b&gt;]\n  direction LR\n  local[&lt;b&gt;Local&lt;/b&gt;\\nManual test, unit test]\n  dev[&lt;b&gt;Dev&lt;/b&gt;\\nManual test, integration test]\n  stg[&lt;b&gt;Staging&lt;/b&gt;\\nSanity checks]\n  prod[&lt;b&gt;Production&lt;/b&gt;\\nData quality checks, monitoring, observaibility]\n  end\n\n  style qualityData fill:#5cb85c,fill-opacity:0.7\n\n  local --&gt; dev\n  dev --&gt; stg\n  stg --&gt; prod</code></pre>"},{"location":"use-case/blog/shift-left-data-quality/#understanding-the-shift-left-approach","title":"Understanding the Shift Left Approach","text":"<p>\"Shift left\" is a philosophy that advocates for addressing tasks and concerns earlier in the development lifecycle. Applied to data quality, it means tackling data issues as early as possible, ideally during the development and testing phases. This approach aims to catch data anomalies, inaccuracies, or inconsistencies before they propagate through the system, reducing the likelihood of downstream errors.</p>"},{"location":"use-case/blog/shift-left-data-quality/#data-caterer-the-catalyst-for-shifting-left","title":"Data Caterer: The Catalyst for Shifting Left","text":"<p>Enter Data Caterer, a metadata-driven data generation and validation tool designed to empower organizations in shifting data quality left. By incorporating Data Caterer into the early stages of development, teams can proactively test complex data flows, validate data sources, and ensure data quality before it reaches downstream processes.</p>"},{"location":"use-case/blog/shift-left-data-quality/#key-advantages-of-shifting-data-quality-left-with-data-caterer","title":"Key Advantages of Shifting Data Quality Left with Data Caterer","text":"<ol> <li>Early Issue Detection<ul> <li>Identify data quality issues early in the development process, reducing the risk of errors downstream.</li> </ul> </li> <li>Proactive Validation<ul> <li>Create production-like data scenarios, validate sources and complex flows with simplicity, promoting proactive    data quality.</li> </ul> </li> <li>Efficient Testing Across Sources<ul> <li>Seamlessly test data across various sources, including databases, file formats, HTTP, and messaging, all within    your local laptop or development environment.</li> <li>Fast feedback loop to motivate developers to ensure thorough testing of data consumers.</li> </ul> </li> <li>Integration with Development Pipelines<ul> <li>Easily integrate Data Caterer as a task in your development pipelines, ensuring that data quality is a continuous    consideration rather than an isolated event.</li> </ul> </li> <li>Integration with Existing Metadata<ul> <li>By harnessing the power of existing metadata from data catalogs, schema registries, or other data validation tools,   Data Caterer streamlines the process, automating the generation and validation of your data effortlessly.</li> </ul> </li> <li>Improved Collaboration<ul> <li>Facilitate collaboration between developers, testers, and data professionals by providing a common platform for   early data validation.</li> </ul> </li> </ol>"},{"location":"use-case/blog/shift-left-data-quality/#realizing-the-vision-of-proactive-data-quality","title":"Realizing the Vision of Proactive Data Quality","text":"<p>As organizations strive for excellence in their data-driven endeavors, the shift left approach with Data Caterer becomes a strategic imperative. By instilling a proactive data quality culture, teams can minimize the risk of costly errors, enhance the reliability of their data, and streamline the entire development lifecycle.</p> <p>In conclusion, the marriage of the shift left philosophy and Data Caterer brings forth a new era of data management, where data quality is not just a final checkpoint but an integral part of every development milestone. Embrace the shift left approach with Data Caterer and empower your teams to build robust, high-quality data solutions from the very beginning.</p> <p>Shift Left, Validate Early, and Accelerate with Data Caterer.</p>"},{"location":"use-case/changelog/0.14.2/","title":"0.14.2","text":"<p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Simplified API for creating HTTP and message fields</li> <li>More powerful data validations (distinctInSet, isIncreasing/Decreasing, matchJsonSchema, and more)</li> <li>JSON Schema for Data Caterer YAML</li> <li>Consistent naming convention for all data sources, changing <code>column</code> to <code>field</code></li> <li>Validate HTTP responses</li> <li>More accurate timing of HTTP request and response</li> <li>Fix bug when field options defined as non-string get ignored by data generator</li> </ul>"},{"location":"use-case/changelog/0.14.2/#simplified-api-for-creating-http-and-message-fields","title":"Simplified API for creating HTTP and message fields","text":"<p>Previously, you had to hand-craft specific fields and their options to create HTTP and message fields.  This was error-prone and time-consuming. Now, you can use helper methods to create these fields with ease.</p> <p>Follow the new steps found in the:</p> <ul> <li>HTTP fields guide</li> <li>Message fields guide</li> </ul>"},{"location":"use-case/changelog/0.14.2/#more-powerful-data-validations","title":"More powerful data validations","text":"<p>A large overhaul of the data validation API has been made to make it more powerful and easier to use. Instead of having two separate methods for each validation, you can now use a single method with a negate flag to  invert the validation. For example, <code>isNotNull</code> is now <code>isNull(true)</code>.</p> <p>You can also now define a field with a list of validations you want to apply to it for YAML. It now saves you from having to define the same field multiple times with different validations.</p> YAML <pre><code>---\nname: \"account_checks\"\ndataSources:\n  ...\n    validations:\n      - field: \"year\"\n        validation:\n          - type: \"null\"\n            negate: true\n          - type: \"equal\"\n            value: 2021\n</code></pre> <ul> <li>Updates<ul> <li><code>...Col</code> -&gt; <code>...Field</code> (i.e. <code>isEqualCol</code> -&gt; <code>isEqualField</code>)</li> <li><code>isNot...</code> -&gt; <code>is...(negate)</code> (i.e. <code>isNotNull</code> -&gt; <code>isNull(true)</code>)</li> <li><code>not...</code> -&gt; <code>...(negate)</code> (i.e. <code>notStartsWith(\"a\")</code> -&gt; <code>startsWith(\"a\", true)</code>)</li> <li><code>equalToOr...</code> -&gt; <code>...(strictly)</code> (i.e. <code>equalToOrLessThan(10)</code> -&gt; <code>lessThan(10, false)</code>)</li> <li><code>notIn(\"a\", \"b\")</code> -&gt; <code>in(List(\"a\", \"b\"), true)</code></li> <li><code>upstreamValidation...withValidation(...)</code> -&gt; <code>upstreamValidation...validations(...)</code></li> </ul> </li> <li>New<ul> <li><code>matchesList(regexes, matchAll, negate)</code></li> <li><code>hasTypes(types, negate)</code></li> <li><code>distinctInSet(set, negate)</code></li> <li><code>distinctContainsSet(set, negate)</code></li> <li><code>distinctEqual(set, negate)</code></li> <li><code>maxBetween(min, max, negate)</code></li> <li><code>meanBetween(min, max, negate)</code></li> <li><code>medianBetween(min, max, negate)</code></li> <li><code>minBetween(min, max, negate)</code></li> <li><code>stdDevBetween(min, max, negate)</code></li> <li><code>sumBetween(min, max, negate)</code></li> <li><code>lengthBetween(min, max, negate)</code></li> <li><code>lengthEqual(value, negate)</code></li> <li><code>isDecreasing(strictly)</code></li> <li><code>isIncreasing(strictly)</code></li> <li><code>isJsonParsable(negate)</code></li> <li><code>matchJsonSchema(schema, negate)</code></li> <li><code>matchDateTimeFormat(format, negate)</code></li> <li><code>mostCommonValueInSet(values, negate)</code></li> <li><code>uniqueValuesProportionBetween(min, max, negate)</code></li> <li><code>quantileValuesBetween(quantileRanges, negate)</code></li> </ul> </li> </ul>"},{"location":"use-case/changelog/0.14.2/#validate-http-responses","title":"Validate HTTP responses","text":"<p>Now you can validate the HTTP response status code, body and headers. This is useful when you want to ensure that the response from the HTTP request is as expected.</p> JavaScalaYAMLUI <pre><code>var httpTask = http(\"my_http\", Map.of(Constants.VALIDATION_IDENTIFIER(), \"POST/pets\"))\n        .fields(\n                ...\n        )\n        .validations(\n                validation().field(\"request.method\").isEqual(\"POST\"),\n                validation().field(\"response.statusCode\").isEqual(200),\n                validation().field(\"response.timeTakenMs\").lessThan(100),\n                validation().field(\"response.headers.Content-Length\").greaterThan(0),\n                validation().field(\"response.headers.Content-Type\").isEqual(\"application/json\")\n        )\n</code></pre> <pre><code>val httpTask = http(\"my_http\", options = Map(VALIDATION_IDENTIFIER -&gt; \"POST/pets\"))\n  .fields(\n    ...\n  )\n  .validations(\n    validation.field(\"request.method\").isEqual(\"POST\"),\n    validation.field(\"response.statusCode\").isEqual(200),\n    validation.field(\"response.timeTakenMs\").lessThan(100),\n    validation.field(\"response.headers.Content-Length\").greaterThan(0),\n    validation.field(\"response.headers.Content-Type\").isEqual(\"application/json\"),\n  )\n</code></pre> <p>In <code>docker/data/custom/validation/http/http-validation.yaml</code>: <pre><code>name: \"http_checks\"\ndataSources:\n  my_http:\n    - options:\n        validationIdentifier: \"POST/pets\"\n      validations:\n        - expr: \"request.method == 'POST'\"\n        - expr: \"response.statusCode == 200\"\n        - expr: \"response.timeTakenMs &lt; 100\"\n        - expr: \"response.headers.Content-Length &gt; 0\"\n        - expr: \"response.headers.Content-Type == 'application/json'\"\n</code></pre></p> <ol> <li>Open <code>Validation</code></li> <li>Click on <code>Manual</code> checkbox</li> <li>Click on <code>+ Validation</code> button and click <code>Select validation type</code> and select <code>Field</code></li> <li>Enter <code>request.method</code> in the <code>Field</code> text box</li> <li>Click on <code>+</code> next to <code>Operator</code> and select <code>Equal</code></li> <li>Enter <code>POST</code> in the <code>Equal</code> text box</li> <li>Continue adding validations for <code>response.statusCode</code>, <code>response.timeTakenMs</code>, <code>response.headers.Content-Length</code> and <code>response.headers.Content-Type</code></li> </ol> <p>Check here for full examples.</p>"},{"location":"use-case/changelog/0.14.3/","title":"0.14.3","text":"<p>Deployed: 23-01-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Update Data Contract CLI metadata gathering to capture primary key<ul> <li>Also, Data Contract CLI was updated to capture more fields from contract when exporting to Data Caterer YAML</li> </ul> </li> <li>Capture <code>map</code> data type when converting from YAML</li> <li>Ensure foreign key insert order is correct and pushes non-foreign key data sources to the end</li> <li>Update to use insta-integration@v2 github action</li> <li>Add <code>exponentialDistribution</code> and <code>normalDistribution</code> to data generator</li> </ul>"},{"location":"use-case/changelog/0.14.4/","title":"0.14.4","text":"<p>Deployed: 24-01-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Ensure step options are persisted when user defined steps come from YAML combined with generated from metadata steps</li> <li>Catch all exception when running plan, return error message and exit accordingly<ul> <li>Always show how long it took to run the plan</li> </ul> </li> </ul>"},{"location":"use-case/changelog/0.14.5/","title":"0.14.5","text":"<p>Deployed: 27-01-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Allow for <code>iceberg</code> data to be tracked for validation</li> <li>Changed order for <code>iceberg</code> connection details in Java to be <code>name</code>, <code>tableName</code> and <code>path</code></li> <li>Add getting metadata source connection details when generated from UI</li> <li>When capturing relationship from UI, convert override options to step name</li> <li>Fix bug when metadata generated details try to match with user defined details fails when no step options match</li> </ul>"},{"location":"use-case/changelog/0.14.6/","title":"0.14.6","text":"<p>Deployed: 31-01-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Fix bug relating to sending API calls to management server</li> <li>Filter out rows that cannot be transposed before sending alert to Slack</li> </ul>"},{"location":"use-case/changelog/0.14.7/","title":"0.14.7","text":"<p>Deployed: 02-02-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Fix bug when trying to use validations on sub-data source that gets generated from metadata</li> <li>Merge in data source options when multiple are defined via connection task builder</li> <li>Add in extra debug logs when saving real-time responses</li> </ul>"},{"location":"use-case/changelog/0.15.0/","title":"0.15.0","text":"<p>Deployed: 20-02-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Add in <code>rabbitmq</code> as a data source</li> <li>Add in <code>bigquery</code> as a data source</li> <li>Allow for empty sequences to be generated for per field counts</li> <li>Calculate number of records generated based on foreign key definitions</li> <li>Unpersist DataFrame after generating data to avoid OOM errors</li> <li>Update to use <code>jakarta.jms</code> v3.1.x<ul> <li>Use <code>sol-jms-jakarta</code> for JMS messaging to Solace</li> </ul> </li> <li>Introduce <code>uuid</code> data generation for random unique strings</li> <li>Introduce <code>oneOfWeighted</code> data generation for weighted random selection from set of values<ul> <li>Can be used for fields or record count</li> </ul> </li> <li>Allow users to use field <code>__index_inc</code> for generating unique values<ul> <li>Done via generation option <code>incremental</code></li> </ul> </li> <li>Update to use Java 17 across all modules and libraries</li> <li>Update to Gradle 8.12</li> </ul>"},{"location":"use-case/changelog/0.16.0/","title":"0.16.0","text":"<p>Deployed: 16-06-2025</p> <p>Latest feature and fixes for Data Catering include:</p> <ul> <li>Add in <code>jsonSchema</code> as a metadata source<ul> <li>Check here for documentation</li> </ul> </li> <li>Add in <code>includeFields</code>, <code>excludeField</code>, <code>includeFieldPatterns</code>, <code>excludeFieldPatterns</code><ul> <li>Check here for doucmentation</li> </ul> </li> <li>Update to Gradle 8.14</li> <li>Update to Spark 3.5.0</li> </ul>"}]}